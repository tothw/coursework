\documentclass{beamer}

\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary[topaths]
\usepackage{comment}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\proj}{proj}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\author{Paper By: Bodur, Dash, and G\"{u}nl\"{u}k\\
Presentation by: William Justin Toth}
\institute{University of Waterloo}
\date{April 8, 2016}
\title{A New Lift-and-Project Operator}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

\AtBeginSubsection[]
{
	\begin{frame}<beamer>
		\frametitle{Outline}
		\tableofcontents[currentsection, currentsubsection]
	\end{frame}
}

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\section{Introduction}

\begin{frame}
\frametitle{A fundamental goal}
Consider a $0$-$1$ mixed integer set \alert{$P^{IP}$} defined by:
$$\alert{P^{IP} := P \cap \{0,1\}^{n_1} \times \R^{n_2}}.$$
Where $\alert{P = \{x \in \R^n : Ax = b \}}$ is a polyhedron, and $\alert{n = n_1+ n_2}$.
\\\ \\
A central question in Integer Programming: Find a better approximation of $\alert{\conv(P^{IP})}$ than its linear relaxation $\alert{P}$.
\end{frame}

\begin{frame}
\frametitle{One approach}
We can define a hierarchy of relaxations $\alert{H^1, \dots, H^{n_1}}$ of $\alert{P^{IP}}$ such that
$$\alert{P \supseteq H^1 \supseteq \dots \supseteq H^{n_1} = \conv(P^{IP})}.$$
Two examples we will discuss: Lov\'asz-Schrijver and Sherali-Adams.\\
$\textbf{New contribution:}$ An operator which obtains $\alert{\conv(P^{IP})}$ in $\alert{\ceil{\frac{n_1}{2}}}$ steps instead of $\alert{n_1}$.
\end{frame}

\begin{frame}
\frametitle{Preliminaries: Extended LP formulations}
$\alert{Q = \{(x,y) \in \R^n \times \R^q : Cx + Dy \leq g \}}$ is called an $\textit{extended formulation}$ of $\alert{P}$ if
$$\alert{P = \proj_x(Q)}.$$
Where $\alert{proj_x(Q)}$ denotes the orthogonal projection of $\alert{Q}$:
$$\alert{\proj_x(Q) = \{x \in \R^: \exists y \in \R^q \text{ s.t. } (x,y) \in Q\}}.$$ 
In this talk we call $\alert{Q}$ an $\textit{extended LP formulation}$ of $\alert{P^{IP}}$.
\end{frame}

\begin{frame}
\frametitle{Preliminaries: $0$-$1$ Split cuts}
 For $\alert{i \in I = \{1, \dots, n_1\}}$ define the $0$-$1$ $\textit{split set}$:
 $$\alert{S_i = \{x \in \R^n : 0 < x_i < 1\}}.$$
 Inequality $\alert{c^Tx \geq d}$ is a $0$-$1$ $\textit{split cut}$ generated by $\alert{S_i}$ if it is valid for $\alert{\conv(P\backslash S_i)}$.
 \\\ \\
 We define $0$-$1$ $\textit{split closure}$ of $\alert{P}$ as
 $$\alert{S(P) = \bigcap_{i \in I} \conv(P \backslash S_i)}.$$
\end{frame}

\begin{frame}
\frametitle{Preliminaries: Notes on Split Closure}
$\alert{S(P)}$ is a polyhedron. So $\alert{S}$ can be applied iteratively.
\\\ \\
Define $\alert{S^k(P)}$ as follows:
\begin{itemize}
\item $\alert{S^0(P) = P}$,
\item $\alert{S^k(P) = S(S^{k-1}(P))}$ for $\alert{k \geq 1}$.
\end{itemize}
Theorem (Balas): $\alert{S^{n_1}(P) = \conv(P^{IP})}$.
\end{frame}

\section{Lov\'asz-Schrijver}

\begin{frame}
\frametitle{Lift-and-Project operators}
All Lift-and-Project operators are based on a common scheme:
\begin{enumerate}
\item $\textbf{Lift:}$ Create an extended formulation of $\alert{P}$.
\item $\textbf{Strengthen:}$ Perform a strengthening of the extended formulation by adding cuts.
\item $\textbf{Project:}$ Project the strengthened formulation onto the original space.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Lov\'asz-Schrijver operator - Lift step}
Let $\alert{(a^\ell)^Tx \leq b^\ell}$, $\alert{\ell \in L = \{1, \dots, m\}}$ be the inequalities in $\alert{Ax \leq b}$. Going forward we assume $\alert{0 \leq x_i \leq 1}$ is included in $\alert{Ax \leq b}$ for each $\alert{i \in I = \{1,\dots,n_1\}}$.
\\ We start with the quadratic inequalities:
\begin{align*}
\alert{x_i(b^\ell - (a^\ell)^Tx) \geq 0}, &\text{ for all } \alert{i\in I,\ \ell \in L} \\
\alert{(1-x_i)(b^\ell - (a^\ell)^Tx) \geq 0}, &\text{ for all }\alert{ i\in I,\ \ell \in L} \\
\end{align*}
\end{frame}
\begin{frame}
\frametitle{Lov\'asz-Schrijver operator - Lift step cont'd}
We linearize each  quadratic term $\alert{x_{ij}}$ by replacing with variable $\alert{y_{ij}}$.\\
Let $\alert{\hat{M}(P)}$ be the set obtained after the Lift step, it satisfies:
\begin{align*}
\alert{b^\ell x_i - \sum_{j=1}^n a_j^\ell y_{ij} \geq 0}, &\text{ for all } \alert{i \in I,\ \ell \in L} \\
\alert{b^\ell - \sum_{j = 1}^n a^\ell_j x_j - b^\ell x_i + \sum_{j = 1}^n a_j^\ell y_{ij} \geq 0}, &\text{ for all } \alert{i \in I,\ \ell \in L} \\
\alert{y_{ij} = y_{ji}}, &\text{ for all } \alert{i<j,\ i,j \in L}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Lemma: $\hat{M(P)}$ is an extended LP formulation of $P^{IP}$}
$\textbf{Proof Idea:}$ Fix $\alert{i, \ell}$. Adding inequalities:
$$\alert{b^\ell x_i - \sum_{j=1}^n a_j^l y_{ij} \geq 0} \text{ and } \alert{b^\ell -\sum_{j=1}^n a_j^\ell x_j - b^\ell x_i + \sum_{j=1}^n a_j^\ell y_{ij} \geq 0},$$
We obtain that $$\alert{b^\ell -(a^\ell)^Tx \geq 0}.$$
Therefore $\alert{\proj_x(\hat{M}(P)) \subseteq P}$.
\end{frame}

\begin{frame}
\frametitle{Lemma: $\hat{M(P)}$ is an extended LP formulation of $P^{IP}$}
$\textbf{Proof Continued:}$ Now let $\alert{\bar{x} \in P}$. Define $$\alert{\bar{y}=(x_i x_j : i \in I, j \in \{1, \dots, n\})}.$$
Then the values of $\alert{\hat{M}}$ inequalities left hand sides are equal to:
$$\alert{\bar{x}_i (b^\ell - (a^\ell)^T \bar{x})} \text{ and } \alert{(1-\bar{x}_i )(b^\ell - (a^\ell)^T \bar{x})}.$$
These are clearly non-negative as $\alert{\bar{x}}$ satisfies $\alert{0 \leq \bar{x} \leq 1}$ and $\alert{A \bar{x} \leq b}$.\\
Therefore $\alert{\proj_x(\hat{M}(P)) = P}$.
\end{frame}

\begin{frame}
\frametitle{Lov\'asz-Schrijver operator - Strengthen and Project steps}
We strengthen $\alert{\hat{M}(P)}$, obtaining the set $\alert{M(P)}$ as follows:
$$\alert{M(P) = \hat{M}(P) \cap \{ (x,y) : y_{ii} = x_{i},\ \forall i \in I\} } $$
Finally the strengthening of $\alert{P}$ after projection is given by:
$$\alert{N(P) = \proj_x(M(P))}$$
$\textbf{Note:}$ $\alert{P^{IP} \subseteq N(P)}$ as for all $\alert{\bar{x} \in P^{IP}}$ and $\alert{i \in I}$, 
$$\alert{\bar{x}_i^2 = \bar{x}_i}$$
\end{frame}

\begin{frame}
\frametitle{Lemma: For any $i\in I$, $y_{ii} = x_{i}$ is valid for $S(\hat{M}(P))$} 
$\textbf{Proof Idea:}$ From our quadratic inequalities, the $\textit{McCormick Inequalities}$ are valid for $\alert{\hat{M}(P)}$:
\begin{align*}
\alert{x_i x_j \geq 0} & \rightarrow \alert{y_{ij} \geq 0} \\
\alert{x_i(1-x_j)\geq 0} & \rightarrow \alert{x_i \geq y_{ij}} \\
\alert{(1-x_i)x_j \geq 0} & \rightarrow \alert{x_j \geq y_{ij}} \\
\alert{(1-x_i)(1-x_j) \geq 0} & \rightarrow \alert{1 - x_i - x_j + y_{ij}  \geq 0}
\end{align*}
Thus $\alert{x_i = 0} \implies \alert{y_{ij} = 0}$ and $\alert{x_i = 1} \implies \alert{y_{ij} = x_j}$. So if $\alert{x_i = 0}$ or $\alert{x_i = 1}$ then
$$\alert{y_{ii} = x_i}$$
\end{frame}

\begin{frame}
\frametitle{New operator - Strengthening Lov\'asz-Shrijver}
Previous Lemma suggests that $\alert{M(P)}$ is obtained from $\alert{\hat{M}(P)}$ by adding one split cut for each $\alert{S_i}$, $\alert{i \in I}$.\\\ \\
What is instead we added all $0$-$1$ split cuts?\\
We'd obtain the new operator:
$$\alert{\tilde{N}(P) = \proj_x(S(\hat{M}(P)))}$$
Notice our Lemma gives $\alert{S(\hat{M}(P)) \subseteq M(P)}$ which implies:
$$\alert{\tilde{N}(P) \subseteq N(P)}$$
\end{frame}

\begin{frame}
\frametitle{Theorem: $S(\hat{M}(P)) \subseteq M(S(P))$}
Says new operator's strengthened extension of $\alert{P}$ is stronger than Lov\'asz-Schrijver's of $0$-$1$ split closure of $\alert{P}$.
\\$\textbf{Proof Idea:}$ Observe that $$\alert{M(S(P)) = \hat{M}(S(P)) \cap \{(x,y):y_{ii} = x_i, \forall i \in I\}}$$
Lemma implies $$\alert{S(\hat{M}(P)) \subseteq \{(x,y):y_{ii}=x_i, \forall i \in I \}}$$  and it is easy to see from straightforward computation that $$\alert{S(\hat{M}(P)) \subseteq \hat{M}(S(P))}.$$ 
\end{frame}

\begin{frame}
\frametitle{Some Corollaries}
Consequences of $\textbf{Theorem:}$ $\alert{S(\hat{M}(P)) \subseteq M(S(P))}$:
\begin{itemize}
\item By projecting onto space of $\alert{x}$ variables: $$\alert{\tilde{N}(P) \subseteq N(S(P))}$$
\item Since $\alert{N(S(P)) \subseteq S(S(P))}$ new operator obtains convex hull in half the steps: $$\alert{(\tilde{N})^{\ceil{\frac{n_1}{2}}}(P) = \conv(P^{IP})}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sherali-Adams level $k$ operator - Lift Step}
Define $\alert{PSA^k(P)}$ by, for each inequality $\alert{b^\ell - (a^\ell)^Tx \geq 0}:$
$$\alert{\prod_{i \in J_1} x_i \prod_{j \in J_2} (1 - x_j) (b^\ell - (a^\ell)^Tx) \geq 0}$$
for all $\alert{(J_1, J_2)}$ "pairs of order $k$".\\
$\textbf{pair of order k:}$ $\alert{J_1, J_2 \subseteq I}$, $\alert{|J_1 \cup J_2| = k}$, and $\alert{J_1 \cap J_2 = \emptyset}$.
\end{frame}

\begin{frame}
\frametitle{Sherali-Adams level $k$ operator - Lift Step cont'd}
Let $\alert{\mathcal{T}}$ be set of tuples with $\alert{\leq k+1}$ indices. Then $\alert{\exists T(J_1,J_2,\ell) \subseteq \mathcal{T}}$, we may rewrite:
$$\alert{\prod_{i \in J_1} x_i \prod_{j \in J_2} (1 - x_j) (b^\ell - (a^\ell)^Tx) \geq 0}$$
as
$$ \alert{\beta - \sum_{S \in T(J_1, J_2,\ell)} (\alpha_S \prod_{j=1}^{|S|} x_{S_j}) \geq 0}$$
which can be linearized to obtain $\alert{ESA^k(P)}$:
$$\alert{\beta -\sum_{S \in T(J_1, J_2,\ell)} \alpha_S y_S \geq 0} $$
$\text{ for all } \alert{\ell \in L},\ \alert{(J_1, J_2)} \text{ pairs of order } \alert{k}$
\end{frame}

\begin{frame}
\frametitle{Sherali-Adams level $k$ operator - Strengthen and Project Steps}
For $\alert{S \in \mathcal{T}}$ let $\alert{[S]}$ be tuple of unique elements of $\alert{S}$ in same order.\\
$\textbf{Example:}$ For $\alert{k=2}$, $\alert{S = (1,1,2)}$ gives $\alert{[S] = (1,2)}$.\\
To strengthen create $\alert{LSA^k(P)}$ by adding to $\alert{ESA^k(P)}:$
$$\alert{y_S = y_{[S]} \quad} \text{for all } \alert{S \in \mathcal{T}}$$
$\textbf{Sherali-Adams operator:}$ $\alert{SA^k(P)}$ given by $\textit{projecting}$ $\alert{ESA^k(P)}$ onto the space of variables $\alert{y_{(1)} ,\dots y_{(n)}}$. Notice each $\alert{y_{(i)} = x_i}$.
\end{frame}

\begin{frame}
\frametitle{Sherali-Adams example}
For $\alert{k=1}$, $\alert{SA^k(P)}$ is simply $\alert{N(P)}$, so let's consider $\alert{k=2}$.\\
First $\alert{PSA^2(P)}\textbf{:}$
\alert{\begin{align*}
x_ix_j(b^\ell -(a^\ell)^Tx \geq 0,&\ &\forall i<j \in I, \ell \in L \\
x_i(1-x_j)(b^\ell -(a^\ell)^Tx \geq 0,&\ &\forall i\neq j \in I, \ell \in L \\
(1-x_i)(1-x_j)(b^\ell -(a^\ell)^Tx \geq 0,&\ &\forall i<j \in I, \ell \in L \\
\end{align*}}
Second obtain $\alert{ESA^2(P)}$ by replacing each $\alert{x_ix_jx_k}$ with $\alert{y_{(i,j,k)}}$, $\alert{x_ix_j}$ with $\alert{y_{(i,j)}}$, and $\alert{x_i}$ with $\alert{y_{(i)}}$.\\
Then obtain $\alert{LSA^2(P)}$ by replacing each $\alert{y_{(i,i,k)}}$ and $\alert{y_{(i,k,k)}}$ with $\alert{y_{(i,k)}}$, and $\alert{y_{(i,i)}}$ with $\alert{y_{(i)}}$.
\end{frame}

\begin{frame}
\frametitle{Some Lemmas}
The following observations about the Sherali-Adams operator are analogous to those we observed regarding Lovasz-Schrijver:\\
\begin{itemize}
\item$\textbf{Lemma 1:}$ For any $\alert{k \geq 1}$, $\alert{ESA^k(P)}$ is an extended LP formulation of $\alert{P^{IP}}$.
\item$\textbf{Lemma 2:}$ For any $\alert{S \in \mathcal{T}}$, $\alert{y_S = y_{[S]}}$ is a $0$-$1$ split cut for $\alert{ESA^k(P)}$.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Comparing new operator to level-2 Sherali-Adams}
$\textbf{Theorem:}$ $\alert{SA^2(P) \subseteq \tilde{N}(P)}$\\
$\textbf{Proof Sketch:}$
\begin{itemize}
\item Recall: $\alert{\hat{M}(P)}$ defined on variables $\alert{x = (x_i : i \in I)}$ and $\alert{y = (y_{ij}: i \in I, j \in J)}$
\item So $\alert{M(\hat{M}(P))}$ defined on variables $\alert{v_{i,jk}}$ (representing products $\alert{x_iy_{jk}}$) in addition to $\alert{x's}$ and $\alert{y's}$
\item Since $\alert{\proj_{(x,y)}(M(\hat{M}(P))) \subseteq S(\hat{M}(P))}$ we have $\alert{\proj_x (M(\hat{M}(P))) \subseteq \proj_x (S(\hat{M}(P))) = \tilde{N}(P)}$.
\end{itemize}
(So just need to show $\alert{SA^2(P) \subseteq \proj_x(M(\hat{M}(P)))}$)
\end{frame}

\begin{frame}
\frametitle{Proof continued - show $\alert{SA^2(P) \subseteq \proj_x(M(\hat{M}(P)))}$}
\begin{itemize}
\item Let $\alert{\hat{x} \in SA^2(P)}$. Then $\alert{\exists \hat{y} = (\hat{y}_S : S \in \mathcal{T}) \in LSA^2(P)}$ such that each $\alert{\hat{x}_i = \hat{y}_{(i)}}$.
\item Construct $\alert{(\tilde{x},\tilde{y},\tilde{z}) \in M(\hat{M}(P))}$ as follows:
\begin{itemize}
\item $\alert{\tilde{x}_i = \hat{y}_{(i)} = \hat{x}_i}$
\item $\alert{\tilde{y}_{ij} = \hat{y}_{(p,q)}}$ where $\alert{p \leq q}$ and $\alert{\{p,q\} = \{i,j\}}$
\item $\alert{\tilde{v}_{i,jk} = \hat{y}_{(p,q,r)}}$ were $\alert{p \leq q \leq r}$ and $\alert{\{p,q,r\} = \{i,j,k\}}$
\end{itemize}
\item Finish showing $\alert{(\tilde{x},\tilde{y}, \tilde{v}) \in M(\hat{M}(P))}$ by case analysis on types of inequalities defining $\alert{M(\hat{M}(P))}$
\end{itemize}
$\blacksquare$
\end{frame}

\begin{frame}
\frametitle{Application to Stable Set Polytope}
$\textit{Stable set polytope}$ of graph $\alert{G = (V,E)}$ is convex hull of incidence vectors of independent sets:
$$\alert{STAB(G) = \conv(\{x \in \{0,1\}^{|V|} : x_i + x_j \leq 1,\ \forall \{i,j\} \in E\})}$$
Linear programming relaxation is called $\textit{fractional stable set polytope}$:
$$\alert{FSTAB(G) = \{x \in \R^{|V|} : x_i + x_j \leq 1,\ \forall \{i,j\} \in E;\ 0 \leq x_i \leq 1,\ \forall i \in V\}}$$
\end{frame}

\begin{frame}
\frametitle{Application to Stable Set Polytope}
From our previous results we have a hierarchy of relaxations:
$$\alert{STAB(G) \subseteq SA^2(P) \subseteq \tilde{N}(P) \subseteq N^2(P) \subseteq N(P) \subseteq P = FSTAB(G)}$$
with $\alert{\tilde{N}(P) \subseteq N^2(P)}$ following from $\alert{\tilde{N}(P) \subseteq N(S(P))}$.\\
\ \\
$\textbf{Lemma:}$ If $\alert{G = K_n}$ and $\alert{P = FSTAB(G)}$, then $\alert{SA^2(P) = N^2(P)}$.\\
\ \\
In other words the containment bounds on the new operator $\alert{\tilde{N}}$ are tight.
\end{frame}

\begin{frame}
\frametitle{Application to Stable Set Polytope}
But can we also demonstrate cases where the containment $\alert{SA^2(P) \subseteq \tilde{N}(P) \subseteq N^2(P)}$ is strict?
\end{frame}

%ENDS HERE
%STOP SCROLLING
%PLEASE SEE THIS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%AND DON'T GO PAST IT
%-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
%Last talk for examples
\begin{comment}
\section{Iterative Rounding}
\subsection{Bipartite Matching}

\begin{frame}
\frametitle{Weighted Bipartite Matching Problem}
Let $G=(V, E)$ be a bipartite graph with bipartition $V = V_0 \dot\cup V_1$. 
\begin{itemize}
\item For each $e \in E$, assign a cost $c_e$
\item $M \subseteq E$ is a matching if $\delta(v) \cap M \leq 1$ for all $v \in V$
\item $M(v)$ denotes the vertex $v$ is matched to under $M$
\item Problem: Find a matching $M$ maximizing $\sum_{e\in M} c_e$.
\end{itemize}
\begin{figure}
\centering
\begin{tikzpicture}
   \node[shape=circle,draw=black] (B) at (0,1) {b};
    \node[shape=circle,draw=black] (A) at (0,3) {a};
    \node[shape=circle,draw=black] (E) at (2.5,1) {e};
    \node[shape=circle,draw=black] (D) at (2.5,2) {d}; 
    \node[shape=circle,draw=black] (C) at (2.5,3) {c};

    \path[-] (A) edge node[above] {$1$} (C);
    \path [line width=1mm,-](A) edge node[above] {$4$} (D);
    \path [line width=1mm,-](B) edge node[above] {$3$} (C);
    \path [-](B) edge node[above] {$2$} (E);
    \path [-](B) edge node[above] {$4$} (D);

\end{tikzpicture}
\caption{A bipartite graph with edge weights. Optimal matching edges are thicker. $M(a) = d$}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Linear Programming Relaxation}
This problem can be formulated as a linear program, denoted $LP_M(G)$:
\begin{align*}
&\text{maximize} &\sum_{e \in E} c_e x_e \\
&\text{subject to} &x(\delta(v)) &\leq 1, &\forall v \in V\\
& &x_e &\geq 0, &\forall e \in E
\end{align*}
Where $x(\delta(v)) = \sum_{e \in \delta(v)} x_e$.
\end{frame}

\subsection{Integrality by Iterative Rounding}
\begin{frame}
\frametitle{Iterative Algorithm}
\begin{enumerate}
\item Input bipartite matching instance $G$
\item Set matching $M$ to $\emptyset$
\item While $E \neq \emptyset$
\begin{enumerate}
\item Find extreme point optimal solution $x$ to $LP_M(G)$.
\item For each $e=vw \in E$ such that $x_e = 1$ add $e$ to $M$ and remove vertices $v,w$ from $G$.
\item For each $e \in E$ such that $x_e = 0$ remove $e$ from $G$.
\end{enumerate}
\item Output $M$
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Characterize Extreme Point Solution}
$\textbf{Rank Lemma:}$ The maximal number of linearly independent tight constraints at an extreme point is equal to the dimension of the polytope.\\
\uncover<2->{$\implies$\\
Let $x$ be an extreme point of $LP_M(G)$ such that $0<x<1$. Then there exists a set $W \subseteq V$ such that:
\begin{enumerate}
\item $|W| = |E|$
\item $x(\delta(v)) = 1$ for all $v \in W$
\item $\{\chi(\delta(v)) : v \in W \}$ is linearly independent
\end{enumerate}
where $\chi(\delta(v))$ denotes the incidence vector of $\delta(v)$.}
\end{frame}

\begin{frame}
\frametitle{Correctness - Finding $0$ or $1$}
\begin{itemize}
\item<1-> Suppose for contradiction we have an extreme point solution $x$ such that $0 < x < 1$
\item<2-> Choose $W \subseteq V$ with $|W| = |E|$ corresponding to a set of tight linearly independent constraints.
\item<3-> Will show $|E| > |W|$ by a Token Argument to obtain contradiction.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correctness - Finding $0$ or $1$ Token Argument}
\begin{itemize}
\item Give each edge a token.
\item Redistribute tokens to tight constraints.
\item Show after redistribution that all constraints have token and some edge has leftover.
\end{itemize}
\uncover<2->{We will redistribute according to the following rule:\\
\begin{enumerate}
\item Each edge $vw$ gives $\frac{1}{2}$ token to $v$ if $v \in W$ and gives $\frac{1}{2}$ token to $w$ if $w \in W$. 
\end{enumerate}}
\end{frame}

\begin{frame}
\frametitle{Correctness - Finding $0$ or $1$ Counting Tokens}
\begin{itemize}
\item<1->Each edge gives away $\leq 1$ token.
\item<2-> For any $v \in W$, $x(\delta(v)) = 1$ and each $x_e < 1$ implies $d(v) \geq 2$.
\item<3-> Each tight constraint receives $\geq 2 \cdot \frac{1}{2} = 1$ tokens.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correctness - Finding $0$ or $1$ Token Leftover}
\begin{itemize}
\item<1-> Since $G$ is bipartite: $\sum_{v \in V_0} \chi(\delta(v)) = \sum_{v \in V_1} \chi(\delta(v))$.
\item<2-> So there exists $v \not\in W$ (otherwise constraints in $W$ are not linearly independent).
\item<3-> Any $e \in \delta(v)$ does not give away all its tokens.
\item<4-> So $|E| > |W|$, a contradiction.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Correctness - Optimality}
Let $M$ be matching returned. Let $x$ be optimal extreme point solution to $LP_M(G)$. Show $c(M) \geq c^Tx$ by induction on number of iterations of algorithm.
\begin{itemize}
\item<1-> If algorithm fixes $x_{vw} = 1$ during iteration then residual problem is: find a matching $M'$ on $G'=G-v-w$. Algorithm returns $M' \cup \{vw\}$.
\item<2-> $x' = x$ restricted to edges of $G'$ is feasible for $LP_M(G')$.
\item<3-> So $c(M' \cup \{vw\}) = c(M') + c_{vw} \geq c(x') + c_{vw} = c(x)$,
\item<4-> The case for fixing $x_{vw} = 0$ is similar.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{What do we get from this?}
Correct algorithm implies integrality of polytope:
\begin{itemize}
\item  From Linear Programming theory:  for extreme point $x$, there is a cost vector $c$, $x$ is unique optimal solution.
\item If $x$ is fractional, and we ran the algorithm with cost $c$ we would obtain an integral solution, $x^{Int}$ of equivalent cost.
\item Since $x^{Int}$ is feasible and $x^{Int} \neq x$ this contradicts uniqueness.
\end{itemize}
\end{frame}

\section{Stable Matching Polytope}
\subsection{Problem Formulation}
\begin{frame}
\frametitle{Stable Matching Problem}
\begin{itemize} \item As before consider a weighted bipartite graph $G$. \item Now each vertex ranks their neighbours in order of preference. \item Write $w_1 >_v w_2$ to denote that $v$ prefers $w_1$ to $w_2$. \item For any neighbour $w$ of $v$ let $\delta^{>w}(v) = \{vw' \in E : w' >_v w\}$. \item Let $n^*(v)$ ($n_*(v)$ respectively) be most (least respectively) preferred neighbour of $v$. \end{itemize} We aim to find a max weight stable matching.
\end{frame}

\begin{frame}
\frametitle{Stability}
A matching $M$ on $G$ is said to be stable if there does not exist $vw \in E$ (called a blocking pair) for which any of the following holds:
\begin{enumerate}
\item Both $v$ and $w$ have matches and $M(v) <_v w$ and $M(w) <_w v$.
\item $v$ has a match but not $w$ and $M(v) <_v w$.
\item $w$ has a match but not $v$ and $M(w) <_w v$.
\item Both $v$ and $w$ have no matches.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{A blocking pair}
Suppose that in the graph below, $c >_a d$ and $a >_c b$. Then $ac$ is a blocking pair.
\begin{figure}
\centering
\begin{tikzpicture}
   \node[shape=circle,draw=black] (B) at (0,1) {b};
    \node[shape=circle,draw=black] (A) at (0,3) {a};
    \node[shape=circle,draw=black] (E) at (2.5,1) {e};
    \node[shape=circle,draw=black] (D) at (2.5,2) {d}; 
    \node[shape=circle,draw=black] (C) at (2.5,3) {c};

    \path[-] (A) edge node[above] {$1$} (C);
    \path [line width=1mm,-](A) edge node[above] {$4$} (D);
    \path [line width=1mm,-](B) edge node[above] {$3$} (C);
    \path [-](B) edge node[above] {$2$} (E);
    \path [-](B) edge node[above] {$4$} (D);

\end{tikzpicture}
\caption{This matching has a blocking pair $ac$.}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{A Linear Programming Relaxation}
Let $LP_{SM}(G)$ be defined as follows:
\begin{align*}
&\text{maximize} &\sum_{e \in E} c_e x_e \\
&\text{subject to} &x(\delta(v)) &\leq 1, &\forall v \in V\\
& &\alert{x(\delta^{>w}(v))} \alert{+ x(\delta^{>v}(w)) + x_{vw}} &\alert{\geq 1,} &\alert{\forall vw \in E}\\
& &x_e &\geq 0, &\forall e \in E
\end{align*}
Let $n^*(x,v)$ ($n_*(x,v)$ respectively) be most (least respectively) preferred neighbour of $v$ such that $x_{vn^*(x,v)} > 0$ ($x_{vn_*(x,v)} > 0$ respectively). \\
Also define $S(x) = \{ v \in V : x(\delta(v)) > 0 \}$.
\end{frame}

\subsection{Some Observations}
\begin{frame}
\frametitle{Integral Points are Stable}
Let $x$ be the incidence vector of a matching on $G$. Then $x$ is stable if and only if $x$ is feasible for $LP_{SM}(G)$.\\
$\textbf{Proof Idea:}$ Since $x$ is integral, $x$ is infeasible if and only if there exists $vw \in E$ such that $$ x(\delta^{>w}(v)) = x(\delta^{>v}(w)) = x_{vw} = 0. $$
Such a $vw$ is a blocking pair. $\blacksquare$
\end{frame}

\begin{frame}
\frametitle{Lemma 1}
Let $x$ be feasible for $LP_{SM}(G)$ and let $vw \in E$. Then $$ v \in S(x) \text{ and } w \geq_v n^*(x,v)) $$ $\implies$ $$ x(\delta(w)) = 1 \text{ and } v \leq_w n_*(x,w). $$\\
$\textbf{Proof Idea:}$ Notice $x(\delta^{>w}(v)) = 0$. So $$1 \leq x(\delta^{>w}(v)) + x(\delta^{>v}(w)) + x_{vw} = x(\delta^{\geq v}(w)) \leq 1. \blacksquare$$\\
\end{frame}

\begin{frame}
\frametitle{Lemma 2}
Let $x$ be feasible for $LP_{SM}(G)$ and let $vw \in E$. Then $$v \in S(x) \text{ and } \alert{w = n^*(x,v)}$$ \alert{if and only if} $$x(\delta(w)) = 1 \text{ and } \alert{v = n_*(x,w)}. $$\\
$\textbf{Proof Idea:}$ Similar to previous with equality holding throughout. $\blacksquare$
\end{frame}

\begin{frame}
\frametitle{Lemma 3}
Let $x$ be feasible for $LP_{SM}(G)$. Then $$ v \in S(x) $$ if and only if $$x(\delta(v)) = 1.$$ \\
$\textbf{Proof Idea:}$
\begin{itemize}
\item By previous Lemmas $n^*(x,\cdot) : S(x) \rightarrow F(x) = \{v \in S(x) : x(\delta(v)) = 1 \}$.
\item If $n^*(x,v_1) = w = n^*(x,v_2)$ then by Lemma $2$ $v_1 = n_*(x,w) = v_2$
\item So $n^*(x,\cdot)$ is injective. $\blacksquare$
\end{itemize}
\end{frame}

\subsection{Rothblum's Original Proof}
\begin{frame}
\frametitle{Proof of Integrality}
Rothblum showed that the extreme points of  $LP_{SM}(G)$:
\begin{align*}
&\text{maximize} &\sum_{e \in E} c_e x_e \\
&\text{subject to} &x(\delta(v)) &\leq 1, &\forall v \in V\\
& &x(\delta^{>w}(v)) + x(\delta^{>v}(w)) + x_{vw} &\geq 1, &\forall vw \in E\\
& &x_e &\geq 0, &\forall e \in E
\end{align*}
are integral.
\end{frame}

\begin{frame}
\frametitle{Proof of Integrality - Strategy}
Let $x$ be an extreme point of $LP_{SM}(G)$. Rothblum uses a convex combination argument: 
\begin{itemize}
\item $x$ is extreme $\iff$ there does not exist $x^1 \neq x^2$ feasible such that $x = \frac{1}{2}(x^1 + x^2)$.
\item Define vector $z$ such that $z = 0$ $\implies$ $n^*(x,v) = n_*(x,v)$ for all $v$.
\item Which would imply $x$ is integral.
\item Rothblum then shows $x \pm \epsilon z$ feasible for sufficiently small $\epsilon$.
\item By extremality of $x$ proof is complete.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Proof of Integrality - Definition of $z$}
Define the vectors $z^*$, $z_*$, and $z$ as follows:
\begin{align*}
(z^*)_{vw} &= \begin{cases}
	1, &\text{if $v\in S(x) \cap V_0$ and $w = n^*(x,v)$} \\
	0, &\text{otherwise}
\end{cases} 
\\
\uncover<2->{(z_*)_{vw} &= \begin{cases}
	1, &\text{if $v\in S(x) \cap V_0$ and $w = n_*(x,v)$} \\
	0, &\text{otherwise}
\end{cases}}
\\
\uncover<3->{z_{vw} &= (z^*)_{vw} - (z_*)_{vw}.}
\end{align*}
\uncover<4->{By invoking our Lemmas, $z^*$ and $z_*$ are equivalently:
\begin{align*}
(z^*)_{vw} &= \begin{cases}
	1, &\text{if $w\in S(x) \cap V_1$ and $v = n_*(x,w)$} \\
	0, &\text{otherwise}
\end{cases} 
\\
(z_*)_{vw} &= \begin{cases}
	1, &\text{if $w\in S(x) \cap V_1$ and $v = n^*(x,w)$} \\
	0, &\text{otherwise}
\end{cases}
\end{align*}}
\end{frame}

\begin{frame}
\frametitle{Proof of Integrality - Showing $x \pm \epsilon z$ is Feasible}
We claim that $z$ satisfies the following:
\begin{itemize}
\item<2-> If $x(\delta(v)) = 1$ then $z(\delta(v)) = 0$ for all $v \in V$.
\item<3-> If $x_e = 0$ then $z_e = 0$ for all $e \in E$.
\item<4-> If $x(\delta^{>w}(v)) + x(\delta^{>v}(w)) + x_{vw} = 1$ then $z(\delta^{>w}(v)) + z(\delta^{>v}(w)) + z_{vw} = 0$.
\end{itemize}
\uncover<5->{Thus we can choose $\epsilon > 0$ such that $x \pm \epsilon z$ is feasible for $LP_{SM}(G)$.} \\
\uncover<6->{Since $x$ is an extreme point this means $z = 0$. That is, $z^* = z_*$ and therefore $x$ is integral. $\blacksquare$}
\end{frame}

\section{An Iterative Rounding Proof?}
\subsection{Extreme Point Characterization}
\begin{frame}
Recall: $LP_{SM}(G)$:
\begin{align*}
&\text{maximize} &\sum_{e \in E} c_e x_e \\
&\text{subject to} &x(\delta(v)) &\leq 1, &\forall v \in V\\
& &x(\delta^{>w}(v)) + x(\delta^{>v}(w)) + x_{vw} &\geq 1, &\forall vw \in E\\
& &x_e &\geq 0, &\forall e \in E
\end{align*}
Is there an Iterative Rounding proof of integrality?\\
Recall Iterative Rounding: \\Characterize tight constraints $\Rightarrow$ Iterative Algorithm $\Rightarrow$ Analysis.
\end{frame}

\begin{frame}
\frametitle{Extreme Point Characterization}
Let $x$ be an extreme point solution to $LP_{SM}(G)$ such that $x>0$. Then there exists $W \subseteq V$ and \alert{$T \subseteq E$} such that the following hold:
\begin{enumerate}
\item $x(\delta(v)) = 1$, $\forall v \in W$.
\item\alert<1->{ $x(\delta^{>w}(v))+ x(\delta^{>v}(w) + x_{vw} = 1$, $\forall vw \in T$.}
\item The vectors in $\{\chi(\delta(v)) : v \in W\}$ together with the vectors in \alert<1->{$\{\chi(\delta_v^>(w)) + \chi(\delta_w^>(v)) + \chi(vw) : vw \in T\}$} are all linearly independent.
\item $|W| \alert{+ |T|} = |E|$.
\end{enumerate}
\end{frame}

\subsection{Finding a $0$ or $1$}

\begin{frame}
\frametitle{Finding a $0$ or $1$ - Choose $W$ and $T$}
\begin{itemize}
\item Suppose for a contradiction we have an extreme point solution of $LP_{SM}(G)$, $x$, such that $0 < x < 1$. 
\item There are $W \subseteq V$ and $T \subseteq E$ corresponding to a maximal set of linearly independent tight constraints such that $|W| + |T| = |E|$
\item Choose the pair $(W,T)$ which  maximizes $|W|$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Finding a $0$ or $1$ - $vn_*(v) \not\in T$}
Let $v \in W$.  Let $w = n_*(v)$. Then $vw \not\in T$.\\
$\textbf{Proof Idea:}$
\begin{itemize}
\item<2-> If $vw$ in $T$ then $x(\delta^{>w}(v)) + x(\delta^{>v}(w)) + x_{vw} = 1 = x(\delta(v))$.
\item<3-> Since $\delta^{\geq w}(v) = \delta(v)$, we have $x(\delta^{>v}(w)) = 0$.
\item<4-> Therefore $\chi(\delta^{>w}(v)) + \chi(\delta^{>v}(w)) + \chi(vw) = \chi(\delta(v))$. This contradicts the linear independence of the constraints chosen.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Finding a $0$ or $1$ - $vn^*(v) \not\in T$}
 Let $v \in V$. Let $w = n^*(v)$. Then $vw \not\in T$.\\
 $\textbf{Proof Idea:}$
\begin{itemize}
\item<2-> If $vw \in T$ then $x(\delta^{>w}(v)) + x(\delta^{>v}(w)) + x_{vw} = 1$.
\item<3-> Since $x(\delta^{>w}(v)) = 0$ we have $x(\delta^{\geq v}(w)) = 1$.
\item<4-> So $\chi(\delta^{\geq v}(w)) = \chi(\delta(w))$ and $\chi(\delta^{>w}(v)) + \chi(\delta^{>v}(w)) + \chi(vw) = \chi(\delta(w)).$
\item<5-> If $w \in W$ this contradicts linear independence. Otherwise this contradicts the maximality of $W$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Finding a $0$ or $1$ - Degree Lower Bound}
\begin{itemize}
\item As before, for any $v \in W$, $d(v) \geq 2$.
\item $\textbf{Proof Idea:}$ $x(\delta(v)) = 1$ and $x_e < 1$ for each $e \in \delta(v)$ so $d(v) \geq 2$. 
\item Since $vn^*(v)$ and $vn_*(v)$ are not in $T$ each vertex in $W$ has two incident edges not in $T$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Finding a $0$ or $1$ - Token argument}
Give each edge in $E$ a token. Redistribute tokens as follows for any $vw \in E$:
\begin{enumerate}
\item If $vw \in T$ have $vw$ give itself token.
\item Otherwise if $v \in W$ have $vw$ give $\frac{1}{2}$ token to $v$, and if $w \in W$ have $vw$ give $\frac{1}{2}$ token to $w$.
\end{enumerate}
\begin{itemize}
\item<2-> Clearly each edge does not give away more than it has, and each edge in $T$ receives a token.
\item<3-> Each vertex in $W$ receives a token since there are at least two adjacent edges not in $T$. 
\item<4-> As in the bipartite matching argument, we can find $v \not\in W$ and the edge $vn^*(v)$ does not give away all of its token. 
\end{itemize}
\uncover<4->{A contradiction.$\blacksquare$}
\end{frame}
\subsection{Algorithm?}
\begin{frame}
\frametitle{Iterative Algorithm}
\begin{enumerate}
\item Input bipartite stable matching instance $G$
\item Set matching $M$ to $\emptyset$
\item While $E \neq \emptyset$
\begin{enumerate}
\item Find extreme point optimal solution $x$ to $LP_M(G)$.
\item For each $e=vw \in E$ such that $x_e = 1$ add $e$ to $M$ and remove vertices $v,w$ from $G$.
\item For each $e \in E$ such that $x_e = 0$ remove $e$ from $G$.
\end{enumerate}
\item Output $M$
\end{enumerate}
\uncover<2->{But this doesn't work!\\ $LP_{SM}(G-e)$ loses stability constraint for $e$. So stable matching returned by algorithm on $G-e$, $M'$, comes with no guarantee that $M' \cup \{e\}$ is a stable matching on $G$}
\end{frame}
\begin{frame}
\frametitle{Can We Fix The Algorithm? - Edges of value $1$}
When we find $x_{vw} = 1$ we can be clever and do the following:
\begin{enumerate}
\item $\forall u \in N(v)$ (the neighbourhood of $v$) such that $v$ prefers $u$ to $w$ remove every edge in $\delta^{<v}(u)$ from $G$
\item Do similarly for $w$.
\item Remove $v, w$ from $G$.
\end{enumerate}
It can be shown that this ensures the matching returned on the residual graph plus edge $vw$ is a stable matching on $G$.\end{frame}
\begin{frame}
\frametitle{Can We Fix The Algorithm? - Edges of value $0$}
Unfortunately it is not clear what to do in the case where edges of value $0$ are found.\\
Questions to think about:
\begin{itemize}
\item Can we guarantee an edge of value $1$ to avoid this?
\item Is there a clever set of edges to remove alongside a $0$ edge to ensure the algorithm works?
\end{itemize}
\end{frame}
\section{References}
\begin{frame}
\frametitle{References}
\begin{enumerate}
\item Rothblum, Uriel G. "Characterization of stable matchings as extreme points of a polytope." $\textit{Mathematical Programming}$ 54.1-3 (1992): 57-67.
\item Lau, Lap Chi, Ramamoorthi Ravi, and Mohit Singh. $\textit{Iterative methods in combinatorial optimization}$. Vol. 46. Cambridge University Press, 2011
\end{enumerate}
\end{frame}
\end{comment}
\end{document}
