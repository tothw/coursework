\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\rhead{William Justin Toth 671 Assignment 1} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section{}
\paragraph{}
Suppose $||\cdot||$ is a norm on $\R^n$ and let $\cM$ denote the space of real $n \times n$ matrices. Consider the function $$\nu(A) = \max_{||x||=1} ||Ax||.$$
We will show it is well defined and a norm on the space of matrices.
\paragraph{}
To show $\nu(A)$ is well defined we will demonstrate that $\nu(A)$ optimizes a continuous function over a compact set. Then, since any continuous function optimized over a compact set attains its maximum at a point in the set, we may conclude that $\nu(A)$ indeed is a function. Let $U = \{x \in \R^n: ||x|| = 1\}$, let $A\in \cM$ and let $f(x) = ||Ax||$ for any $x \in \R^n$. The set $U$ is bounded as by definition for any $x \in U$, $||x|| \leq 1$. Consider the complement of $U$, $$U^c = \R^n \backslash U = \{x \in \R^n: ||x|| \neq 1\}.$$ Let $a \in U^c$.
\subparagraph{}
If $||a|| < 1$ then choose $\epsilon = 1 - ||a||$. Let $x \in \{x \in \R^n: ||x-a|| < \epsilon\}$. Then we have that
\begin{align*}
||x|| &= ||x-a + a|| \\
&\leq ||x-a|| + ||a|| \\
&< \epsilon + ||a|| \\
&= 1 - ||a|| + ||a|| \\
&= 1.
\end{align*} 
Since $||x|| \neq 1$, $x\in U^c$.
\subparagraph{}
If $||a|| > 1$ then choose $\epsilon = ||a|| - 1$. Let $x \in \{x \in \R^n: ||x-a|| < \epsilon\}$. Then we have that
\begin{align*}
||x|| &\geq ||a|| - ||x-a|| \\
&> ||a|| - \epsilon \\
&= ||a|| -(||a|| - 1) \\
&= 1.
\end{align*}
Since $||x|| \neq 1$, $x \in U^c$. So in either case, $U^c$ is open and therefore $U$ is closed. 
Since $U$ is a closed, bounded subset of $\R^n$, $U$ is compact.
\paragraph{}
Now we demonstrate that $f$ is continuous. Let $y \in \R^n$. Let $\epsilon > 0 $. Choose $\delta = \epsilon$. Let $x \in \R^n$ and suppose that $||x - y|| < \delta = \epsilon$. Then we have that
\begin{align*}
|f(x) - f(y)| &= |f(x - y + y) - f(y)| \\
&\leq |f(x-y) + f(y) -f(y)| &\text{by triangle inequality applied to $f$}\\
&= f(x-y) \\
&= ||x-y|| \\
&< \epsilon.
\end{align*}
Therefore $f$ is continuous. Since $f$ is continuous over $U$ there exists $c \in U$ such that $f(x) \leq f(c)$ for all $x \in U$. Such $f(c)$ is the unique value of $\max_{||x|| =1} ||Ax||$, and hence $\nu(A)$ is well defined.
\paragraph{}
Now we will show that $\nu$ is a norm. To show this we will demonstrate that $\nu$ is everywhere non-negative and equals $0$ only at the $0$ vector, $\nu$ is absolute scalable, and $\nu$ satisfies the triangle inequality.
\subparagraph{}
First observe that
$$\nu(A) = \max_{||x||=1} ||Ax|| \geq \max_{||x||=1} 0 = 0.$$
So $\nu(A) \geq 0$ for any $A$. Now let $A \in \cM$ and suppose that $\nu(A) = 0$. That is,
$$0 = \max_{||x|| = 1} ||Ax||.$$
Since $\nu(A)$ is non-negative this implies that $||Ax|| = 0$ for any $x \in U$.
We will show $A$ is the $0$ matrix by showing its product with every non-zero $z \in \R^n$ is $0$. We will let $x = \frac{z}{||z||}$ be the normalization of $z$. We have that
\begin{align*}
||Az|| &= ||(||z||)\cdot \frac{Az}{||z||}|| \\
&=||z|| \cdot ||A\frac{z}{||z||}|| \\
&= ||z||\cdot ||Ax|| \\
&= 0 &\text{since $x \in U$}.
\end{align*}
Therefore $||Az|| = 0$ for any non-zero $z \in \R^n$. Since $||\cdot||$ is norm, this implies $Az = 0$ for every $z \in \R^n$. That is to say $A=0$.
\subparagraph{}
Now we consider absolute scalability. Let $c \in \R$ and let $A \in \cM$. Then
\begin{align*}
\nu(cA) &= \max_{||x||=1} ||cAx|| \\
&= \max_{||x||=1} |c|\cdot||Ax|| &\text{since $||\cdot||$ is a norm}\\
&= |c| \max_{||x||=1} ||Ax|| \\
&= |c| \nu(A)
\end{align*}
as desired.
\subparagraph{}
Finally we have the triangle inequality. Let $A,B \in \cM$. Then
\begin{align*}
\nu(A + B) &= \max_{||x||=1} ||(A+B)x|| \\
&\leq \max_{||x||=1} (||Ax|| + ||Bx||) &\text{since $||\cdot||$ is a norm}\\
&= \max_{||x||=1} ||Ax|| + \max_{||x||=1} ||Bx|| \\
&= \nu(A) + \nu(B).
\end{align*}
So $\nu(A+B) \leq \nu(A) + \nu(B)$. Therefore $\nu$ satisfies the triangle inequality, and so is a norm. $\blacksquare$

\section{}
\paragraph{}
Let $|| \cdot ||$ be a norm on $\R^n$ and let $\nu$ be an induced norm on the space $\cM$ of real $n \times n$ matrices. Let $A, B \in \cM$. Then
\begin{align*}
\nu(AB) &= \max_{||x|| = 1} ||ABx|| \\
&= \max_{||x|| = 1} ||A\frac{Bx}{||Bx||}||\cdot ||Bx||\\
&\leq \max_{||x|| = 1} \max_{||v|| = 1} ||Av|| \cdot ||Bx||\\
&= \max_{||v|| =1} ||Av||\cdot \max_{||x|| = 1} ||Bx|| \\
&=\nu(A) \nu(B).
\end{align*}
Therefore $\nu(AB) \leq \nu(A) \nu(B)$ as desired.$\blacksquare$

\section{}
\paragraph{}
Suppose $M \succcurlyeq 0$ and $x^TMx=0$. Since $M$ is positive semidefinite, there exists a matrix $B$ such that
$$M = B^TB.$$
Therefore $$0 = x^TMx = x^TB^TBx = (Bx)^T(Bx) = ||Bx||_2^2$$
and thus, as the $2$-norm of $Bx$ is zero, $Bx = 0$. Since $M = B^TB$, we may conclude that $$Mx = B^T(Bx) = B^T0 = 0.$$$\blacksquare$

\section{}
\subsection{}
\paragraph{}
Let $A$ and $B$ be $n \times n$ matrices. Suppose that $A$ is positive definite and $B$ is positive semidefinite. Let $x \in \R^n$, such that $x \neq 0$. Then
\begin{align*}
x^T(A + B)x &= x^TAx + x^TBx \\
&\geq x^TAx +0 &\text{since $B$ is positive semidefinite}\\
&> 0. &\text{since $A$ is positive definite and $x\neq 0$}\end{align*}
Therefore $A+B$ is positive definite. $\blacksquare$
\subsection{}
\paragraph{}
Let $B$ be an $n \times n$, $01$-matrix. Suppose that there exist positive integers $n$ and $\lambda$ such that
$$BB^T = nI + \lambda J.$$
Let $x \in \R^n$ such that $x\neq 0$. Then 
\begin{align*}
x^T BB^T x &= x^T(nx + \lambda Jx) \\
&=\sum_{i=1}^n x_i(nx_i + \lambda \sum_{j=1}^n x_j) &\text{as each $(Jx)_i$ is equal to $\sum_{j=1}^n x_j$} \\
&= n\sum_{i=1}^n x_i^2 + \lambda (\sum_{i=1}^n x_i)^2.
\end{align*}
Observe that $\sum_{i=1}^n x_i^2 > 0$. This follows since $x \neq 0$ implies there is at least one $x_i \neq 0$. Thus $n \sum_{i=1}^n x_i^2 >0$ as $n >0$. We also have $\lambda (\sum_{i=1}^n x_i)^2 \geq 0$ as $\lambda >0$ and $(\sum_{i=1}^n x_i))^2 \geq 0$. Therefore we may conclude that $x^TBB^Tx >0$ for any $x \neq 0$. That is, $BB^T$ is positive definite.
\paragraph{}
Since $BB^T$ is positive definite it is non-singular. Therefore,
\begin{align*}
&\det(BB^T) \neq 0 \\
\implies &\det(B)\det(B^T) \neq 0 \\
\implies &\det(B)^2 \neq 0 \\
\implies &\det(B) \neq 0. 
\end{align*}
Thus $B$ is non-singular, and so the rows or $B$ are linearly independent. $\blacksquare$

\section{}
\paragraph{}
Let $V$ be a real inner product space of dimension $n$. Let $x_1, \dots, x_d$ be elements of $V$. Let $G$ be the $d \times d$ matrix given by $$ G_{i,j} = \left \langle x_i, x_j \rangle \right.$$ Let $y \in V$. Let $||\cdot ||$ be the norm induced by $\langle \cdot,\cdot \rangle$. Then we have that
\begin{align*}
y^TGy &= \sum_{i=1}^n\sum_{j=1}^n \langle x_i,x_j \rangle y_i y_j \\
&= \sum_{i=1}^n \sum_{j = 1}^n  \langle x_i y_i, x_j y_j \rangle  &\text{by linearity of real inner product in each element} \\
&= \langle \sum_{i=1}^n x_i y_i, \sum_{j=1}^n x_j y_j \rangle &\text{again by linearity} \\
&= ||\sum_{i=1}^n x_i y_i ||^2 \numberthis\label{eq:norm}\\
&\geq 0.
\end{align*}
Therefore $G$ is positive semidefinite.
\paragraph{}
Now, by (\ref{eq:norm}), there exists $y\neq 0$ such that $y^TGy = 0$ if and only if
\begin{equation}\sum_{i=1}^n x_i y_i = 0. \label{eq:li}\end{equation}
So by definition of linear dependence  (\ref{eq:li}) holds if and only if $x_1, \dots, x_d$ are linearly dependent. Therefore $G$ is positive definite if and only if $x_1, \dots, x_d$ are linearly independent. $\blacksquare$
\end{document}
