\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\rhead{William Justin Toth 671 A2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section*{6}
\paragraph{}
Let $A, B \succcurlyeq 0$. Then there exist $X, Y$ such that $$A = X^TX$$ and $$B = Y^TY.$$
Then we have that
\begin{align*}
A \otimes B &= X^TX \otimes Y^TY \\
&= (X^T \otimes Y^T)(X \otimes Y) \\
&= (X \otimes Y)^T(X \otimes Y).
\end{align*}
Therefore  there exist $W = X \otimes Y$ such that $$A \otimes B = W^TW.$$
and hence $A \otimes B \succcurlyeq 0$. $\blacksquare$
\section*{7}
\paragraph{}
Let $A, X, B$ be matrices such that $AXB^T$ is defined. Suppose $A$ is  of order $m \times n$, $X$ of order $n \times p$, and $B$ of order $q \times p$. Think of the rows of $B \otimes A$ as $q$ sequential blocks of $m$ rows. Consider the $i^\text{th}$ block of rows $(B \otimes A)vec(X)$:
\begin{equation}\begin{bmatrix} b_{i1} A & \cdots & b_{ip}A \end{bmatrix}
\begin{bmatrix}
x_1 \\ \vdots \\ x_p
\end{bmatrix} \label{eq:iblock}\end{equation}
where $x_1, \dots, x_p$ are the column vectors of $X$ (that is, $X = \begin{bmatrix} x_1 & \cdots & x_p\end{bmatrix}$). Then (\ref{eq:iblock}) is equal to
\begin{equation} \sum_{j=1}^p b_{ij}Ax_j.  \label{eq:tensorsum}\end{equation}
\paragraph{}
Observe that the $i^\text{th}$ block of $m$ rows of $vec(AXB^T)$ is equal to the $i^\text{th}$ column of $AXB^T$. Thus we need to show that (\ref{eq:tensorsum}) is equal to the $i^\text{th}$ column of $AXB^T$. Let $b_i^T$ denote the $i^{th}$ row of $B$. Then we have that
\begin{align*}
(AXB^T)_i &= (A\begin{bmatrix} x_1 & \cdots & x_p\end{bmatrix}B^T)_i \\
&=(\begin{bmatrix} Ax_1 &\cdots & Ax_p \end{bmatrix} B^T)_i \\
&= \begin{bmatrix} Ax_1 &\cdots & Ax_p \end{bmatrix} b_i \\
&= \sum_{j=1}^p b_{ij}Ax_j \\
&= (\ref{eq:tensorsum}). 
\end{align*}
Therefore $vec(AXB^T) = (B \otimes A)vec(X)$ as desired. $\blacksquare$
\section*{8}
\paragraph{Lemma 8.1}
Let $A$ be a $m\times n$ matrices and $B$ be an $n \times n$ nonsingular matrix. Then $rk(AB) = rk(A)$.
\paragraph{Proof of Lemma 8.1}
Since rank is equivalent to the dimension of the column space we will show that $col(AB) = col(A)$. Indeed, since $col(B) = \R^n$ we have that
\begin{align*}
col(AB) = \{ABx : x \in \R^n\} = \{Ax : x \in \R^n\} = col(A).
\end{align*}
Therefore $rk(AB) = rk(A)$.$\blacksquare$
\paragraph{Note}
We similarly have $rk(AB) = rk(B)$ for nonsingular $A$. The proof simply changes to reasoning about the transpose.
\paragraph{Lemma 8.2}
Let $A,B$ be nonsingular matrices. Then $A \otimes B$ is nonsingular.
\paragraph{Proof of Lemma 8.2}
We demonstrate that $A^{-1} \otimes B^{-1}$ is the inverse of $A \otimes B$ to obtain the result:
$$(A \otimes B)(A^{-1} \otimes B^{-1}) = (AA^{-1} \otimes BB^{-1}) = (I \otimes I) = I $$
where we use $I$ to mean the identity of the appropriate order which should be clear by context. $\blacksquare$
\paragraph{Problem 8}
Let $A$ be a matrix of rank $r$ and let $B$ be a matrix of rank $q$. Our goal is to reduce the proof to the case of diagonal matrices for which the result is more clearly true. Let $U_AD_AV_A^*$ be the singular value decomposition of $A$. Let $U_BD_BV_B^*$ be the singular value decomposition of $B$. Then by Lemma $8.1$,
$$rk(A) = rk(D_A) \quad \text{and} \quad rk(B) = rk(D_B).$$ 
Now we see that
\begin{align*}
rk(A\otimes B) &= rk(U_AD_AV_A^* \otimes U_BD_BV_B^*) \\
&= rk((U_A \otimes U_B)(D_A \otimes D_B)(V_A^* \otimes V_B^*)) \\
&= rk(D_A \otimes D_B).
\end{align*}
The last equality follows from three observations. First $U_A$ and $U_B$ are nonsingular implies $U_A \otimes U_B$ is nonsingular by Lemma $8.2$. Second, similarly $V_A^* \otimes V_B^*$ is nonsingular. Finally those two observations come together with two applications of Lemma $8.1$ to obtain the last equality.
\paragraph{}
Therefore the problem is reduced to the question:
$$rk(D_A \otimes D_B) = rk(D_A)rk(D_B) $$
for diagonal matrices $D_A$ and $D_B$. But this is obvious, as $D_A \otimes D_B$ is a diagonal matrix with diagonal entries of the form $d_Ad_B$ for $d_A$ on the diagonal of $D_A$ and $d_B$ on the diagonal of $D_B$. There are $rk(D_A)$ nonzero $d_A$ and $rk(D_B)$ nonzero $d_B$ and thus $rk(D_A)rk(D_B)$ nonzero diagonal entries of $D_A \otimes D_B$. Since $D_A \otimes D_B$ is diagonal we have $rk(D_A \otimes D_B) = rk(D_A)rk(D_B)$ and hence $rk(A \otimes B) = rk(A)rk(B)$. $\blacksquare$
\section*{9}
Let $x_1, \dots, x_n$ be a set of unit vectors in $\R^d$ such that for some scalar $\alpha$ with $0 \leq \alpha < 1$ we have $|\langle x_i, x_j \rangle | = \alpha$ whenever $i \neq j$. Let $P_i = x_i x_i^T$ for each $i$. Let $G$ be the matrix of the matrices $P_i$ with inner product $\langle P_i, P_j \rangle = tr(P_i^T P_j)$. Consider an entry of $G$, $G_{ij}$:
\begin{align*}
G_{ij} &= tr(P_i^T P_j) \\
&= tr(x_i x_i^T x_j x_j^T) \\
&= tr(x_i \langle x_i, x_j \rangle x_j^T) \\
&= \langle x_i, x_j \rangle tr(x_i x_j^T) \\
&= \langle x_i, x_j \rangle \sum_{k=1}^d (x_i)_d (x_j)_d \\
&= \langle x_i, x_j \rangle ^2\\
&= \begin{cases}
1, \text{if $i=j$}\\
\alpha^2, \text{if $i \neq j$}
\end{cases}. \numberthis \label{eq:onealpha}
\end{align*}
Let $y \in \R^n$ such that $y \neq 0$. Then we have that
\begin{align*}
y^TGy &= \sum_{i} \sum_{j} G_{ij} y_i y_j\\
&= \sum_{i} [y_i^2 + \alpha^2(\sum_{j \neq i} y_iy_j)] &\text{by (\ref{eq:onealpha})}\\
&= (1-\alpha^2)\sum_{i}y_i^2 + \alpha^2\sum_i\sum_j y_iy_j \\
&=(1-\alpha^2)\sum_i y_i^2 + \alpha^2 \begin{bmatrix}1 & \cdots & 1 \end{bmatrix}yy^T\begin{bmatrix}1\\\vdots\\1\end{bmatrix} \\
&\geq (1-\alpha^2)\sum_i y_i^2 &\text{since $yy^T \succcurlyeq 0$} \\
&> 0. &\text{since $1-\alpha^2\geq 0$ and $\sum_i y_i^2 >0$ as $y \neq 0$}
\end{align*}
Therefore $G$ is positive definite. By problem $5$ of Assignment $1$ this implies $P_1, \dots, P_n$ are linearly independent in the vector space $V$ where $V$ denotes the space of real symmetric $d \times d$ matrices. But by a basic fact of linear algebra, the dimension of $V$ is $d+1 \choose 2$ (to see this, observe that $\{e_ie_j^T + e_je_i^T : i \leq j \in \{1,\dots,d\}\}$ forms a basis of $V$ of size $\sum_{k=1}^d k = $$d+1\choose 2$). Thus for $P_1, \dots, P_n$ to be linearly independent there must be at most $d+1 \choose 2$ of them. That is to say $n \leq $$d+1 \choose 2$. $\blacksquare$
\section*{10}
\paragraph{}
Let $A \succcurlyeq 0$ and let $x,y$ be vectors for which $(x+ty)^TA(x+ty)$ is defined. We aim to characterize $t$ for which
\begin{equation}(x+ty)^TA(x+ty) = 0\label{eq:goal}.\end{equation} Observe the following:
\begin{align*}
(x+ty)^TA(x+ty) &= x^TAx + x^TAty + ty^TAx + t^2y^TAy \\
&= x^TAx + t(x^TAy + y^TAx) + t^2y^TAy. \numberthis \label{eq:quad}
\end{align*}
If we let $a,b,c$ be scalars such that $a = y^TAy$, $b=x^TAy + y^TAx$, and $c = x^TAx$ then (\ref{eq:quad}) is equal to
$$at^2 + bt + c.$$
Thus we must resolve when $at^2 + bt +c =0$. This is a simple matter of high school algebra. We describe the cases below for completeness.
\paragraph{Case $a \neq 0$}
If $a \neq 0$ then we may apply the quadratic formula. If $b^2-4ac <0$ then (\ref{eq:goal}) never holds Otherwise (\ref{eq:goal}) holds when
$$t = \frac{-b \pm \sqrt{b^2-4ac}}{2a}.$$
\paragraph{Case $a = 0$}
If $a = 0$ we are solving a linear equation. If $b \neq 0$ then (\ref{eq:goal}) holds when $$t = \frac{-c}{b}.$$ Otherwise, if $b = 0$, (\ref{eq:goal}) holds for all $t$ provided $c=0$ and never holds when $c\neq 0$.$\blacksquare$
\end{document}
