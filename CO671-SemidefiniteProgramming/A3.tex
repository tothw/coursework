\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\DeclareMathOperator{\conv}{conv}
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\rhead{William Justin Toth 671 A3} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section*{11}
\paragraph{}
Let $A, B$ be $n \times n$ symmetric matrices of rank $1$ such $A$ is not a scalar multiple of $B$. Since $A$ and $B$ are rank $1$ symmetric matrices there exists $a, b \in \R^n$ such that $$A = aa^T \quad \text{and}\quad B = bb^T.$$
We will show that $\{a,b\}$ is a basis for the column space of $A+B$ and thus $rk(A+B) = 2 > 1$ as desired.
\paragraph{}
First observe that $a$ and $b$ are linearly independent. To see this, suppose that there exists $t$ for which $a = tb$. Then we have that $$A = aa^T = t^2bb^T = t^2B.$$
Which contradicts that $A$ is not a scalar multiple of $B$.
\paragraph{}
Now to show that $span\{a,b\} = col(A+B)$. We claim that for any $\alpha a + \beta b \in span \{a,b\}$ there exists $x \in \R^n$ such that $\alpha = a^Tx$ and $\beta = b^Tx$. Consider the system
\begin{equation} \begin{bmatrix} a^T \\b^T\end{bmatrix} x = \begin{bmatrix}\alpha \\ \beta\end{bmatrix}. \label{sys:star}\end{equation}
If (\ref{sys:star}) has no solutions then $a$ and $b$ are collinear. This contradicts that $a$ and $b$ are linearly independent. Thus (\ref{sys:star}) has a solution, and hence the claim holds. Now using the claim we may obtain that $span\{a,b\} = col(A+B)$ as follows. Let $\alpha a + \beta b \in span\{a,b\}$. Then there exists $x \in \R^n$ such that $$\alpha a + \beta b = aa^Tx + bb^Tx + (A+B)x.$$
Therefore $\alpha a + \beta b \in col(A+B)$. So $span\{a,b\} \subseteq col(A+B)$. Now let $(A+B)x \in col(A+B)$. Then $$(A+B)x = (a^Tx)a + (b^Tx)b$$
and so $(A+B)x \in span\{a,b\}.$ Therefore $span\{a,b\} = col(A+B)$, so we may conclude that $\{a,b\}$ is a basis for $col(A+B)$ and hence $rk(A+B) > 1$. $\blacksquare$
\section*{12}
\paragraph{}
Let $A$ and $B$ be $n \times n$ positive semidefinite matrices. We aim to show that the Schur product
$A \circ B$ is positive semidefinite. We will proceed by a series of claims. The first claim will show that $A\circ B$ is a principle submatrix of $A \otimes B$. Then we will show that any principle submatrix of of a positive semidefinite matrix is positive semidefinite. From there, since we know by problem $6$ that $A \otimes B$ is positive semidefinite we can conclude that $A \circ B$ is positive semidefinite.
\paragraph{Claim 1}
We claim that $A \circ B$ is a principle submatrix of $A \otimes B$. Formally, there exists $I \subseteq \{1, \dots, n\}$ such that $(A\otimes B)_I = A\circ B$. As a matter of notation, for any matrix $M$ by $M_I$ we mean the submatrix of $M$ consisting of rows indexed by $I$ and columns indexed by $I$. Observe that, by definition:
\begin{align*}
(A \circ B)_{i,j} &= A_{i,j}B_{i,j} \\
&= (A \otimes B)_{n(i-1) + i, n(j-1) + j} \\
&= (A \otimes B)_{(n+1)i -n, (n+1)j - n}.
\end{align*}
Thus if we let $I = \{(n+1)i - n: i \in \{1, \dots, n\}\}$ then $$A\circ B = (A\otimes B)_{I,I}.$$ So $A \circ B$ is a principle submatrix of $A\otimes B$ and the claim holds.
\paragraph{Claim 2}
Now we claim that any principle submatrix of a positive semidefinite matrix is positive semidefinite. Let $M$ be an $n \times n$ positive semidefinite matrix. Let $I \subseteq \{1,\dots, n\}$. Since $M$ is positive semidefinite $M$ is a Gram matrix. Let $m_1, \dots, m_n$ be the vectors describing the Gram matrix $M$. That is to say,
$$M_{i,j} = \langle m_i,m_j\rangle.$$
Then we have that $M_I$ is a Gram matrix formed by vectors $m \in \{m_i : i \in I\}$. Since Gram matrices are positive semidefinite, $M_I$ is positive semidefinite and the claim holds.
\paragraph{Finishing the problem}
Thus we have by problem $6$ that $A \otimes B$ is positive semidefinite as $A$ and $B$ are positive semidefinite. Now by Claim $1$ $A \circ B$ is principle submatrix of $A\otimes B$, and since $A \otimes B$ is positive semidefinite by Claim $2$ $A \circ B$ is positive semidefinite as desired. $\blacksquare$
\section*{13}
\paragraph{}
Let $S$ be a subspace of a real vector space of dimension $d$. Let $\conv(S)$ denote the convex hull of $S$. Let $x \in \conv(S)$. Then $x$ is a convex combination of vectors in $S$. Let $k$ be the minimum natural number for which
$$\sum_{i=1}^k a_i x_i = x$$
where $\sum_{i=1}a_i = 1$, each $a_i \geq 0$, and each $x_i \in S$. Suppose for a contradiction that $k > d+1$. Then $k-1 >d$ and since the dimension of $S$ is $d$, the vectors
$$x_2 - x_1, \dots, x_k -x_1$$
are linearly dependent. Then there exists $\lambda_2, \dots, \lambda_k$ not all zero for which
$$0 = \sum_{i=2}^k \lambda_i (x_i-x_1) = \sum_{i=2}^k \lambda_i x_i - \sum_{i=2}\lambda_i x_1.$$
So if we let $\lambda_1 = -\sum_{i=2}^k\lambda_i$ then
$$ \sum_{i=1}^k \lambda_i x_i = 0 \quad\text{and}\quad \sum_{i=1}^k \lambda_i = 0.$$
Notice that $\sum_{i=1}^k \lambda_i = 0$ yet not all $\lambda_i$ are zero, so there exists $\lambda_i >0$. Now for any scalar $\alpha$ we have that
$$x = \sum_{i=1}^k a_i x_i + \alpha \sum_{i=1}^k \lambda_i x_i = \sum_{i=1}^k (a_i - \alpha\lambda_i)x_i.$$
In particular we may choose
$$\alpha = \min\{\frac{a_i}{\lambda_i} : \lambda_i >0\}.$$
Since there exists $\lambda_i >0$ this choice of $\alpha$ is well defined. Let $i$ denote the index of the minimizing $\lambda_i$. Furthermore for any $j \in \{1, \dots, k\}$ we have
\begin{align*}
a_j - \alpha \lambda_j &= a_j - \frac{a_I}{\lambda_i} \lambda_i \\
&\geq a_j - \frac{a_j}{\lambda_j}\lambda_j \\
&= 0.
\end{align*}
In particular $a_i - \alpha \lambda_i = 0$. Also observe that $$\sum_{j=1}^k (a_j - \alpha \lambda_j) = \sum_{j=1}^k a_j - \alpha \sum_{j=1}^k \lambda_j = 1.$$
Therefore $$x = \sum_{j=1}^{i-1} (a_j - \alpha \lambda_j)x_j + \sum_{j=i+1}^{k} (a_j - \alpha \lambda_j)x_j.$$
That is, $x$ is a convex combination of $x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_k$. So $x$ can be written as a convex combination of $k-1$ vectors and this contradicts the minimality of $k$. Thus $k \leq d+1$ as desired. $\blacksquare$
\section*{14}
\paragraph{}
Let $A$ and $B$ be convex sets. Let $A+B$ denote their Minkowski sum, defined by:
$$A+B = \{x+y : x\in A\text{ and } y\in B \}.$$
Let $z_1, \dots, z_n \in A+B$ and let $\alpha_1, \dots, \alpha_n$ be scalars such that each $\alpha_i \geq 0$ and $\sum_{i=1}^n \alpha_i = 1$. By the definition of $A+B$, for each $z_i$ there exists $x_i \in A$ and $y_i \in B$ such that $$z_i = x_i + y_i.$$
Since $A$ is convex observe that
\begin{equation}
\sum_{i=1}^n \alpha_i x_i \in A
\label{eq:convA}
\end{equation}
and similarly since $B$ is convex
\begin{equation}
\sum_{i=1}^n \alpha_i y_i \in B.
\label{eq:convB}
\end{equation}
Let $a = \sum_{i=1}^n \alpha_i x_i$ and let $b = \sum_{i=1}^n \alpha_i y_i$. By (\ref{eq:convA}) $a\in A$ and by (\ref{eq:convB}) $b \in B$. Now we have that
\begin{align*}
\sum_{i=1}^n \alpha_i z_i &= \sum_{i=1}^n \alpha_i(x_i + y_i) \\
&= \sum_{i=1}^n \alpha_i x_i + \sum_{i=1}^n \alpha_i y_i \\
&= a + b.
\end{align*}
Hence we may conclude that $\sum_{i=1}^n \alpha_i z_i \in A + B$, and therefore $A+B$ is convex. $\blacksquare$
\section*{15}
\paragraph{}
Let $f$ be a non-constant complex polynomial of degree $n$. Then there exists $z_1, \dots, z_n$ and $a \in \C$ such that we may write $f$ as:
$$f(z) = a\Pi_{i=1}^n(z-z_i).$$
Then the derivative of $f$, denoted $f'$, is
$$f'(z) = a\sum_{i=1}^n \Pi_{j\neq i} (z-z_j). $$
So the ratio $\frac{f'}{f}$ is given by:
$$ \frac{f'}{f}(z) = \frac{a\sum_{i=1}^n \Pi_{j\neq i} (z-z_j)}{a\Pi_{i=1}^n(z-z_i)} = \sum_{i=1}^n \frac{1}{z-z_i}.$$
Let $z_0$ be a root of $f'$. If $z_0$ is a root of $f$ then it is trivially contained in the convex hull of the roots of $f$. So we may assume that $z_0$ is not a root of $f$. That is, $f(z_0) \neq 0$. Therefore we have
$$ 0 = \frac{f'}{f}(z_0) = \sum_{i=1}^n\frac{1}{z_0-z_i}.$$
Multiplying each $i^\text{th}$ term by $1 = \overline{z_0-z_i}/\overline{z_0-z_i}$. we obtain
$$ 0 = \sum_{i=1}^n\frac{\overline{z_0 - z_i}}{|z_0-z_i|^2}.$$
Using the basic fact of complex variables that $\overline{z_0-z_i} = \overline{z_0}-\overline{z_i}$ we may rewrite this as
$$\sum_{i=1}^n \frac{\overline{z_i}}{|z_0-z_i|^2} = \sum_{i=1}^n\frac{\overline{z_0}}{|z_0-z_i|^2}.$$
Taking complex conjugates gives
$$\sum_{i=1}^n \frac{1}{|z_0-z_i|^2}z_i = (\sum_{i=1}^n \frac{1}{|z_0-z_i|^2}) z_0.$$
Therefore if we let $q = \sum_{i=1}^n \frac{1}{|z_0-z_i|^2}$ and let each $\alpha_i = \frac{1}{q|z_0-z_i|^2}$ then
$$ \sum_{i=1}^n \alpha_i z_i = z_0.$$
So $z_0$ is a linear combination of the zeros of $f$. To see this combination is in fact convex observe that each $\alpha_i \geq 0$ and that:
$$\sum_{i=1}^n \alpha_i = \frac{1}{q} \sum_{i=1}^n \frac{1}{|z_0 - z_i|^2} = \frac{1}{q}\cdot q = 1.$$
Hence $z_0$ is contained in the convex hull of the roots of $f$. $\blacksquare$
\section*{16}
\paragraph{}
Let $C$ be a subset of $\R^d$ and let $\{x_0, \dots, x_e\}$ be an affinely independent subset of $C$ of maximum size. Let $A$ be the affine subspace generated by $x_0, \dots, x_e$. Let $c \in C$. If $c \not\in A$ then \begin{equation}\{x_0, \dots, x_e, c\}\text{ is an affinely independent subset of $C$}\label{claim:ai}\end{equation} contradicting the maximality of $\{x_0, \dots, x_e\}$. Thus $c \in A$ and therefore $C \subseteq A$. To see (\ref{claim:ai}) observe that if, to the contrary, $\{x_0, \dots, x_e,c\}$ is affinely dependent then there exists $\alpha_0, \dots, \alpha_e, \beta$ not all zero, and in particular $\beta \neq 0$ as otherwise $\{x_0, \dots, x_e\}$ is affinely dependent, such that $$\beta c + \sum_{i=0}^e \alpha_e x_e = 0 \quad \text{ and } \quad \beta + \sum_{i=0}^e \alpha_e = 0.$$
Then we have that $\beta = -\sum_{i=0}^e \alpha_e$, so if we let $\gamma_i = \alpha_i/\beta$ for $i \in \{0,\dots,e\}$ we have
$$\sum_{i=0}^e \gamma_e x_e = c \quad \text{ and } \quad \sum_{i=0}^e \gamma_e = \frac{-1}{\beta}\sum_{i=0}^e \alpha_e = 1.$$
Which implies that $c \in A$, a contradiction.
\paragraph{}
It remains to show that the affine dimension of $A$ is $e$. Indeed since $\{x_0, \dots, x_e\}$ is an affinely independent subset of $A$ the affine dimension of $A$ is at least $e$. Suppose for a contradiction that the affine dimension of $A$ is greater than $e$. Then there exists an affinely independent subset of $A$ of size $e+1$, say $\{y_0, \dots, y_{e+1}\}$. We claim that by induction for any $k$, $\{y_0, \dots, y_k, x_{k+1}, \dots, x_e\}$ affinely spans $A$ (up to relabelling indices of $x_i$'s). For the base consider $k=0$. Since $\{x_0, \dots, x_e\}$ spans $A$ there exist $\alpha_i$, not all zero, such that $$\sum_{i=0}^e \alpha_i x_i = y_0 \quad\text{ and }\quad \sum_{i=0}^e \alpha_i = 1.$$
Suppose without loss of generality that $\alpha_0 \neq 0$. Then rearranging we obtain:
$$x_0 = \frac{-1}{\alpha_0}y_0 + \sum_{i=1}^e \frac{\alpha_i}{\alpha_0} x_i$$
and further that
$$\frac{-1}{\alpha_0} + \sum_{i=1}^e \frac{\alpha_i}{\alpha_0} = \frac{1}{\alpha_0}(-1+\sum_{i=1}^e \alpha_i) = \frac{1}{\alpha_0}\alpha_0 = 1.$$
Therefore $x_0$ is an affine combination of $\{y_0, x_1, \dots, x_e\}$. Since each vector in $\{x_0, \dots, x_e\}$, which spans $A$, can be obtained from affine combinations of vectors in $\{y_0, x_1, \dots, x_e\}$ 
\paragraph{}
Now for induction let $k>0$ and suppose the claim holds for smaller $k$. That is, $\{y_0, \dots, y_{k-1}, x_k, \dots, x_e\}$ spans $A$. So $y_k$ can be written as an affine combination of $y_0, \dots, y_{k-1}, x_k, \dots, x_e$. That is there exists $\alpha_0, \dots, \alpha_e$ satisfying
$$ y_k = \sum_{i=0}^{k-1} \alpha_iy_i + \sum_{i=k}^e \alpha_ix_i \quad \text{and}\quad \sum_{i=0}^e \alpha_i=1.$$
If each $\alpha_i = 0$ for $i \geq k$ then $y_k$ is an affine combination of $y_0, \dots, y_{k-1}$, which contradicts the affine independence of $\{y_0, \dots, y_{e+1}\}$. So without loss of generality $\alpha_k \neq 0$ and hence we may write $x_k$ as an affine combination of $y_0, \dots, y_k, x_{k+1}, \dots, x_e$,
$$x_k = \frac{1}{\alpha_k}y_k+\sum_{i=0}^{k-1} \frac{-\alpha_{i}}{\alpha_k}y_i + \sum_{i=k+1}^e \frac{-\alpha_i}{\alpha_k}x_i \quad\text{and}\quad \frac{1}{\alpha_k} + \sum_{i=0}^{k-1} \frac{\alpha_i}{\alpha_k} + \sum_{i=k+1}^e \frac{-\alpha_i}{\alpha_k} = \frac{1 - 1 +\alpha_k}{\alpha_k} = 1.$$
Therefore $\{y_0, \dots, y_k, x_{k+1}, \dots, x_e\}$ affinely spans $A$ as desired.
\paragraph{}
So with $k=e$ we conclude that $\{y_0, \dots, y_e\}$ affinely spans $A$. But this contradicts that $\{y_0, \dots, y_{e+1}\}$ is affinely independent. Hence our assumption that the affine dimension of $A$ is greater than $e$ was false. Therefore the affine dimension of $A$ is $e$. $\blacksquare$
\section*{17}
\paragraph{}
Let $\{x_0, \dots, x_e\}$ be an affinely independent subset of $\R^e$. By affine independence, the only solution to the system of equations
$$\sum_{i=0}^e \alpha_e x_e =0 \quad \text{and}\quad \sum_{i=0}^e\alpha_i = 0$$
is the trivial solution $\alpha_0 = \dots = \alpha_e = 0$. So the matrix $$A=\begin{bmatrix} x_0 &\dots & x_e\\ 1 & \dots & 1\end{bmatrix}$$
is nonsingular. For any $x \in \R^e$ observe that $\alpha=A^{-1}\begin{bmatrix}x\\1\end{bmatrix}$ is a vector of coefficients $\alpha_0, \dots, \alpha_e$ with which $x$ can be written as an affine combination of $x_0, \dots, x_e$.
\paragraph{}
Let $f : \R^e \rightarrow \R^{e+1}$ be the map given by $$f(x) = A^{-1}\begin{bmatrix}x\\1\end{bmatrix}.$$ We claim that $f$ is uniformly continuous. Let $\epsilon >0$. Let $||\cdot || $ denote a vector norm (or the induced matrix norm when applied to a matrix). Let $(A^{-1})_{1\rightarrow e}$ denote the first $e$ columns of $A^{-1}$. Choose $$\delta = \frac{\epsilon}{||(A^{-1})_{1\rightarrow e}||} > 0.$$ Since $A$ has trivial nullspace this choice of delta is well-defined. Now let $x,y \in \R^e$ such that $||x-y|| < \epsilon$. Then
\begin{align*}
||f(x) - f(y)|| &= ||(A^{-1})_{1\rightarrow e}(x-y)|| \\
&\leq ||(A^{-1})_{1\rightarrow e}|| \cdot ||x-y||  &\text{(proven in the course of problem $1$ that $||Ax|| \leq ||A|| \cdot ||x||$)}\\
&< ||(A^{-1})_{1\rightarrow e}|| \cdot \delta \\
&= \epsilon,
\end{align*}
and hence $f$ is uniformly continuous. Let $R^+ = \{x \in \R^{e+1} : x > 0 \}$. Then $R^+$ is open, and since $f$ is continuous the preimage $f^{-1}(R^+) = B$ is open. Notice every $a \in f(B)$ is a convex combination of $x_0, \dots, x_e$.
\paragraph{}
Now to show that the convex hull of $\{x_0, \dots, x_e\}$ has a nonempty interior. Let $$x = \frac{1}{e+1}\sum_{i=0}^e x_i.$$
Then $x$ is a convex combination of $x_0, \dots, x_e$. Hence $x$ is in the convex hull of $x_0, \dots, x_e$, and it remains to show that we can put an open ball around $x$ that remains contained in the convex hull.
\paragraph{}
 Now $$y = \frac{1}{e+1}\begin{bmatrix}1\\ \vdots \\ 1\end{bmatrix} \in \R^{e+1}$$ is in $R^+$ and $f(x) = y$. So $x \in B$. Since $B$ is open there exists $\epsilon >0$ such that $$B_\epsilon(x) = \{a \in \R^e : ||x-a|| < \epsilon\} \subseteq B.$$
Since $B_\epsilon(x) \subseteq B$, $f(B_\epsilon(x)) \subseteq \R^+$. Therefore each $a \in B_\epsilon(x)$ is a convex combination of $x_0, \dots, x_e$ and hence $x$ is in the interior of the convex hull of $x_0, \dots, x_e$ and so the convex hull is nonempty. $\blacksquare$
\section*{19}
\paragraph{}
Let $C$ be a convex set. Let $x,y$ be in the interior of $C$. Let $\alpha$ be a nonnegative scalar. Let $$z = \alpha x + (1-\alpha)y.$$
We will show that $z$ is in the interior of $C$. Let $B_\epsilon(w)$ denote an $\epsilon$ radius ball centred at $w$. Since $x$ and $y$ are in the interior of $C$ there exists $\epsilon_x, \epsilon_y$ such that $B_{\epsilon_x}(x) \subseteq C$ and $B_{\epsilon_y}(y) \subseteq C$. Choose $\epsilon = \min\{\epsilon_x, \epsilon_y\}$. Let $$a \in B_\epsilon(z).$$
That is that $||a-z|| < \epsilon$. We must show that $a \in C$ to complete the proof.
\paragraph{}
Let $x' = x + a-z$ and let $y' = y + a - z$. Now $$||x' - x|| = ||a-z|| < \epsilon \leq \epsilon_x.$$ So $x' \in B_{\epsilon_x}(x)$ and hence $x' \in C$. Similary $$||y'-y|| = ||a-z|| < \epsilon \leq \epsilon_y.$$ So $y' \in B_{\epsilon_y}(y)$ and hence $y' \in C$. Since $C$ is convex if we can show that $a$ is a convex combination of $x'$ and $y'$ then we are done. We claim that $\alpha x' + (1-\alpha)y' = a$ :
\begin{align*}
\alpha x' + (1-\alpha)y' &= \alpha(x + a - z) + (1-\alpha)(y+a-z) \\
&= \alpha x + (1-\alpha)y + a - z \\
&= z + a -z \\
&= a.
\end{align*}
So $a \in C$ as desired. Therefore $B_\epsilon(z) \subseteq C$ and hence $z$ is in the interior of $C$. Therefore the interior of $C$ is closed under taking convex combinations, that is to say the interior of $C$ is convex. $\blacksquare$
\end{document}
