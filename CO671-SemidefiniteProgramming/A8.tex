\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{bbm}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
\newcommand{\1}{\mathbbm{1}} 

\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\rhead{William Justin Toth 671 Assignment 8 - Problem Set 2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section*{10}
\paragraph{}
Let $v_1, \dots, v_n$ be an orthonormal representation of $\overline{G}$ and let $w_1, \dots, w_m$ be an orthonormal representation of $\overline{H}$. We claim that the vectors $v_i \otimes w_j$ for all $i,j$ gives an orthonormal representation of $\overline{G \boxtimes H}$. Let $(t,x), (u,y) \in V(G\boxtimes H)$ such that $(t,x) \sim (u,y)$. First we show that each $v_i \otimes w_j$ is normal:
\begin{align*}
\langle v_i \otimes w_j, v_i \otimes w_j \rangle &= (v_i \otimes w_j)^T (v_i \otimes w_j) \\
&= (v_i^T \otimes w_j^T)(v_i \otimes w_j) \\
&= v_i^Tv_i \otimes w_j^Tw_j \\
&= 1 \otimes 1 &\text{ as $v_i$ and $w_j$ are normal} \\
&= 1.
\end{align*}
Now we need to show that $$\langle v_t \otimes w_x, v_u \otimes w_y \rangle = 0.$$
First observe that:
\begin{align*}\langle v_t \otimes w_x, v_u \otimes w_y \rangle &= (v_t \otimes w_x)^T(v_u \otimes w_y) \\
&= (v_t^T \otimes w_x^T) (v_u \otimes w_y) \\
&= v_t^Tv_u \otimes w_x^Tw_y \\
&= v_t^Tv_u w_x^Tw_y \\
&= \langle v_t, v_u \rangle \langle w_x, w_y \rangle.
\end{align*}
Thus it is enough to show that 
$$\langle v_t, v_u \rangle = 0 \quad\text{or}\quad \langle w_x, w_y \rangle = 0.$$
\paragraph{}
Since $(t,x) \sim (u,y)$ we have one of three cases, either:
\begin{enumerate}
\item $t=u$ and $x \sim y$ or
\item $t\sim u$ and $x = y$ or
\item $t\sim u$ and $x \sim y$.
\end{enumerate}
\paragraph{}
In case $1$, since $x\sim y$ and the vectors $w_1, \dots, w_n$ form an orthnormal representation of $\overline{H}$
$$\langle w_x, w_y \rangle  = 0$$
as desired. In case $2$ and $3$, since $t \sim u$ and the vectors $v_1, \dots, v_n$ form an orthonormal representation of $\overline{G}$
$$\langle v_t, v_u \rangle = 0$$
as desired. $\blacksquare$
\section*{11}
\paragraph{}
Lovasz showed that for any graph $G$,
\begin{equation}\theta(G) = \max \sum_{i} \langle d, v_i\rangle ^2\label{eq:sdp?} \end{equation}
where the maximum is taken over all unit vectors $d$ and orthonormal representations $v_1, \dots, v_n$ of $\overline{G}$. So let $G$ and $H$ be graphs. Let $d_G$ and $v_1, \dots, w_n$ be the unit vector and orthonormal representation of $\overline{G}$ which optimize (\ref{eq:sdp?}) for $G$. Let $d_H$ and $w_1, \dots, w_m$ be the unit vector and orthonormal representation of $\overline{H}$ which optimize (\ref{eq:sdp?}) for $H$.
\paragraph{}
Now by problem $10$, the vectors $v_i \otimes w_j$ form an orthonormal representation of $\overline{G \boxtimes H}$. Further $d_G \otimes d_H$ is a unit vector in $\R^nm$ (in problem $10$ we showed that Kronecker product of unit vectors are unit vectors). So together $d_G \otimes d_H$ and the vectors $v_i \otimes w_j$ form a feasible solution to the optimization problem (\ref{eq:sdp?}) for $G \boxtimes H$. Therefore
$$\theta(G\boxtimes H) \geq \sum_{i,j} \langle d_G\otimes d_H, v_i \otimes w_j \rangle^2.$$
Now we compute
\begin{align*}
\sum_{i,j} \langle d_G\otimes d_H, v_i \otimes w_j \rangle^2  &= \sum_{i,j}[(d_G\otimes d_H)^T(v_i \otimes w_j)]^2 \\
&= \sum_{i,j}[(d_G^T \otimes d_H^T)(v_i \otimes w_j)]^2 \\
&= \sum_{i,j}(d_G^Tv_i \otimes d_H^Tw_j)^2 \\
&= \sum_{i,j}(d_G^Tv_i)^2 (d_H^Tw_j)^2 \\
&= \sum_i (d_G^Tv_i)^2 \sum_j (d_H^Tw_j)^2\\
&= \theta(G)\theta(H).
\end{align*}
Hence $\theta(G\boxtimes H) \geq \theta(G)\theta(H)$. On the other hand, by Lemma $4.5.2$ of the course notes we have
$$\theta(G\boxtimes H) \leq \theta(G) \theta(H).$$
Therefore $\theta(G \boxtimes H) = \theta(G) \theta(H)$. $\blacksquare$
\section*{15}
\paragraph{}
Let $A$ and $B$ be $m \times n$ and $n \times m$ matrices respectively and let 
$$N = \begin{bmatrix} I & 0 \\-B & I \end{bmatrix}, \quad M = \begin{bmatrix} I & A \\B & I \end{bmatrix}.$$
Then 
$$NM = \begin{bmatrix} I & 0 \\-B & I \end{bmatrix}\begin{bmatrix} I & A \\B & I \end{bmatrix} = \begin{bmatrix}I & A \\ 0 & I-BA\end{bmatrix}$$
and
$$MN = \begin{bmatrix} I & A \\B & I \end{bmatrix}\begin{bmatrix} I & 0 \\-B & I \end{bmatrix} = \begin{bmatrix} I-AB & A \\ 0 & I\end{bmatrix}.$$
Since $NM$ and $MN$ are block upper trianglular we have
$$\det(NM) = \det(I)\det(I-BA) = \det(I-BA)$$
and
$$\det(MN) = \det(I)\det(I-AB) = \det(I-AB).$$
Further since $\det(NM) = \det(N)\det(M) = \det(MN)$ we have
$$\det(I-BA) = \det(I-AB)$$
as desired. Hence if you replace $A$ with $tA$ in the argument above you obtain that
$$\det(I-tAB) = \det(I-tBA).$$
\paragraph{}
To show $AB$ and $BA$ have the same eigenvalues we will use a similar argument to obtain a relationship between the characteristic polynomial of $AB$ and that of $BA$. We redefine $N$ and $M$ as:
$$N = \begin{bmatrix} \lambda I & 0 \\-B & I \end{bmatrix}, \quad M = \begin{bmatrix} I & A \\B & \lambda I \end{bmatrix}.$$
For some indeterminant $\lambda$. Then we have
$$NM = \begin{bmatrix} \lambda I & 0 \\-B & I \end{bmatrix}\begin{bmatrix} I & A \\B & \lambda I \end{bmatrix} = \begin{bmatrix}\lambda I & \lambda A \\ 0 & \lambda I-BA\end{bmatrix}$$
and
$$MN = \begin{bmatrix} I & A \\B & \lambda I \end{bmatrix}\begin{bmatrix} \lambda I & 0 \\-B & I \end{bmatrix} = \begin{bmatrix} \lambda I-AB & \lambda A \\ 0 & \lambda I\end{bmatrix}.$$
Again since our matrices are block upper trianglular we have
$$\det(NM) = \det(\lambda I)\det(\lambda I - BA) = \lambda^m \det(\lambda I - BA).$$
and 
$$\det(MN) = \det(\lambda I) \det(\lambda I -AB) = \lambda^n \det(\lambda I - AB).$$
Since $\det(NM) = \det(N)\det(M) = \det(MN)$, we have
$$\lambda^m \det(\lambda I - BA) = \lambda^n \det(\lambda I - AB).$$
That is the characteristic polynomials of $AB$ and $BA$ agree regarding their non-zero roots and their multiplicities, and hence $AB$ and $BA$ have the same non-zero eigenvalues with the same multiplicities. $\blacksquare$
\section*{16}
\paragraph{Lemma 16.1}
Let $N$ be a positive semidefinite matrix. Let $A$ be a symmetric matrix. Then
$$ANA \succcurlyeq 0.$$
\paragraph{Proof of Lemma 16.1}
Sicne $N \succcurlyeq 0$ there exists $M$ such that
$$N = MM^T.$$
Now $$ANA = AMM^TA = AM(A^TM)^T = AM(AM)^T \succcurlyeq 0.$$ 
The third equality follows by the symmetry of $A$. $\blacksquare$
\paragraph{}
Let $A$ and $B$ be positive definite matrices of the same order. Suppose that $A - B \succcurlyeq 0$. Since $A$ and $B$ are positive definite there exist positive definite matrices $A^\frac{1}{2}$ and $B^\frac{1}{2}$ such that
$$ A= A^\frac{1}{2}A^\frac{1}{2} \quad B = B^\frac{1}{2} B^\frac{1}{2}.$$
We will use $A^\frac{-1}{2}$ and $B^\frac{-1}{2}$ to denote the inverses of $A^\frac{1}{2}$ and $B^\frac{1}{2}$ respectively. Note that $A^\frac{-1}{2}A^\frac{-1}{2} = A^{-1}$ and similar for $B^{-1}$.
\paragraph{}
By Lemma $16.1$
$$A^\frac{-1}{2}(A-B)A^\frac{-1}{2} \succcurlyeq 0.$$
Therefore
$$I - A^\frac{-1}{2}BA^\frac{-1}{2} \succcurlyeq 0.$$
Now for any matrix $M$, the eigenvalues of $I-M$ are $1-\lambda$ where $\lambda$ is an eigenvalue of $M$, so the eigenvalues of $I - A^\frac{-1}{2}BA^\frac{-1}{2}$ are $1- \lambda$ where $\lambda$ is an eigenvalue of $A^\frac{-1}{2}BA^\frac{-1}{2}$. Hence any matrix $M$ with the same non-zero eigenvalues as $A^\frac{-1}{2}BA^\frac{-1}{2}$ would also be such that $I - M \succcurlyeq 0$. But $$\A^\frac{-1}{2}BA^\frac{-1}{2}  = (A^\frac{-1}{2}B^\frac{1}{2})(B^\frac{1}{2}A^\frac{-1}{2})$$
and we have by problem $15$ of this problem set that commuting matrices does not change non-zero eigenvalues. So the non-zero eigenvalues of $A^\frac{-1}{2}BA^\frac{-1}{2}$ are the same as the non-zero eigenvalues of $$(B^\frac{1}{2}A^\frac{-1}{2})(A^\frac{-1}{2}B^\frac{1}{2}) = B^\frac{1}{2}A^{-1}B^\frac{1}{2}.$$
Therefore
$$I - B^\frac{1}{2}A^{-1}B^\frac{1}{2} \succcurlyeq 0.$$
\paragraph{}
Apply Lemma $16.1$ again we have that
$$B^\frac{-1}{2}(I - B^\frac{1}{2}A^{-1}B^\frac{1}{2})B^\frac{-1}{2} \succcurlyeq 0$$
and thus by multiplying through we see
$$B^{-1} - A^{-1} \succcurlyeq 0.$$
$\blacksquare$
\section*{17}
\paragraph{}
Let $\lambda_1(A)$ denote the largest eigenvalue of symmetric matrix $A$. By Laurent and Vallentin we have
$$\lambda_1(A) = \max_{x: ||x|| = 1} x^TAx.$$
Let $A$ and $B$ be symmetric matrices  and let $\alpha \in [0,1]$.
Then we have
\begin{align*}
\lambda_1(\alpha A + (1-\alpha)B) &= \max_{x: ||x|| =1} x^T(\alpha A + (1-\alpha)B)x \\
&= \max_{x: ||x|| =1}\alpha x^TAx + (1-\alpha)x^TBx \\
&\leq \alpha\max_{x:||x|| =1}x^TAx + (1-\alpha)\max_{x:||x||=1} x^TBx\\
&= \alpha\lambda_1(A) + (1-\alpha)\lambda_1(B).
\end{align*}
The $\leq$ in the third line follows from observing that the optimal $x$ for the program in the second line is simply feasible for the programs in the third line. Therefore $\lambda_1$ is convex. $\blacksquare$
\section*{18}
\paragraph{}
The domain of $\lambda_1$ is the set of all symmetric matrices, so the domain of $\lambda_1$ is open. By problem $17$, $\lambda_1$ is convex. Since $\lambda_1$ is a convex function and its domain is open, $\lambda_1$ is continuous (a well-known fact of convex analysis, see Rockafellar's Convex Analysis for proof).
\paragraph{}
Despite $\lambda_1$ being continuous it is not differentiable everywhere. Consider the family of symmetric matrices
$$A(\alpha) = \begin{bmatrix} \alpha & 0 \\ 0 & -\alpha \end{bmatrix}.$$
Observe that $\lambda_1(A(\alpha)) = |\alpha|$. Now consider the derivative of $\lambda_1$ at $0$ restricting to the set of matrices $A(\alpha)$. Then the derivative is
$$\lim_{\alpha\rightarrow 0} \frac{\lambda_1(0 + A(\alpha)) - \lambda_1(0)}{\alpha} = \lim_{\alpha\rightarrow 0}\frac{\lambda_1(A(\alpha)}{\alpha} = \frac{|\alpha|}{\alpha}.$$
But if we approach from the left:
$$\lim_{\alpha\rightarrow 0^-} \frac{|\alpha|}{\alpha} = \frac{-\alpha}{\alpha} = -1$$
and if we approach from the right:
$$\lim_{\alpha \rightarrow 0^+}\frac{|\alpha|}{\alpha} = \frac{\alpha}{\alpha} = 1.$$
Hence the limit is undefined and so the derivative does not exist at $0$. 
\section*{19}
\paragraph{}
Let $K$ be a closed convex cone such that $int(K) \neq \emptyset$. We will disprove the statement:
$$\text{$x\in int(K) \iff $ there exists $y \in K^*$ such that $y^Tx > 0$}$$
from both directions.
\paragraph{}
Let $K = \R^n$. Then $K$ is a closed convex cone and $int(K) = K \neq \emptyset$. Let $x \in int(K)$. Observe $K^* = \{0\}$ and hence for all $y \in K^*$, $y^Tx = 0^Tx = 0 \not> 0$. This contradicts the "$\implies$" direction of the statement.
\paragraph{}
Now let $K$ be the set of positive semidefinite matrices of order $n = 2$. Then $K$ is self-dual, that is $K^* = K$. Consider the all ones matrix $J \in K$. Now $I \in K^*$ since $I \succcurlyeq 0$ and $\langle I, J\rangle = tr(J) = 2 > 0$. But $J$ has eigenvalue $0$ and hence is nonsingular. Now since $int(K)$ is the set of positive definite matrices, $J \not\in int(K)$. This contradicts the "$\impliedby$" direction of the statement.
\section*{20}
\paragraph{}
Consider the semidefinite program in variables $x_1, \dots, x_n$:
$$\inf x_n,\quad \begin{bmatrix} 1 & 2 \\2 & x_1\end{bmatrix} \succcurlyeq 0, \quad \begin{bmatrix} 1 & x_{i-1}\\ x_{i-1} & x_i \end{bmatrix} \succcurlyeq 0, \quad (i=2,\dots,n).$$
\paragraph{}
We claim that for each $i = 1,\dots, n$, $x_i \leq 2^{2^{i}}$. Proceed by induction on $i$. If $i = 1$ then consider the constraint
$$\begin{bmatrix} 1 & 2 \\2 & x_1\end{bmatrix} \succcurlyeq 0.$$
We have that
$$\det(\begin{bmatrix} 1 & 2 \\2 & x_1\end{bmatrix}) \geq 0$$
as otherwise the matrix has at least one negative eigenvalue. Therefore
$$ x_1 - 4 \geq 0$$
that is,
$$ x_1 \geq 2^{2^1}$$
and the claim holds in the base case. Now let $i \in \{2,\dots, n\}$ and suppose the claim holds for $i-1$, that is
$$x_{i-1} \geq 2^{2^{i-1}}.$$
Again consider the constraint
$$ \begin{bmatrix} 1 & x_{i-1}\\ x_{i-1} & x_i \end{bmatrix} \succcurlyeq 0$$
from which we see that
$$\det( \begin{bmatrix} 1 & x_{i-1}\\ x_{i-1} & x_i \end{bmatrix}) \geq 0.$$
Thus we have 
$$x_i - x_{i-1}^2 \geq 0.$$
So by the induction hypothesis:
$$x_i \geq x_{i-1}^2 \geq (2^{2^{i-1}})^2 = 2^{2\cdot 2^{i-1}} = 2^{2^i}.$$
Hence by induction the claim holds for all $i = 1, \dots, n$. When $i = n$ we have
$$x_n \geq 2^{2^n}$$
as desired. $\blacksquare$

\section*{21}
\paragraph{}
Let $f(X) = -\log(\det(X))$ for $X \succ 0$. Let $t$ be a scalar parameter and let $H$ be a symmetric matrix. Consider the second order approximation:
\begin{equation}f(X + tH) \approx f(X) + t\langle \nabla f(X), H \rangle + \frac{t^2}{2}\langle \nabla^2f(X)H, H\rangle.\label{eq:soa}\end{equation}
Since $X \succ 0$ there exist positive definite $X^\frac{1}{2}$ such that $X = X^\frac{1}{2}X^\frac{1}{2}$. We have
\begin{align*}
f(X + tH) &= -\log(\det(X + tH)) \\
&= -\log(\det(X^\frac{1}{2}(I + tX^\frac{-1}{2}HX^\frac{-1}{2})X^\frac{1}{2})) \\
&= -\log(\det(X^\frac{1}{2})\det(I + tX^\frac{-1}{2}HX^\frac{-1}{2})X^\frac{1}{2})\det(X^\frac{1}{2}))\\
&=-\log(\det(X)\det(I + tX^\frac{-1}{2}HX^\frac{-1}{2})) \\
&=-\log(\det(X)) -\log(\det(I + tX^\frac{-1}{2}HX^\frac{-1}{2}))\\
&= f(X) + -\log(\det(I + tX^\frac{-1}{2}HX^\frac{-1}{2})) \label{eq:taylor} \numberthis.
\end{align*}
Now since determinant is equal to the product of the eigenvalues, and adding $I$ merely shifts eigenvalues by $1$ we have
$$f(X+tH) = f(X) -\log(\Pi_{i}(1+t\lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2}))) = f(X) -\sum_i \log(1+t\lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2})),$$
where $\lambda_i(\cdot)$ denotes the $i^\text{lth}$ largest eigenvalue. Let $g(t)$ be the function
$$g(t) = \sum_i \log(1+t\lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2})).$$
We will approximate $g$ be a Taylor polynomial centred at $0$. Note that $g(0) = \sum_i \log(1) = 0$. Observe that
$$g'(t) = \sum_{i}\frac{\lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2})}{1 + t(\lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2}))}$$
and hence
$$g'(0) = \sum_{i} \lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2}).$$
Also observe that
$$g''(t) = -\sum_{i} \frac{\lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2})^2}{(1+t\lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2}))^2}$$
and hence
$$g''(0)  = -\sum_{i} \lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2})^2.$$
Therefore $g(t)$ can be approximated by a Taylor polynomial as
\begin{align*}g(t) &\approx g(0) + g'(0)t + \frac{g''(0)}{2} t^2 \\
&= 0 + t\sum_{i} \lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2}) - t^2 \sum_{i} \lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2})^2 \\
&= t\cdot tr(X^\frac{-1}{2}HX^\frac{-1}{2}) -t^2 \sum_i\lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2})^2
\end{align*}
since trace is the equal to the sum of the eigenvalues. Further note that for any matrix $A$, if $\lambda$ is an eigenvalue of $A$ then $\lambda^2$ is an eigenvalue of $A^2$. For a quick proof suppose that $x$ is the corresponding eigenvector to eigenvalue $\lambda$ of $A$. Then
$$A^2x = \lambda Ax = \lambda^2x$$
as desired. Therefore
$$\sum_{i} \lambda_i(X^\frac{-1}{2}HX^\frac{-1}{2})^2 = \sum_{i} \lambda_i((X^\frac{-1}{2}HX^\frac{-1}{2})^2) = tr((X^\frac{-1}{2}HX^\frac{-1}{2})^2).$$
So then
$$g(t) \approx t\cdot tr(X^\frac{-1}{2}HX^\frac{-1}{2}) - t^2 tr((X^\frac{-1}{2}HX^\frac{-1}{2})^2)).$$
Now substituting into (\ref{eq:taylor}) we observe
$$f(X+tH) \approx f(X) + t(-tr(X^\frac{-1}{2}HX^\frac{-1}{2})) + \frac{t^2}{2}(tr((X^\frac{-1}{2}HX^\frac{-1}{2})^2)).$$
Comparing terms with (\ref{eq:soa}) we have
$$\langle \nabla f(X), H \rangle = -tr(X^\frac{-1}{2}HX^\frac{-1}{2})$$
and
$$\langle \nabla^2f(X)H, H\rangle = tr((X^\frac{-1}{2}HX^\frac{-1}{2})^2)$$
as desired.
\paragraph{}
Notice that
$$\langle \nabla^2f(X)H, H\rangle = tr((\nabla^2f(X)H)^TH) = tr(\nabla^2f(X)^2(HH^T)) = \langle \nabla^2f(X), HH^T\rangle$$
for any symmetric matrix $H$.
Let $A \succcurlyeq 0$. Then there exists $H$ such that $A = HH^T$, and so
$$\langle\nabla^2f(X), A \rangle = \langle \nabla^2f(X), HH^T\rangle = \langle \nabla^2f(X)H, H\rangle = tr((X^\frac{-1}{2}HX^\frac{-1}{2})^2).$$
If we can show that $\langle\nabla^2f(X), A \rangle \geq 0$ then we may conclude that $\nabla^2f(X) \succcurlyeq 0$ by Theorem $2.1.2$ of the course notes. So it remains to show
$$tr((X^\frac{-1}{2}HX^\frac{-1}{2})^2) \geq 0.$$
Indeed this holds as $X^\frac{-1}{2}HX^\frac{-1}{2})^2 \succcurlyeq 0$. To see that observe
\begin{align*}
X^\frac{-1}{2}HX^\frac{-1}{2})^2 &= (X^\frac{-1}{2}HX^\frac{-1}{2})(X^\frac{-1}{2}HX\frac{-1}{2})^T
\end{align*}
as $$(X^\frac{-1}{2}HX^\frac{-1}{2})^T = (X^\frac{-1}{2})^TH^T(X^\frac{-1}{2})^T = X^\frac{-1}{2}HX^\frac{-1}{2}$$
by symmetry of $H$ and $X^\frac{-1}{2}$. $\blacksquare$
\end{document}
