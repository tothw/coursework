\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\rhead{William Justin Toth 671 Assignment 6 - Problem Set 2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section*{1}
\paragraph{}
Let $K$ be a convex cone. Let $B$ be a compact base for $K$.  We will use a basic result in Analysis to show that $K$ is closed:
$$
\text{A set $C$ is closed $\iff$ every convergent sequence in $C$ converges to a point in $C$.}
$$
Let $(x)_n \subseteq K$ be a convergent sequence in $K$. Let $x$ be the limit point of $(x)_n$. Then for every $x_n$ there exists $b_n \in B$ and $\alpha_n > 0$ such that
$$x_n = \alpha_n b_n.$$
Thus we have a sequence $(b)_n \subseteq B$ corresponding to the sequence $(x)_n$. Since $B$ is compact this sequence $(b)_n$ has a convergent subsequence $$(b)_{n_i}$$ that converges to $b \in B$.
Since $(x)_n$ converges, the subsequence $$(x)_{n_i}$$ corresponding to $(b)_{n_i}$ converges and its limit is also $x$. We also have a sequence of $(\alpha)_{n_i}$ satisfying
$$x_{n_i} = \alpha_{n_i} b_{n_i}$$
for all $n_i$.
Taking norms and dividing by $||b_{n_i}||$ we obtain
$$\alpha_{n_i} = \frac{||x_{n_i}||}{||b_{n_i}||}.$$
Since each $b_{n_i}$ is not $0$ this is well-defined. The norm $||\cdot||$ is a continuous map, so since $(x)_{n_i}$ and $(b)_{n_i}$ are convergent, the sequences of norms
$$||x_{n_i}|| \quad \text{and}\quad ||b_{n_i}||$$
are convergent. Then $(\alpha)_{n_i}$ is the quotient of two convergent sequences and this therefore continuous. Further since $b \in B$, $b \neq 0 $ and thus the limit of $\alpha_{n_i}$ is just the limit of quotients:
$$\lim_{n_i \rightarrow \infty} \alpha_{n_i} = \lim_{n_i \rightarrow \infty} \frac{||x_{n_i}||}{||b_{n_i}||} = \frac{||x||}{||b||}.$$
Let $\alpha$ denote $\frac{||x||}{||b||}$. Then $(\alpha)_{n_i}$ is convergent and converges to $\alpha$. Further $\alpha \geq 0$.
\paragraph{}
But now we have that
$$x_{n_i} = \alpha_{n_i} b_{n_i}.$$
That is, $x_{n_i}$ is the product of two convergent sequences and hence converges to the product of their limits. Thus
$$\lim_{n_i \rightarrow \infty} x_{n_i} = \lim_{n_i \rightarrow \infty} \alpha_{n_i} b_{n_i} =\alpha b.$$
Since $\alpha \geq 0$ and $b \in K$, $\alpha b \in K$. Therefore the subsequence $(x)_{n_i}$ of $(x)_n$ converges to a point in $K$. Since convergent sequences and their convergent subsequences has the same limit, $(x)_n$ converges to a point in $K$. Therefore $K$ is closed. $\blacksquare$
\section*{2}
\subsection*{Part 1}
\paragraph{}
Let $K$ be a pointed convex cone. Let $H$ be an affine hyperplane that does not contain $0$. Let this affine hyperplane we describe by all $x$ satisfing:
$$\langle h, x \rangle = c $$ where $c \neq 0$. Suppose that $H\cap K$ is bounded and $H \cap K \neq 0$. Let $k \in K$ such that $k \neq 0$. Let $R$ be the ray in $K$ formed by nonegative scalar multiples of $k$. Then either $R$ intersects $H$ at a unique point $b$ or $R$ is parallel to $H$ (possibly with infinite intersections or none).
\paragraph{}
Suppose for a contradiction that $R$ is parallel to $H$. Then, as $k \in R$,
$$\langle h, k \rangle = 0.$$
Let $a \in H \cap K$. Then for all $\alpha \geq 0$, $a + \alpha k \in H \cap K$. We can see this by observing that $a$ and $\alpha k$ are in $K$ and $K$ is closed under addition, and observing that
$$\langle h, a + \alpha k \rangle = \langle h, a \rangle + \alpha \langle h, k \rangle = \langle h, a \rangle + 0 = c.$$
But by appropriating choosing $\alpha$ we can make $|| a+ \alpha k||$ arbitrarily large. This contradictions that $H\cap K$ is bounded. Hence $R$ is not parallel to $H$.
\paragraph{}
So $R$ interesects $H$ at some unique point $b$. Since $b \in R$ we have that $b \in H \cap K$. Since $k \in R \backslash\{0\}$ and $b \in R \backslash \{0\}$ (as $0\not\in H$) there exists $\alpha > 0$ such that $$k = \alpha b.$$
Such choice of $\alpha$ is unique. Therefore for any $k \in K \backslash\{0\}$ there is a unique point $b$ in $H \cap K$, and a unique scalar $\alpha > 0$ such that $k = \alpha b$. That is, $H \cap K$ is a base for $K$. $\blacksquare$
\subsection*{Part 2}
\paragraph{}
We will us Part $1$ to show that positive semidefinite matrices of trace $1$ form a base for the cone of positive semidefinite matrices. But by Part $1$ it is enough to show that set of trace $1$ matrices form an affine hyperplance and that the set of positive semidefinite trace $1$ matrices is bounded.
\paragraph{}
Let $J$ be the all ones matrice. Then $$J = \begin{bmatrix}
1 \\
\vdots \\
1
\end{bmatrix}\begin{bmatrix}
1& \hdots & 1
\end{bmatrix}^T.
$$
Consider the affine hyperplane $H$ given by the set of $X$ satisfying
$$\langle I, X \rangle = 1$$
where $I$ is the identity matrix.
We claim that $H$ is exactly the set of trace $1$ matrices. Indeed
\begin{align*}
\langle I , X \rangle = 1 &\iff tr(I^TX) = 1 \\
&\iff tr(X) = 1.
\end{align*} 
Therefore the set of trace $1$ matrices is an affine hyperplane.
\paragraph{}
It remains to show that the set of trace $1$ positive semidefinite matrices in bounded. Let $X\succcurlyeq 0$ and suppose that $tr(X) = 1$. Let $r$ be the rank of $X$. Since $X \succcurlyeq 0$ there exists vectors $x_1, \dots, x_r$ such that
$$ X = \sum_{i=1}^r x_ix_i^T.$$
Let for each $i = 1,\dots,r$ let $\hat{x}_i$ be given by
$$ \hat{x}_i = \frac{x_i}{||x_i||}.$$
Then $$X = \sum_{i=1}^r ||x_i||^2 \hat{x}_i\hat{x}_i^T.$$
So taking the trace gives
\begin{align*}
1 &= tr(X) \\
&= \sum_{i=1}^r ||x_i||^2 tr(\hat{x}_i \hat{x}_i^T) \\
&= \sum_{i=1}^r ||x_i||^2 \hat{x}_i^T \hat{x}_i \\
&= \sum_{i=1}^r ||x_i||^2 ||\hat{x}_i||^2 \\
&= \sum_{i=1}^r ||x_i||^2.
\end{align*}
Now using this equality we show that $||X|| \leq 1$:
\begin{align*}
||X|| &= ||\sum_{i=1}^r ||x_i||^2 \hat{x}_i\hat{x}_i^T|| \\
&\leq \sum_{i=1}^r ||x_i||^2 ||\hat{x}_i\hat{x}_i^T|| \\
&= \sum_{i=1}^r ||x_i||^2 \langle \hat{x}_i \hat{x}_i^T, \hat{x}_i \hat{x}_i^T\rangle^{\frac{1}{2}} \\
&= \sum_{i=1}^r ||x_i||^2 tr(\hat{x}_i \hat{x}_i^T\hat{x}_i \hat{x}_i^T)^{\frac{1}{2}} \\
&= \sum_{i=1}^r ||x_i||^2 tr(\hat{x}_i \hat{x}_i^T)^\frac{1}{2} \\
&= \sum_{i=1}^r ||x_i||^2 (\hat{x}_i^T\hat{x}_i) ^\frac{1}{2} \\
&= \sum_{i=1}^r ||x_i||^2 \\
&= 1.
\end{align*}
Thus all positive semidefinite matrices of trace $1$ have norm at most $1$, and so the intersection of the hyperplane of trace $1$ matrices with the cone of positive semidefinite matrices is bounded. Therefore by Part 1 the set of trace $1$ positive semidefinite matrices is a base for the cone of positive semidefinite matrices. $\blacksquare$
\section*{3}
\paragraph{}
Let $C\subseteq V$ be a compact convex set. Let $L\subseteq V$ be an affine subspace. Suppose that $C \cap L = \emptyset$. We interpret the distance between $C$ and $L$, denoted $d(C,L)$ to be
$$d(C,L) = \inf \{||x - y|| : x \in C, y\in L\}.$$
For any $x \in C$ define $d(x,L)$ as
$$d(x,L) = \inf \{||x-y|| : y \in L\}.$$
We claim that there exists $x \in C$ such that $d(x,L) = d(C,L)$. Indeed since
$$d(C,L) = \inf_{x \in C} d(x,L)$$
and since the compactness of $C$ implies the minimum of continuous functions over $C$ is attained at a point in $C$ the claim holds provided we can show that $d(x,L)$ is continuous.
\paragraph{}
Let $x \in C$. Let $\epsilon > 0$. Choose $\delta = \frac{\epsilon}{2}$. Now let $z \in C$ such that $||x - z || < \delta$. Then if $d(x,L) \leq d(z,L)$, let $y \in L$ such that $$||x-y|| < d(x,L) + \delta.$$
Such a $y$ exists by the definition of $d(x,L)$. We have:
\begin{align*}
|d(z,L) - d(x,L)|  &= d(z,L) - d(x,L) \\
&\leq ||z-y|| - d(x,L) \\
&\leq ||z-x|| + ||x-y|| - d(x,L) \\
&< \delta + \delta + d(x,L) - d(x,L) \\
&= 2\delta \\
&< \epsilon.
\end{align*}
Now if $d(x,L) > d(z,L)$, let $y \in L$ such that $$||z-y|| < d(z,L) + \delta.$$
Such a $y$ exists by the definition of $d(z,L)$. We have:
\begin{align*}
|d(z,L) - d(x,L)| &= d(x,L) - d(z,L) \\
&\leq ||x-y|| - d(z,L) \\
&\leq ||x-z|| + ||y-z|| - d(z,L) \\
&< \delta + \delta + d(z,L) - d(z,L) \\
&= 2\delta \\
&= \epsilon.
\end{align*}
Therefore $d(x,L)$ is a continuous function at any $x\in C$.
\paragraph{}
Thus there exists $x \in C$ such that $$ d(x,L) = d(C,L).$$ Now it remains to find $p \in L$ for which $||x - p || = d(x,L)$. Let $a \in L$. Since $L$ is an affine subspace there exists a linear subspace $S$ such that $$ L = a + S.$$
Let $P_S(v)$ denote the standard orthogonal projection of linear algebra from any $v \in V$ onto $S$. In particular we note that for any $v \in V$ and $s \in S$, $\langle v - P_S(v), s - 0 \rangle = 0$. We claim that the orthogonal projection onto $L$, denoted $P_L(v)$ is of the form:
$$P_L(v) = a + P_S(v-a).$$
\paragraph{}
First since $P_S(v-a) \in S$ we have that $P_L(v) \in a + S = L$ so indeed $P_L(v)$ maps into $L$. To verify that the map is orthogonal consider any direction $L$: $p -a$ for $p \in L$. We aim to show that $v - P_L(v)$ is orthogonal to any such direction, that is that:
$$\langle v - P_L(v), p -a \rangle = 0.$$
Indeed  let $p_S \in S$ such that $p_S + a = p$. Then we have
\begin{align*}
\langle v- P_L(v), p-a \rangle &= \langle v - a - P_S(v-a), p_S + a - a \rangle \\
&= \langle (v-a) - P_S(v-a), p_S \rangle \\
&= 0.
\end{align*}
Thus $P_L(v)$ is the orthogonal projection map onto $L$.
\paragraph{}
Hence it remains to show that $P_L(x)$ minimizes the distance from $x$ to $L$. Let $p \in L$ and let $p_S \in S$ such that $p = a + p_S$. Then
\begin{align*}
||x- P_L(x) ||^2 &\leq ||x - P_L(x) ||^2 + ||P_L(x) - p||^2 \\
&= ||(x-a) - P_S(x-a) ||^2 + ||a + P_S(x) - a - p_S ||^2 \\
&= ||(x-a) - P_S(x-a) ||^2 + ||P_S(x) - p_S||^2.
\end{align*}
Now by the definition of orthogonal projection onto a subspace, $(x-a) - P_S(x-a)$ is an element of the orthogonal complement of $S$. Further since $S$ is a vector subspace and $P_S(x), p_S \in S$, $P_S(x) - p_S \in S$. So by the Pythagorean Theorem
$$ ||(x-a) - P_S(x-a) ||^2 + ||P_S(x) - p_S||^2 = ||(x-a) - P_S(x-a) + P_S(x-a) - p_S||^2 = ||x-a - p_s||^2.$$
Therefore 
\begin{align*}
||x- P_L(x) ||^2 &\leq ||x-a - p_s||^2 \\
&= ||x - (a+ p_S) ||^2 \\
&= ||x - p||^2.
\end{align*}
So $||x- P_L(x) || \leq ||x-p||$ for any $p \in L$. That is $||x - P_L(x)|| = d(x,L)$. So there exists $p = P_L(x) \in L$ such that $||x-p||$ is equal to $d(C, L)$ and the line through $x$ and $p$ is orthogonal to $L$. $\blacksquare$
\section*{4}
\paragraph{}
Let $C \subseteq V$ be a closed convex set. Let $L \subseteq V$ be an affine subspace. Suppose that $C \cap L = \emptyset$. By the separating hyperplane thereom there exists $h$ and $b$ such that for $x \in C$
$$\langle h, x \rangle \leq b.$$
and for all $y \in L$
$$\langle h, y \rangle \geq b.$$
\paragraph{}
We claim that there exists $c\geq b$ such that for all $y \in L$
$$\langle h, y \rangle = c.$$
That $c \geq b$ is obvious by our choice of $h$ and $b$. Now suppose for a contradiction there exist $y_1, y_2 \in L$ and $c_1, c_2 \geq b$ with $c_1 > c_2$ (without loss of generality) such that $$\langle h, y_1 \rangle = c_1 \quad\text{and}\quad \langle h, y_2 \rangle = c_2.$$
Then $c_1 - c_2 > 0$. So we may choose
$$\alpha = \frac{b-c_2}{2(c_1-c_2)}.$$
Then since $L$ is affine $\alpha y_1 + (1-\alpha) y_2 \in L$. But we have
\begin{align*}
\langle h, \alpha y_1 + (1-\alpha) y_2 \rangle &= \alpha \langle h,y_1\rangle + (1- \alpha) \langle h, y_2 \rangle \\
&= \alpha c_1 + (1-\alpha) c_2 \\
&= c_2 + \alpha (c_1 - c_2) \\
&= c_2 + \frac{b-c_2}{2(c_1-c_2}(c_1-c_2) \\
&< c_2 + b-c_2 \\
&= b. 
\end{align*}
But this contradicts that for all $y \in L$
$$\langle h, y \rangle \geq b.$$
Hence there exists $c \geq b$ such that for all $y \in L$
$$\langle h, y \rangle = c.$$
Choose $H$ to be the hyperplane defined by all $x$ satisfying
$$\langle h, x \rangle = c.$$
Then $L \subseteq H$. 
\paragraph{}
Now if $b=c$ then for all $x  \in C$ 
$$\langle h, x \rangle < b=c.$$
Otherwise there exists a point in $C \cap L$, but $C\cap L = \emptyset$, a contradction. So we may assume that $b < c$. Now let $x \in C$. Then
$$
\langle h, x \rangle  \leq b < c.
$$
Hence $H \cap C = \emptyset$. $\blacksquare$
\section*{5}
\paragraph{}
Let $x \in \R^d$ and $t\in \R$. We will show
$$\begin{bmatrix}tI & x \\ x^T & t \end{bmatrix} \succcurlyeq 0 \iff ||x|| \leq t.$$
\paragraph{Case 1}
We first consider the case where $t = 0$. Then the matrix $$\begin{bmatrix}0 & x \\ x^T & 0 \end{bmatrix} \succcurlyeq 0$$ if and only if for all $\begin{bmatrix} \bar{y} \\y \end{bmatrix} \in \R^d \times \R$ we have
$$\begin{bmatrix}\bar{y}^T & y \end{bmatrix} \begin{bmatrix}0 & x \\ x^T & 0 \end{bmatrix} \begin{bmatrix} \bar{y} \\y \end{bmatrix} \geq 0$$
which holds if and only if
$$2y\bar{y}^Tx \geq 0.$$
Thus if $$\begin{bmatrix}0 & x \\ x^T & 0 \end{bmatrix} \succcurlyeq 0$$ choose $\bar{y} = x$ and $y = 1$ and we have
$$2y\bar{y}^Tx  = 2||x||^2 \geq 0.$$
Also if $||x|| \leq t = 0$ then $x = 0$ and thus
$$2y\bar{y}^Tx = 0  \geq 0$$
for any $\bar{y}, y$. Therefore the result holds in this case.
\paragraph{Case 2}
Now we may assume that $t\neq 0$. In this case $tI$ is nonsingular and so we may use Theorem $2.4.1$ (Cholesky) of the course notes:
$$\begin{bmatrix}tI & x \\ x^T & t \end{bmatrix} \succcurlyeq 0 \iff tI \succcurlyeq 0 \quad \text{and}\quad t - x^T(tI)^{-1}x  = t - \frac{1}{t}x^Tx \succcurlyeq 0.$$
But $tI \succcurlyeq 0$ if and only if
$$ t\geq 0,$$
and hence 
$$t - \frac{1}{t}x^Tx \succcurlyeq 0 $$
if and only if $$t^2 \geq x^Tx.$$
But $t^2 \geq x^Tx$ simply states $t^2 \geq ||x||^2$ and thus taking square roots this holds if and only if
$$t \geq ||x||.$$
Therefore the result holds in both cases. $\blacksquare$
\section*{6}
\paragraph{}
Let $x\in \R^d$ and $t\in \R$. Then
$$\begin{bmatrix}tI & x \\ x^T & t \end{bmatrix} \succcurlyeq 0$$ if and only if for all $\begin{bmatrix} \bar{y} \\y \end{bmatrix} \in \R^d \times \R$ we have
$$\begin{bmatrix}\bar{y}^T & y \end{bmatrix} \begin{bmatrix}tI & x \\ x^T & t \end{bmatrix} \begin{bmatrix} \bar{y} \\y \end{bmatrix} \geq 0.$$
Multiplying out we have
$$ty^2 + 2\bar{y}^Txy + t||\bar{y}||^2.$$
If $t = 0$ then we are back in problem $5$ case $1$. Thus we may assume $t\neq 0$. In this case we complete the square:
$$ty^2 + 2\bar{y}^Txy + t||\bar{y}||^2 + \frac{(\bar{y}^Tx)^2}{t}-\frac{(\bar{y}^Tx)^2}{t}.$$
Which simplies giving $\begin{bmatrix}tI & x \\ x^T & t \end{bmatrix} \succcurlyeq 0$ if and only if
\begin{equation}t(y+\frac{\bar{y}^Tx}{t})^2 + t||\bar{y}||^2 - \frac{(\bar{y}^Tx)^2}{t} \geq 0\label{eq:quadratic}\end{equation}
for all $\begin{bmatrix} \bar{y} \\y \end{bmatrix} \in \R^d \times \R$.
\paragraph{}
Thus if $\begin{bmatrix}tI & x \\ x^T & t \end{bmatrix} \succcurlyeq 0$ then choosing $y =1$ and $\bar{y} = 0$ and substituting into (\ref{eq:quadratic}) we obtain:
$$t(1 + 0)^2 + 0 \geq 0$$
and thus $t \geq 0$. Now choosing $\bar{y} = x$ and $y = -\frac{\bar{y}^Tx}{t}$ and substituting into (\ref{eq:quadratic}) we obtain:
$$0 + t||x||^2 - \frac{(x^Tx)^2}{t} \geq 0$$
Since $x^Tx = ||x||^2$ and $t\geq 0$ we may rearrange giving
$$t^2||x||^2 \geq ||x||^4.$$
If $x = 0$ then since $t \geq 0$ we would have $||x|| \leq t$ as desired. Hence we may assume $x \neq 0$. Thus we may divide by $||x||^2$ obtaining
$$t^2 \geq ||x||^2$$
and thus taking square roots gives
$$t \geq ||x||$$ as desired.
\paragraph{}
Now if $||x|| \leq t$ then $t \geq 0$ (more precisely $t > 0$). Let $\bar{y} \in \R^d$ and $y \in \R$. Then $$t(y++\frac{\bar{y}^Tx}{t})^2 \geq 0$$
as it is the product of two non-negative quantities. Thus by (\ref{eq:quadratic}) it remains to show that
$$t||\bar{y}||^2 - \frac{(\bar{y}^Tx)^2}{t} \geq 0.$$
Indeed we have
\begin{align*}
t||\bar{y}||^2 - \frac{(\bar{y}^Tx)^2}{t} &= \frac{t^2||\bar{y}||^2 - (\bar{y}^Tx)^2}{t} \\
&\geq \frac{||x||^2||\bar{y}||^2 - (\bar{y}^Tx)^2}{t}  &\text{since $t \geq ||x||$}\\
&\geq \frac{(\bar{y}^Tx)^2 - (\bar{y}^Tx)^2}{t} &\text{by Cauchy-Schwarz} \\
&= 0.
\end{align*}
Therefore by (\ref{eq:quadratic}) the result holds. $\blacksquare$
\end{document}
