\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{bbm}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
\newcommand{\1}{\mathbbm{1}} 

\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\rhead{William Justin Toth 671 Assignment 10 - Problem Set 3} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section*{7}
\paragraph{}
Let $K(n,k)$ be the Kneser graph. We define a homomorphism, $h$ from $K(n,k)$ to $S(n, \frac{-1}{\frac{n}{k}-1}) = S(n, \frac{-k}{n-k})$ as follows: Let $A \in V(K(n,k))$ and let $\chi(A)$ denote the characteristic vector of $A$. That is $\chi(A) \in \R^n$ with $\chi(A)_i = 1$ if $i \in A$ for $i =1,\dots, n$ and $\chi(A)_i = 0$ otherwise. Define
$$h(A) = \frac{1}{\sqrt{k}\sqrt{1 - \frac{k}{n}}} (\chi(A) - \frac{k}{n} \1)$$
where $\1$ is taken to be the all ones vector of size $n$ in this context. We claim that $h$ indeed maps to $S(n,\frac{-k}{n-})$. To show this we prove that $h(A)$ is a unit vector:
\begin{align*}
\langle h(A), h(A) \rangle &= \frac{1}{k(1-\frac{k}{n})}\langle \chi(A) - \frac{k}{n}\1, \chi(A) - \frac{k}{n}\1\rangle \\
&= \frac{n}{k(n-k)} [\langle \chi(A), \chi(A) \rangle - \langle \frac{k}{n}\1, \chi(A) \rangle -\langle \chi(A), \frac{k}{n}\1 \rangle + \frac{k^2}{n^2}\langle \1, \1\rangle]\\
&= \frac{n}{k(n-k)}[k - \frac{k^2}{n} - \frac{k^2}{n} + \frac{k^2}{n}] \\
&= \frac{n}{k(n-k)}[\frac{nk - k^2}{n}] \\
&= 1.
\end{align*}
Thus $h(A)$ is a unit vector in $\R^n$, that is a vertex of $S(n,\frac{-k}{n-k})$.
 We prove now that $h$ is a graph homomorphism. Let $A,B \in V(K(n,k))$ and suppose that $A\sim B$. Then $A\cap B = \emptyset$. More pertinently this means $\langle \chi(A), \chi(B) \rangle = 0$. Also observe that $\langle \chi(A), \1\rangle = k$ since $A$ is a $k$-subset of $\{1, \dots, n\}$. We need to show that $h(A) \sim h(B)$. Indeed we have:
\begin{align*}
\langle h(A), h(B) \rangle &= \langle  \frac{1}{\sqrt{k}\sqrt{1 - \frac{k}{n}}} (\chi(A) - \frac{k}{n} \1),  \frac{1}{\sqrt{k}\sqrt{1 - \frac{k}{n}}} (\chi(B) - \frac{k}{n} \1) \rangle \\
&= \frac{1}{k(1-\frac{k}{n})}[\langle \chi(A), \chi(B)\rangle + \langle \chi(A), \frac{-k}{n}\1\rangle + \langle \frac{-k}{n} \1, \chi(B) \rangle + \langle \frac{-k}{n} \1, \frac{-k}{n} \1\rangle] \\
&= \frac{n}{k(n-k)}[\frac{-k^2}{n} + \frac{-k^2}{n} + \frac{k^2}{n}] \\
&= \frac{-k}{n-k}.
\end{align*}
So $h(A) \sim h(B)$ by the definition of $S(n, \frac{-k}{n-k})$. Therefore $h$ is a homomorphism, and thus $K(n,k)$ has a vector $\frac{n}{k}$-colouring. $\blacksquare$
\section*{8}
\paragraph{}
Consider the program for $\theta(\overline{G})$:
\begin{align*}
\max sum(N) \label{eq:sum}\numberthis  \\
\ni tr(N) &= 1\\
N \circ (J - I - A) & =0 \\
N & \succcurlyeq 0
\end{align*}
and the program
\begin{align*}
\max \frac{1}{tr(N)} \label{eq:tr}\numberthis  \\
\ni sum(N) &= 1\\
N \circ (J - I - A) & =0 \\
N & \succcurlyeq 0.
\end{align*}
It is enough to show that the value of (\ref{eq:sum}) is equal to the value of (\ref{eq:tr}) to obtain that $\theta(\overline{G}) = $ value of (\ref{eq:tr}).
\paragraph{}
First we will show that  value of (\ref{eq:sum}) $\leq$ value of (\ref{eq:tr}). Let $N$ be an optimal solution to (\ref{eq:sum}). We observe that $e_1e_1^T$ is feasible for (\ref{eq:sum}) and hence $sum(N) \geq 1$. To see this observation note that $e_1e_1^T \succcurlyeq 0$, $tr(e_1e_1^T) = 1$, and (since $A_{ii}= 0$):
$$(e_1e_1^T) \circ (J- I - A) = e_1e_1^T - e_1e_1^T - 0 = 0.$$
We claim that $\frac{1}{\sum(N)} N$ is feasible for (\ref{eq:tr}) with objective value equal to the optimal value of (\ref{eq:sum}) and hence (\ref{eq:sum}) $\leq$ (\ref{eq:tr}) as desired. Indeed since  $\frac{1}{sum{N}} \geq 0$ and $N \succcurlyeq 0$, 
$$\frac{1}{sum(N)}N \succcurlyeq 0.$$
We also have $$sum(\frac{1}{sum(N)} N)= \frac{1}{sum(N)}sum(N) = 1.$$
And finally that
$$\frac{1}{sum(N)} N \circ (J - I - A) = \frac{1}{sum(N)} 0 = 0.$$
So $\frac{1}{sum(N)} N$ is feasible for (\ref{eq:tr}). Its objective value is
$$\frac{1}{tr(\frac{1}{sum(N)}N)} = \frac{sum(N)}{tr(N)} = sum(N)$$
which is the optimal value of (\ref{eq:sum}). Hence the value of (\ref{eq:sum}) $\leq$ the value of (\ref{eq:tr}).
\paragraph{}
We finish by showing that the value of (\ref{eq:tr}) $\leq$ value of (\ref{eq:sum}) and hence the values of the two programs are equal. Let $N$ be an optimal solution to (\ref{eq:tr}). We observe that $e_1e_1^T$ is feasible for (\ref{eq:tr}), and hence $\frac{1}{tr(N)} \geq 1$. As we have previously considered $e_1e_1^T$ it only remains to verify that
$$sum(e_1e_1^T) = 1$$
but this is clear. We claim that $\frac{1}{tr(N)} N$ is feasible for (\ref{eq:sum}) with objective value equal to the optimal value of (\ref{eq:tr}) and hence (\ref{eq:tr}) $\leq$ (\ref{eq:sum}) as desired. Indeed since $\frac{1}{tr(N)} \geq 0$ and $N\succcurlyeq 0$,
$$\frac{1}{tr(N)} N \succcurlyeq 0.$$
We also have $$tr(\frac{1}{tr(N)}N) = \frac{tr(N)}{tr(N)} = 1.$$
And finally that
$$\frac{1}{tr(N)} N \circ(J - I - A) = \frac{1}{tr(N)} 0 = 0.$$
So $\frac{1}{tr(N)}N$ is feasible for (\ref{eq:sum}). Its objective value is
$$sum(\frac{1}{tr(N)}N) = \frac{1}{tr(N)} sum(N) = \frac{1}{tr(N)}$$
which is the optimal value of (\ref{eq:tr}). Hence the value of (\ref{eq:tr}) $\leq$ the value of (\ref{eq:sum}).
\paragraph{}
Therefore the optimal values of (\ref{eq:sum}) and (\ref{eq:tr}) are equal and hence $\theta(\overline{G}) =$ value of (\ref{eq:tr}). $\blacksquare$
\section*{10}
\paragraph{}
Consider the inner product on polynomials
$$\langle f, g\rangle = \int_{-1}^1 f(t) g(t) dt.$$
Let $(g_i)i\geq 0$ be a sequence of orthogonal polynomials obtained by Gram-Schmidt from the sequence of powers of $t$. That is,
$$g_i(t) = t^i - \sum_{j=0}^{i-1} \frac{\langle t^i, g_j(t)\rangle}{\langle g_j(t), g_j(t)\rangle} g_j(t).$$
\paragraph{Lemma $10.1$}
For all $i$, $g_i$ has $i$ distinct odd roots of odd multiplicity in the interval $(-1,1)$.
\paragraph{Proof of Lemma $10.1$}
Let $i \geq 0$. Consider the polynomial $g_i$. Let $t_1, \dots, t_n$ be the distinct odd multiplicity roots in the interval $(-1,1)$. Suppose for a contradiction that $n < i$. Consider the polynomial $p(t)$ of the form
$$p(t) = (t-t_1)\cdot \dots \cdot(t-t_n)\cdot 1.$$
Then on the interval $(-1,1)$ the polynomial
$$g_i(t)p(t)$$
has all roots of even multiplicity. Thus $g_i(t)p(t)$ is an even function. Since Gram-Schmidt forms a basis, $g_i \neq 0$, and since $g_i(t)p(t)$ is even on $(-1, 1)$ implies $g_i(t)p(t)$ is not both $\geq 0$ and $\leq 0$ on the interval $(-1,1)$ we may conclude that
$$\int_{-1}^1 g_i(t)p(t) dt \neq 0.$$
But on the other hand, $p(t)$ is a polynomial of degree $n$ and so, since $g_0, \dots, g_{n}(t)$ form a basis of the space of polynomials of degree at most $n$, there exist $a_0, \dots, a_n$ such that
$$p(t) = \sum_{k=0}^n a_k g_k(t).$$
Now since $n < i$ we have for all $k \in \{0,\dots, n\}$
$$\langle g_i, g_k \rangle = 0$$
by Gram-Schmidt. Thus we see that
\begin{align*}
\int_{-1}^1 g_i(t)p_(t) dt &= \int_{-1}^1 g_i(\sum_{k=0}^n a_k g_k(t)) dt\\
&= \sum_{k=0}^n a_k \int_{-1}^1 g_i(t)g_k(t) dt\\
&= \sum_{k=0}^n a_k \langle g_i(t), g_k(t) \rangle\\
&= 0.\end{align*}
But this contradicts that the integral is non-zero. Hence $n \geq i$. But since $g_i(t)$ is a polynomial of degree $i$, $n \leq i$ and so $n=i$. That is $g_i(t)$ has $i$ distinct roots of odd multiplicity in the interval $(-1,1)$. $\blacksquare$
\paragraph{}
Let $p$ be a polynomial such that $p(t) \geq 0$ for all $t \in [-1,1]$, and $p$ divides $g_i$ for all $i \geq 1$. We compute $g_0$ and $g_1$. First $g_0$,
$$g_0 = t^0 = 1 $$
immediately. Now for $g_1$, we have that $\langle t, g_0 \rangle$ is $\int_{-1}^1 t dt = 1 - 1 = 0$, and so
$$g_1(t) = t - \frac{\langle t, 1\rangle}{\langle 1, 1\rangle} 1 = t.$$
Thus since $p$ divides $g_1$ we know $p$ divides $t$, and the degree of $p$ is at most $1$. But if $p$ is of degree $1$ there exists $a$ and $b$ such that $p$ is of the form
$$p(t) = at + b.$$
Since $p$ divides $t$ we can be a little stricter and say either $b=0$ or $a=0$. But if $b=0$ then $p(t) = at$ which changes sign at $0$ in the interval $[-1,1]$. This contradicts that $p(t) \geq 0$ on the interval $[-1,1]$. So $b \neq 0$ and hence $a=0$. That is, $p$ is a constant.
\paragraph{}
Now we deduce that $g_i(1) \neq 0$ for all $i$. But Lemma $10.1$ has already done all the work for us. The polynomial $g_i$ has at most $i$ roots, and by Lemma $10.1$, $g_i$ has $i$ roots in the interval $(-1,1)$. That is all $i$ roots of $g_i$ are accounted for. If $g_i$ had a root at $1$ it would have at least $i+1$ roots, a contradiction. $\blacksquare$
\section*{13}
Let $p_1(t)$ and $p_2(t)$ be two real-valued polynomials. Suppose that $p_1$ is the sum of two squares. That is there exist $a(t), b(t)$ such that
$$p_1(t) = a(t)^2 + b(t)^2.$$
Suppose that $p_2$ is the sum of two squares. That is there exist $c(t), d(t)$ such that
$$p_2(t) = c(t)^2 + d(t)^2$$
Now for any complex number $a+ib$ we have
$$||a + ib||^2 = a^2 + b^2.$$
So applying to our polynomials we see
$$p_1(t) = ||a(t) + ib(t)||^2$$
and
$$p_2(t) = ||c(t) + id(t)||^2.$$
Now using the identity
$$||a + ib||^2||c+id||^2 = ||ac - bd + i(ad + bc)||^2$$
we can obtain that
\begin{align*}
p_1(t)p_2(t) &= ||a(t) + ib(t)||^2||c(t) + i d(t)||^2 \\
&= ||a(t)c(t) - b(t)d(t) + i(a(t)d(t) + b(t) c(t))||^2 \\
&= (a(t)c(t) - b(t)d(t))^2 + (a(t)d(t) + b(t)c(t))^2.
\end{align*}
Therefore $p_1(t)p_2(t)$ is the sum of two squares. $\blacksquare$
\section*{14}
\paragraph{}
Let $p(t)$ be a real-valued polynomial and suppose that $p(t) \geq 0$ for all $t$. By the Fundamental Theorem of Algebra we may factor $p$ as:
$$p(t) = a \Pi_{i=1}^k q_i(t) \Pi_{i=1}^j (t-x_i)^{m_i} \Pi_{i=1}^\ell (t-y_i)^{n_i}$$
where $a$ is a constant, $q_i(t)$ are irreducible quadratics (no real roots), $x_i$ are roots of even multiplicity, and $y_i$ are roots of odd multiplicity. We may assume without loss of generality that each $q_i(t) > 0$ for all $i$. If some $q_i(t) < 0$ factor out $-1$ and modify $a$ as $(-1)a$. Now each $q_i$ is strictly greater than $0$. We claim that $q_i$ can be written as a sum of squares.
\paragraph{Claim 14.1}
If $q_i$ is a strictly positive quadratic then $q_i$ can be written as a sum of squares.
\paragraph{Proof of Claim 14.1}
We may write $q_i$ as
$$q_i(t) = at^2 + bt + c$$
with $a > 0$ and $b^2 - 4ac < 0$ as $q_i(t)$ is strictly positive.	We complete the square as
\begin{align*}
q_i(t) &= at^2 + bt + c \\
&= at^2 + bt +\frac{b^2}{4a} - \frac{b^2}{4a} + c \\
&= (at - \frac{b}{2\sqrt{a}})^2 + \frac{4ac - b^2}{4a}.
\end{align*}
Now $(at - \frac{b}{2\sqrt{a}})^2$ is a square, so it remains to verify that $\frac{4ac - b^2}{4a}$ is a square. Since $b^2 - 4ac < 0$, $4ac - b^2 > 0$, and since $a> 0$, $4a>0$. Therefore $\frac{4ac - b^2}{4a}$ has a square root, that is to say it is a square. Hence $q_i(t)$ is a sum of squares. $\blacksquare$
\paragraph{Main Proof Continues}
Thus by claim $14.1$, each $q_i(t)$ can be written as a sum of squares. Further each $(t-x_i)^m_i$ can be written as a sum of squares: since $m_i$ is even there exists $c$ such that $2c = m_i$. Thus $(t-x_i)^m_i = ((t-x_i)^c)^2$, and since $(t-x_i)^c$ is a polynomial in $t$ we see that $(t-x_i)^m_i$ is a square (that is, can be written as a sum of one square).
\paragraph{}
If $a = 0$ then $p$ vanishes, and clearly is a sum of squares. So we may assume $a \neq 0$. If $\ell$ (the count of odd multiplicity roots) is $0$ then we have $a > 0$. This follows as each $q_i$ is strictly positive, and each even multiplicity linear factor $(t-x_i)^{m_i}$ is non-negative, so $a>0$ as otherwise $p(t) <0$. In claim $14.2$ to follow, we show that indeed $\ell = 0$ and thus $a > 0$ and has a square root. In doing so we prove that $p(t)$ is a product of sums of squares.
\paragraph{Claim 14.2}
The non-negative polynomial $p(t)$ as described above has no roots of odd multiplicity.
\paragraph{Proof of Claim 14.2}
Consider $y_1, \dots, y_\ell$, the roots of odd multiplicity of $p$. Suppose for a contradiction $\ell \geq 1$. We may assume without loss that the roots $y_1, \dots, y_\ell$ are ordered satisfying
$$y_1 < \dots y_{\ell -1}< y_\ell.$$
Choose $t^* \in (y_{\ell-1}, y_\ell)$ if $\ell >1$, or choose $t^* \in (-\infty, y_\ell)$ if $\ell = 1$. Then for all $i \in \{1, \dots, y_{\ell - 1}\}$, $(t^* - y_i)^{n_i} > 0$, but $(t^* - y_\ell)^{n_\ell} < 0$. Further each $q_i(t^*) > 0$ and each even multiplicity linear factor $(t^* - x_i)^{m_i} \geq 0$. Thus $p(t^*) < 0$, contradicting that $p(t) \geq 0$ for all $t$. Hence $\ell = 0$, that is $p(t)$ has no roots of odd multiplicity. $\blacksquare$
\paragraph{Main Proof Continues}
Hence $p(t)$ has no roots of odd multiplicity. Therefore $p(t)$ is a product of sums of squares. To finish the proof we will show in Claim $14.3$ that the product of sums of squares is a sum of squares. Note it is enough to show that the product of two sums of squares is a sum of squares and finish by induction. $\blacksquare$
\paragraph{Claim 14.3}
Let $p_1,p_2$ be polynomials that are sums of squares. Then $p_1p_2$ is a sum of squares.
\paragraph{Proof of Claim 14.3 - Finishes Proof}
We may write $p_1$ as
$$p_1 = \sum_i q_i^2$$
for polynomial $q_i$, and $p_2$ as 
$$p_2 = \sum_j r_j^2.$$
for polynomials $r_j$. Then we have
$$p_1p_2 = \sum_i q_i^2 \sum_j r_j^2 = \sum_i\sum_j q_i^2r_j^2 = \sum_{i,j} (q_ir_j)^2.$$
Therefore $p_1p_2$ is a sum of squares. $\blacksquare$
\end{document}
