\documentclass[letterpaper,11pt,oneside,onecolumn]{article}
\usepackage[margin=0.01in, bottom=0in, top=0.75in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%Page style
\pagestyle{fancy}
\listfiles
\setlength{\parskip}{0em}
\setlength{\parindent}{0em}
\renewcommand{\baselinestretch}{0.1}
\raggedbottom

\rhead{William Justin Toth 671 Course Summmary} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\subsection*{Foundations: Positive Semidefinite Matrices and Convex Sets}
\paragraph{Equivalent Definitions} The following are equivalent: $(a)$ Matrix $A$ is positive semidefinite ($A\succcurlyeq 0$), $(b)$ $A$ is a symmetric and for all $x$, $x^TAx \geq 0$, $(c)$ $A = B^TB$ for some $B$, $(d)$ all eigenvalues of $A$ are non-negative, $(e)$ $\langle A, X\rangle \geq
 0$ for all $X \succcurlyeq 0$. Notes: $\langle A, X\rangle = trace(A^TX)$. Non-singular positive-semidefinite matrices are called positive definite.
 \paragraph{Necessary Conditions}
 If $A \succcurlyeq 0$ and $rank(A) = r$ then $A$ can be written as the sum of $r$ rank $1$ PSD matrices. Also each principal submatrix of $A$ is PSD.
 \paragraph{Cholesky} $M = \begin{bmatrix} A & B \\  B^T &D\end{bmatrix} \succcurlyeq 0$ iff $A \succcurlyeq 0$ and $D - B^TA^{-1}B \succcurlyeq 0$ (note: $A$ is non-singular).
\paragraph{Kronecker and Schur Product}Kronecker: $(A \otimes B)_{(i,k),(j,\ell)} = A_{i,j}B_{k,\ell}$. Properties: $(A\otimes B)(C\otimes D) = AC \otimes BD$, $(A\otimes B)^T = A^T \otimes B^T$, $tr(A \otimes B)= tr(A) tr(B)$, $vec(AXB^T) = (A\otimes B)vec(X)$. Schur: $(A \circ B)_{i,j} = A_{i,j}B_{i,j}$. Properties: $(A\otimes B) \circ(C\otimes D) = (A\circ C) \otimes (B \circ D)$ and $(A \circ B)\otimes(C\circ D) = (A\otimes C) \circ(B \otimes D)$, $\langle A, B \rangle = sum(A \circ B)$.Note: if $A, B\succcurlyeq 0$ then $A\otimes B \succcurlyeq 0$. If $A$ and $B$ are of the same order then $A\circ B \succcurlyeq 0$.
\paragraph{Theorem} If $f$ is a continuous function on compact set $C$ then there exists $c\in C$ such that $f(x) \leq f(c)$ for all $x \in C$.
\paragraph{Affine space} Affine subspaces of a vector space are closed under affine combinations: $y = \sum_r a_r x_r$ with $\sum_r a_r =1$ means $y$ is an affine combination of $x_r$'s. If the maximum size of an affinely independent spanning set of $C$ is $d+1$ then $C$ has affine dimension $d$.
\paragraph{Convex space} Convex combinations are affine combinations where scalars $a_r \geq 0$. Convex spaces are those closed under convex combinations. Half-spaces are convex sets of the form $\{x \in V: \langle \alpha, x \rangle \leq b\}$. A compact convex set is the convex hull of its extreme points. If $x = sa + (1-s)b$ for $0\leq s \leq 1$, $a,b \in C$ implies $s=0$ or $s=1$ then $x$ is an extreme point of $C$. A convex hull of a set of point is the smallest convex set containing those points.
\paragraph{Hyperplanes and Separation}
A hyperplane $\langle h, x\rangle = c$ separates $A,B$ if for all $a \in A$, $\langle h, a\rangle \leq c$ and for all $b \in B$, $\langle h, b \rangle \geq c$. Hyperplane $H$ supports convex set $C$ at $y \in C$ if $C$ lies entirely in one half-space determined by $H$ and $y \in H \cap C$. The metric projection $\pi_C(x) :=$ point in $C$ nearest to $x \not\in C$ has the property that the hyperplane with normal $x - \pi_C(x)$ supports $C$ at $\pi_C(x)$.
\paragraph{Cones} A convex cone is a convex set closed under non-negative linear combinations. The set of PSD matrices of order $n$ form a cone. The dual of cone $K$, denoted $K^*$ is $K^*=\{v \in V: \langle v,k \rangle \geq 0 \text{ for all } k \in K \}$. The PSD cone is self-dual.
\subsection*{Semidefinite Programs: Canonical Form, Duality, Algorithm}
\paragraph{Primal Conic Program} If $K$ is a closed convex cone then then problem:
$\sup \{\langle C, X \rangle : \langle A_i, X \rangle = b_i (i=1,\dots, m), X \in K \} $ is called our primal problem. If $K$ is the cone of PSD matrices then we have a Semidefinite Program.
\paragraph{Dual Conic Program} Corresponding to the primal we have a dual problem given by $\inf \{ y^Tb : \sum_i y_iA_i - C \in K^*\}.$
\paragraph{Duality Theorems} (Weak:) If $y$ is dual feasible and $X$ is primal feasible then $y^Tb \geq \langle C, X\rangle$ with equality iff $\langle \sum_i y_i A_i - C, X\rangle = 0$. (Strong:) If there is $y$ s.t. $\sum_i y_i A_i - C \in int(K^*)$ and dual is bounded below then primal has an optimal solution with no duality gap (difference between primal/dual optimal solutions).
\paragraph{Farkas-Type Lemma} If $K$ is a pointed closed convex cone s.t $int(K) \neq \emptyset$, and system $\langle A_i, X \rangle = b_i (i=1,\dots,m)$ has a solution then exactly one of: (a) There is $X \in int (K)$ st. $\langle A_i, X\rangle = b_i$ for all i, (b) There is $y$ s.t. $\sum_i y_i A_i \in K^*\backslash \{0\}$ and $y^Tb \leq 0$ holds.
\paragraph{Interior Point Method} Problems of the form $\max\{\langle C, X\rangle + \mu \log(\det(X)) : \langle A_i, X\rangle = b_i (i=1,\dots,m),\ X\succ 0 \}$ -$(1)$ can be solved using Newton's gradient descent method since the barrier function: $\mu \log\det(X)$ for $\mu > 0$ allows us to ignore the constraint $X \succ 0$, resulting in a standard problem of convex maximization over an affine space. Hence to solve a standard SDP, solve a corresponding sequence of problems $(1)$ with $\mu \rightarrow 0$ and the optimal solutions $X^*(\mu)$ will converge to the optimal solution of the SDP.
\subsection*{Lovasz Theta}
\paragraph{Strong Product, Shannon Capacity, Lovasz Theta} For graphs $G, H$ the strong product $G \boxtimes H$ is the graph s.t. $V(G\boxtimes H) = V(G) \times V(H)$ and $(u,x)(v,y) \in E(G\boxtimes H)$ iff $u=v$ and $x \sim y$, or $u\sim v$ and $x=y$, or $u\sim v$ and $x\sim y$. The Shannon capcity of a graph is given by $\Theta(G) = \sup_k \alpha(G^{\boxtimes k})^\frac{1}{k}$ where $\alpha(G)$ is max stable set size of $G$. If $V(G) = \{1,\dots, n\}$ and $u_1, \dots, u_n$ are unit vectors s.t. $\langle u_i, u_j\rangle = 0$ if $i \sim j$ (orthogonal representation) then $\theta(G) = \min_{||c|| = 1} \max_{1\leq i \leq n} \frac{1}{\langle, c,u_i\rangle^2}$ is Lovasz's Theta. We have $\Theta(G) \leq \theta (G)$.
\paragraph{SDP for $\theta(G)$} For graph $G$ with $V(G) = \{1,\dots,n\}$ let $\Omega$ be set of $n\times N$ PSD matrices $N$ s.t. $tr(N) = 1$ and $N_{i,j} = 0$ if $i\sim j$. Then $\theta(G) = \max_{N \in \Omega} \langle J, N\rangle$.
\subsection*{Applications: Codes, Colouring, Packing, Quantum Channels}
\paragraph{Codes, Hamming Distance, Packing Radius} The Hamming Distance between codewords $\alpha,\beta \in \Z^n_2$ is $h(\alpha,\beta) = |\{i:\alpha_i \neq \beta_i \}|$. A code $C$ is a set of codewords in $\Z^n_2$. The packing radius $e = \sup \{ r: B_r(\alpha) \cap B_r(\beta) = \emptyset \text{ for all } \alpha,\beta \in C\}$ where $B_r(\alpha)$ is the ball of radius $r$ centred at $\alpha$ w.r.t norm $h$. Our goal is to bound the size of a code $C$ given the packing radius $e$.
\paragraph{Hamming Graph, $\tau$ Algebra} The Hamming graph $H(n,2)$ has vertex set $\Z^n_2$ with vertices adjacent iff their hamming distance is $1$. Let $A_0, A_1, \dots, A_n$ be s.t. $A_i$ is the adjacency matrix of the $i$-th distance graph of $H(n,2)$, where we set $A_0 = I$. Let $D_0,\dots, D_n$ be the $0$$1$ diagonal matrices where $(D_i)_{u,u}= 1$ iff $h(0,u) = r$. Let $\tau$ be the matrix algebra generated by these matrices: $\tau = \langle A_0, \dots, A_n, D_0, \dots, D_n \rangle$.
\paragraph{Code size SDP} A code with packing radius $e$ has minimum distance between two distance words $2e+1$. We can bound the size of $C$ as $|C| \leq \sup \{ \frac{\sum(N)}{tr(N)} : N \circ A_i = 0 (i = 1, \dots, 2e), N \geq 0, N \succcurlyeq 0, N \in \tau \}$.
\paragraph{Vector Colouring, $\chi_{\text{vec}}(G)$} A graph $G$ has a vector $\beta$-colouring if there is a homomorphism $G \rightarrow S(d, \frac{-1}{\beta -1})$ where $S(d,\alpha)$ is the graph with vertices unit vectors in $\R^d$ and adjacency iff $\langle x, y\rangle \leq \alpha$. Let $\chi_{\text{vec}}(G)$ be the least $\beta$ s.t. $G$ has a vector $\beta$-colouring. As an SDP $\chi_{\text{vec}}(G) = \min\{\alpha: M\circ I = I, M\circ A\leq \alpha A, M \succcurlyeq 0\}$ where $A$ is adjacency matrix of $G$. Its dual is $\min\{tr(N) : \sum(N) = 1, N\circ(J-I-A) = 0, N \geq 0, N \succcurlyeq 0\}$.
\paragraph{Kissing Number} Kissing number $\tau_d$ is the max number of pairwise disjoint unit spheres in $\R^d$ we can arrange so they all touch some given sphere. Equivalently $\tau_d$ is the max size of set $C$ of unit vectors s.t $x^Ty\leq \frac{1}{2}$ for all $x\neq y \in C$. We have $\tau_d \leq \inf \{\lambda: p(1) = \lambda, p(t) \leq -1 (t \leq \frac{1}{2}), p \succcurlyeq 0\}$. Where $p$ are polynomials defined on $[-1,1]$, and we say $p\succcurlyeq 0$ iff for all $N$ , for all $x_1,\dots, x_N$ unit vectors the matrix $(p(x_i^Tx_j))_{i,j} \succcurlyeq 0$.
\paragraph{Gegenbauer Polynomials} For continuous functions on unit sphere define $\langle f, g\rangle = \int f(z)g(z) dz$. Obtain the polynomials $g_0, g_1, g_2,\dots$ by applying Gram-Schmidt to $1,t,t^2,\dots$. These are Gegenbauer Polynomials. Our SDP for $\tau_d$ can be written as an LP: $\inf\{\lambda: a_0 + \dots + a_d = \lambda - 1, \sum_r a_r g_r(t) \leq -1 (t\leq \frac{1}{2}), a_1, \dots, a_d \geq 0\}$. Difficulty being constraints for $t\in [-1,\frac{1}{2}]$ are infinite in number so this LP and SDP cannot be computed. Optimal solutions need be creatively constructed by hand.
\paragraph{Quantum Systems, Density matrices, Measurements}A quantum system is a complex inner product space whose states are its $1$-dimensional subspaces. A density matrix is PSD with trace $1$. In general they specify mixed states, but rank $1$ density matrices specify pure states. If a quantum system has dimension $d$ then a measurement $\cM$ of this system is a sequence of matrices $M_1, \dots, M_d$ s.t. $M_i \succcurlyeq 0$ and $\sum M_i = I$. Result of a measurement is $i$ with probability $tr(DM_i)$.
\paragraph{Optimal Measurement}
Our goal is to choose $\cM$ so that $\sum_i p_i \langle D_i, M_i \rangle$ is maximized. This is an SDP: $\max \{\sum_i p_i\langle M_i, D_i \rangle : \sum_i M_i = I, M_i \succcurlyeq )\}$ with dual $\min\{tr(X) : X \succcurlyeq p_iD_i (i =1, \dots, d), X = X^*\}$ ($X^*$ is conjugate-transpose). By complementary slackness one sees $X$ and $\cM$ are optimal iff $X=\sum_i p_iD_iM_i$.
\subsection*{Copositive, Completely Positive, Motzkin-Straus}
\paragraph{Motzkin-Straus} $\frac{1}{\alpha(G)} = \min\{x^T(A+I)x : \textbf{1}^Tx = 1, x\geq 0\}$, where $A$ is adjacency matrix of $G$, and $\textbf{1}$ is all-ones vector.
\paragraph{Completely Positive, Conic Program} As a conic program this is $\min\{\langle A+I, X\rangle : \langle J, X\rangle = 1, X \in C_n^*\}$ where $C_n^*$ is the cone of completely positive matrices (those given by $\sum_i x_i x_i^T$ with $x_i \geq 0$) of order $n$.
\paragraph{Copositve, Dual Program} The cone, $C_n$ of copositive matrices (symmetric matrices $M$ with $x^TMx \geq 0$ for all $x \geq 0$) is dual to $C_n^*$, and the conic program above has dual $\max\{\lambda : I + A -\lambda J \in C_n \}$.
\end{document}
