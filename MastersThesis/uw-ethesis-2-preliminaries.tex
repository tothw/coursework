%======================================================================
\chapter{Preliminaries}
%======================================================================
This chapter gives background on the tools used in the main body of work. Most of this material, with possible exceptions being iterative rounding and the structure of stable matching instances, should be familiar to a student with comparable background to a Combinatorics and Optimization undergraduate at the University of Waterloo. We will emphasize the results in the covered areas that we will need later, but it is important to point out that these areas go much deeper than what is written here and so the end of each section includes suggested texts for further reading.

\section{Matching Theory}
\subsection{Graphs and Matchings}
\paragraph{Graphs} A graph $G$ is an ordered pair $(V,E)$ where $V$ is called the vertex set and $E \subseteq \{\{a,b\} : a,b \in V, a \neq b\}$ is called the edge set. The clause $a \neq b$ forbids self-loops and insisting that $E$ is a proper set forbids parallel edges. Some authors choose to work with more generality but for ease of exposition we will not. As defined above, $G$ is an undirected graph, but if one were to change $E$ to a set of ordered pairs in $V \times V$ then $G$ would be called a directed graph. We will work here with undirected graphs in most chapters. When $V$ and $E$ are not explicit we can used $V(G)$ and $E(G)$ to refer to respectively the vertex and edge sets of $G$.
\paragraph{Paths and Adjacency} In a graph $G$, two vertices $a,b \in V(G)$ are said to be adjacent, denoted $a \sim b$, if $ab \in E$ (notice $ab \in E$ is shorthand for $\{a,b\} \in E$). For any vertex $a$ we call $\delta(a) = \{b \in V(G): b \sim a\}$ the neighbourhood of $a$. The degree of $a$, $d(a)$, is defined as $d(a) = |\delta(a)|$. A path $P \subseteq E$ is a set of edges that describe a sequence of vertices $v_0, v_1, \dots, v_n$ such that for any $i \in \{0,\dots, n-1\}$, $v_iv_{i+1} \in P$. That is a path is a set of edges describing a sequence of adjacent vertices. A cycle is a path with $v_0 = v_n$.
\paragraph{Bipartite Graphs} A graph $G = (V,E)$ is said to be bipartite if there exists a partition of $V$ into $V_0, V_1$ such that for every edge $ab \in E$, $a \in V_0$ and $b \in V_1$. We will restrict our attention to bipartite graphs in this section as most of our work on stable matchings presupposes this case.
\paragraph{Matching} Given a graph $G = (V,E)$, a matching on $G$ is any $M \subseteq E$ satisfying that for all $e_1, e_2 \in M$, $e_1 \cap e_2 = \emptyset$. We use $V(M) = \{v \in V: \exists e \in M, v \in e\}$ to denote the set of vertices matched in $M$. Intuitively our definition of matching means that each vertex matched in $M$ (those in $V(M)$) is matched to exactly one "partner" (ie $|\delta(v) \cap M| = 1$). The partner of vertex $v$, denoted $M(v)$, is the vertex such that $vM(v) \in M$. When it is clear from contex we may also use $M$ to refer to the graph "induced" by $M$, by which we mean the graph $(V(M), M)$.
\subsection{Maximum Cardinality Matching}
\paragraph{Problem} One classical problem in Matching Theory is the question of finding a maximum cardinality matching given a graph. This question was first was investigated by Berge in TODOCITE. He gave a characterization of when a matching isa maximum. To understand the theorem statement we will need just a bit more definitions.
\paragraph{Alternating Paths} A path $P$ in graph $G$ is said to be $M$-alternating for matching $M$ if its sequence of edges alternate being in $M$ and not being in $M$. More precisely if we order the edges of $P$ as $e_0, \dots, e_{n}$ then for any $i \in \{0, \dots, n-1\}$, edge $e_i \in M$ if and only if $e_{i+1} \in E \backslash M$. An $M$-alternating path $P$ is called $M$-augmenting if the first and last edges of $P$ are not in $M$. The name purposefully evokes the image of flow augmenting paths in network flow theory as there is a way to increase the cardinality of a matching $M$ by augmenting (taking symmetric difference, defined below) it with an $M$-augmenting path.
\paragraph{Symmetric Difference} For any sets $S,T \subseteq U$ the symmetric difference of $S$ and $T$, denoted $S \triangle T$, is given by $S \triangle T = (S \cup T) \backslash (S \cap T)$. 
\paragraph{Augmenting Path Theorem (Berge TODOCITE)} A matching $M$ in graph $G$ is maximum if and only if there does not exist any $M$-augmenting path $P$.
\paragraph{Proof} First suppose to the contrary that there exists $M$-augmenting path $P$. We claim that $M \triangle P$ is a matching of greater cardinality than $M$. To see that $M \triangle P$ is a matching let $v \in V(M \triangle P)$. If $v \in V(M)$ and $v \not\in V(P)$ then the edge incident upon $v$ has not changed, and no new edges incident to $v$ were added by symmetric difference with $P$. If $v \in V(M)$ and $v \in V(P)$ then as $P$ is an $M$-augmenting path, $vM(v) \in P$. Hence $vM(v) \notin M \triangle P$, and further there is $u \in V(G)$ such that $vu \in P\backslash M$. Thus $|\delta(v) \cap (M\triangle P)| =1$) as desired. Lastly if $v \not\in V(M)$ then $v$ is one of the endpoints of $P$ and is matched along its edge in $P$. So $M \triangle P$ is a matching. Further since every vertex in $V(M)$ is matched in $M \triangle P$, and the start and end vertices of $P$ are also matched, $M \triangle P$ matches more vertices that $M$ and thus is of greater cardinality.
\paragraph{}
Now suppose that $M$ is a matching which is not of maximum cardinality. That is there exists some matching $M'$ such that $|M'| > |M|$. Consider their symmetric difference $J = M' \triangle M$. Observe that each vertex in the graph $(V(G), J)$ has degree at most two (attaining this if it is matched along difference edges in $M$ and $M'$). Therefore $(V(G), J)$ consists only of vertex disjoint paths and cycles. The edges of said paths and cycles alternate belonging to $M'$ and to $M$ (observe that if this claim were to fail there would be a vertex with two edges incidenct upon it in the same matching, a contradiction). So the cycles are even in number of edges and contain the same number of edges from each of $M'$ and $M$. But since $|M'| > |M|$ there is a path with more edges in $M'$ than in $M$, $P$. This follows from counting edges in $M$ and $M'$ noticing that cycles contribute the same number to each. The path $P$ is $M$-augmenting. $\blacksquare$
\paragraph{}
We cover this proof not only because it is instrinsically interesting, but also because the structure of $J$, the symmetric difference of two matchings, will arise in the future when we study the structure of stable matchings.
\paragraph{Further Reading}
This problem is very well understood. For instance Tutte's classic min-max theorem TODOCITE, and Edmond's Blossom algorithm TODOCITE. A textbook appropriate for advanced undergraduate or beginner graduate students is Combinatorial Optimization by Cook, Cunningham, Pulleybank, and Schrijver TODOCITE which contains a chapter covering the results mentioned here.
\subsection{Maximum Weight Matching}
\paragraph{Problem} Suppose we are given a graph $G$ and a weight function $w : E(G) \rightarrow \R$. The problem now is to find a matching $M$ which maximizes $\sum_{e \in M} w(e)$. This problem and its solution via the Hungarian Algorithm attributed to Kuhn and Munkres TODOCITE is one of the earliest success stories of Combinatorial Optimization. Another approach which gives some flavour of the work to come is to model the problem via a linear program. It is this approach we will explain here.
\paragraph{Linear Program}
If you are uncomfortable with linear programming please see section X.Y TODOCITE, otherwise read on. Consider the following linear program:
\begin{align*}
	\text{max} &\sum_{e \in E(G)} w(e) x_e \\
	\ni \sum_{e \in \delta(v)} x_e &\leq 1 &\text{for all $v \in V(G)$} \\
	x_e &\geq 0 &\text{for all $e \in E(G)$.}
\end{align*}
If we let $M$ be a maximum weight matching of $G$ with weight function $w$ then its incidence vector $\chi(M) \in \R^{|E(G)|}$ is the vector with $\chi(M)_e = 1$ if $e \in M$ and $\chi(M)_e = 0$ otherwise. Since $M$ is a matching $\chi(M)$ is feasible for the above linear program. Hence the optimal solution to the linear program is an upper bound on the maximum weight of a $M$ in $G$. In fact Birkhoff TODOCITE showed that all extreme point solutions to this program are integral and hence optimal extreme point solutions are incidence vectors of matchings. TODOCITE (Edmonds should be here somewhere). In the next section we will break down the previous statement and discuss a proof, not the original proof of Birkhoff, but a proof using the techniques of iterative rounding.
\section{Iterative Rounding}
\paragraph{}
The technique of iterative rounding was originally inspired by Jain's work on survivable network design TODOCITE. Since that time it has proven to be a versatile technique seeing application across a wide variety of branches of Combinatorial Optimization including Matchings, Spanning Trees, Flows, and Network Design TODOCITE to name a few. Iterative Rounding was initially studied as a procedure for obtaining approximation algorithms TODOCITE, but has since been adapted to reprove many classical results TODOCITE. The earliest known use of iterative rounding for exact optimization problems was given by Steinitz in his study of rearrangements TODOCITE. It is this latter application to proofs of integrality that we will focus on, but in the context of matching. We will begin with some necessary background from the theory of linear programming, then proceed to discuss the general form the iterative rounding technique takes in the context we are studying, and finish by demonstrating its application to maximum weight bipartite matching.
\subsection{Linear Programming Tools}
\paragraph{}
Linear programming has proven to be a powerful and unifying tool in combinatorial optimization TODOCITE. In this section we will review the fundamentals of linear programming theory and discuss results necessary for application to iterative rounding. For a comprehensive treatment of linear programming consider the classic text of Chvatal TODOCITE.
\paragraph{Basic Definitions}
Let $A$ be an $m \times n$ matrix, let $b \in \R^m$ and let $c \in \R^n$. The goal of a standard linear programming problem is to find $x \in \{x \in \R^n : Ax \leq b, x \geq 0 \}$ which maximizes $c^Tx$. This is commonly written in the compact form $\max\{c^Tx : Ax \leq b, x \geq 0 \}$ or displayed as
\begin{align*}
\max\quad &c^Tx \\
Ax &\leq b \\
x &\geq 0.
\end{align*}
The linear function $c^Tx$ is referred to as the objective function in variables $x$. Given any row $a_i$ of $A$ and the corresponding $b_i$ entry of $b$, $a_i^Tx \leq b_i$ is a constraint. The constraints $x \geq 0$ (shorthand for $x_i \geq 0$ for all $i \in \{1, \dots, n\}$) are called non-negativity constraints. The choice of the above standard form eases exposition, but it is by no means rigid. If one wishes to apply a constraint of the form $a^T x = b$ or $a^T x \geq b$, or even finishes to be free of non-negativity constraints then such a linear program could be written down and converted to an equivalent program (possible with more variables) in standard form. To see how to do this reference Chvatal TODOCITE, or try it as an easy exercise.
\paragraph{Geometry} Given a linear program in standard form $P = \{ x \in \R^n: Ax \leq b, x \geq 0 \}$ is referred to as the feasible region since it describes the space of vectors which satisfy the constraints of the linear program. Such $P$ is a geometric object called a polyhedron, which can be described as the space formed by the intersection of a finite number of half-spaces. A polyhedron is unbounded if there exists $d \in \R^n\backslash \{0\}$ such that for all $\alpha \in \R$ with $\alpha \geq 0$ and $x \in P$, $x + \alpha d \in P$. Inuitively this says unbounded polyhedra have directions in which any point in the polyhedron can be translated arbitrarily far along and remain in the polyhedron. A polyhedron which is not unbounded is called bounded, and such polyhedra are called polytopes. Observe that if $P$ is unbounded the corresponding linear program may not have a finite optimal value. In particular this happens when translating along a direction $d$ can increase the objective value, that is for $x \in P$ we have the situation $c^T(x + \alpha d) > c^Tx$. It should be intuitive from the geometry that linear programs whose feasible regions are polytopes always have finite optimal values, but for the critical reader who needs a proof I refer you to Chvatal TODOCITE.
\paragraph{Vertices, Extreme Points and Basic Feasible Solutions}
Let $x$ be a vector in $P$. Then $x$ is a vertex of $P$ if there exists $h$ and $\delta$ such that $h^Tx = \delta$ and for all $y \neq x \in P$ $h^Ty < \delta$. The hyperplane specified by $(h,\delta)$ is called a supporting hyperplane for $x$. We say $x$ is an extreme point if there does not exist distinct $y^1, y^2 \in P$ such that $x$ is a convex combination of $y^1, y^2$. Precisely there does not exists $y^1 \neq y^2 \in P$ such that $x = \frac{1}{2} y^1 + \frac{1}{2}y^2$. The vector $x$ is called a basic feasible solution if there exists $n$ linearly independent rows of $A$, say $a_1, \dots, a_n$ (not necessarily the first $n$ rows of $A$), with corresponding entries of $b$: $b_1, \dots, b_n$ satisfying $a_i x = b_i$ for all $i\in \{1,\dots, n\}$. It is not immediately obvious that the three concepts $x$ being a vertex, an extreme point, or a basic feasible solution are equivalent but in the next lemma we will show the surprising fact that they are indeed.
\paragraph{Lemma} Let $x \in P$ then the following are equivalent: $x$ is a vertex of $P$, $x$ is an extreme point of $P$, and $x$ is a basic feasible solution of $P$.
\paragraph{Proof}
First suppose that $x$ is a vertex of $P$ with supporting hyperplane $h^T x = \delta$. Then for all $y \neq x \in P$, $h^Ty < \delta$. Suppose for a contradiction that $x$ is not an extreme point of $P$. Then there exists $y^1 \neq y^2 \in P$ such that $x = \frac{1}{2}y^1 + \frac{1}{2} y^2$. So then $$\delta = h^T x = h^T(\frac{1}{2} y^1 + \frac{1}{2} y^2) = \frac{1}{2}h^Ty^1 + \frac{1}{2} h^Ty^2 < \frac{1}{2}\delta + \frac{1}{2} \delta = \delta,$$ yielding $\delta < \delta$, a contradiction. Thus $x$ is a vertex implies $x$ is an extreme point.
\paragraph{}
For ease of exposition, at this point let $$A' = \begin{bmatrix} A \\ -I \end{bmatrix} \quad b' = \begin{bmatrix} b \\ 0 \end{bmatrix}$$ and thus $P = \{x\in P : A'x \leq b'\}$. We proceed by contraposition. Suppose that $x$ is not a basic feasible solution. Then there are not $n$ linearly independent constraints tight at $x$. Let $k$ be the number of constraints satisfied by $x$ at equality. We may assume that the first $k$ rows of $A'$, $a_1, \dots, a_k$ along with the first $k$ entries of $b'$ describe the constraints $x$ satisfies at equality. Notice that either $k<n$, or $k=n$ and $a_1, \dots, a_k$ are not linearly independent. In either case the system (in variables $y$):
\begin{align*}
a_1^T y &= 0 \\
&\dots \\
a_k^T y &= 0
\end{align*}
has a non-trivial solution. Let $d \neq 0 \in \R^n$ be such a solution. Since adding or subtracting multiples of $d$ from $x$ does not affect satisfaction of tight constraints, and all other constraints satisfied by $x$ are not tight (that is, $a_i x < b_i$ for such constraints), there exists $\epsilon > 0$ such that $x+ \epsilon d, x-\epsilon d \in P$. Thus letting $y^1 = x+ \epsilon d$ and $y^2 = x-\epsilon d$ we have
$$\frac{1}{2}(y^1 + y^2) = \frac{1}{2}(2x) = x.$$
Since $y^1 \neq y^2 \in P$ and $x = \frac{1}{2}y^1 + \frac{1}{2}y^2$, $x$ is not an extreme point. Thus we have shown that $x$ is extreme point implies that $x$ is a basic feasible solution.
\paragraph{}
Now suppose that $x$ is a basic feasible solution. Suppose that rows of $A'$, $a_1, \dots, a_n$, along with $b_1, \dots, b_n$ describe the linearly independent constraints satisfied by $x$ at equality. By linear independence, $x$ is the unique solution to 
$$\begin{bmatrix} a_1 \\ \dots \\ a_n \end{bmatrix} x = \begin{bmatrix} b_1 \\ \dots \\ b_n \end{bmatrix}.$$
Let $h^T = \sum_{i=1}^n a_i$ and let $\delta = \sum_{i=1}^n b_i$. We claim that $(h, \delta)$ describe a supporting hyperplane for $x$. Indeed let $y \in P$. Then 
$$ h^T y = \sum_{i=1}^n a_i y \leq \sum_{i=1}^n b_i$$
with the last inequality following from $y \in P$ as $a_i y \leq b_i$ for all $i$. Observe that equality holds above if and only if $a_i y = b_i$ for all $i$, but such an event would imply that $y = x$ by the uniqueness of $xs$ previously demonstrated. Hence if $y \in P$ with $y \neq x$ then $h^T y < \delta$. Therefore $x$ is a basic feasible solution implies that $x$ is a vertex, and hence the lemma holds. $\blacksquare$
\paragraph{Lemma} The following lemma claims that we can always find extreme point optimal solutions. Let $x$ be an optimal solution to the linear program $\max\{c^Tx : x \in P \}$. We claim there exists $x^* \in P$ such that $x^*$ is an extreme point and $c^Tx = c^Tx^*$.
\paragraph{Proof}
We proceed by induction on the number of tight constraints (including non-negativity constraints). If all constraints are tight, clearly $x$ is extreme. Hence we will demonstrate that if $x$ is not extreme we can always find a point with more tight constraints (possibly non-negativity constraints, which manifest as $0$-entries of $xs$), and so the result holds inductively. Since $x$ is not extreme there exists $y^1 \neq y^2 \in P$ such that $$x = \frac{1}{2} y^1 + \frac{1}{2}y^2.$$
We define a direction $d\neq 0$ along which we will translate $x$ without ruining optimality. Let
$$ d = \frac{1}{2}y^2 - \frac{1}{2} y^1.$$
Then $x + d = y^2 \in P$ and $x - d = y^1 \in P$. We have the following
\begin{align*} A(x+d) &\leq b \\ x+d &\geq 0 \\
A(x-d) &\leq b \\ x-d &\geq 0 .\end{align*}
Let $A^=$ be the collection of rows of $A$ along with corresponding entries of $b$, $b^=$, that satisfy $A^= x = b^=$. So by the above inequalities $A^=(-d) \leq 0$ and $A^=d \leq 0$. Therefore $A^=d = 0$. Further, by the optimality of $x$,
\begin{align*}
c^Tx &\geq c^T(x+d) \\
\text{and } c^Tx &\geq c^T(x-d) \\
\text{which implies } c^Td = 0.
\end{align*}
Thus for any $\epsilon$, $x+\epsilon d$ satisfies $A^=(x+\epsilon d) = b^=$ and $c^T(x+\epsilon d) = c^Tx$. Since $d \neq 0$, we may assume that there exists $i$ such that $d_i < 0$ (otherwise we may used $-d$ instead of $d$). We define $\epsilon > 0$, which is finite as $d_i < 0$, as follows:
$$ \epsilon = \min\{ \min\{\frac{x_j}{-y_j}: y_j < 0\}, \min \{\frac{a_i x - b_i}{-a_iy}: a_i x < b_i, a_i y < 0 \}\}.$$
where $a_i$ are the rows $A$ and $b_i$ is the corresponding entry of $b$. By our choice of $\epsilon$ either $x+\epsilon d$ has one more $0$ entry (in the $\frac{x_j}{-y_j}$ case) or one more tight constraint (in the $\frac{a_i x - b_i}{-a_i}$ case) than $x$. Further my the minimality of $\epsilon$ the non-tight constraints remain feasible, and as previously argued the tight constraints are not broken and $x + \epsilon d$ remains optimal. $\blacksquare$
\paragraph{Integrality}
The previous lemma tells us that an algorithm which solves linear programs need only concern itself with extreme point solutions. So extreme point solutions of are of particular interest in applications. We call a polyhedron $P$ integral if every extreme point $x$ of $P$ is contained in $\Z^n$. Integrality is important when using linear programs to model discrete optimization problems since fractional solutions rarely have an interpretation in this context. Later in this section we will demonstrate how iterative rounding can be used to prove that the feasible region of a linear program is integral.
\paragraph{Algorithms for Linear Programming}
The most widely taught algorithm for solving linear programs is the simplex method TODOCITE. The algorithm works by starting at an extreme point and traveling along edges to adjacent extreme points which improve the objective value until the optimal solution is found. Theoretically the simplex method needs to examine an exponential number of extreme points to find an optimal solution TODOCITE, yet it has been observed to work efficiently in practice TODOCITE possibly due to its average case performance under certain assumption TODOCITE that seem to be satisfied by problems arising in applications. The ellipsoid method came later, initially as an approximation algorithm for real convex minimization TODOCITE, and later shown to be able to solve linear programs in polynomial time TODOCITE. While not thought to be faster than simplex in practice, the polynomial time worst case bound of the ellipsoid method is important for discrete optimization as it allows the use of linear programming for giving polynomial time algorithms for combinatorial problems,
\paragraph{Further Reading}
The theory of linear programming is rich and very well understood. One particularly important topic not mentioned here that one needs to know to truly understand linear programming is duality theory and complementary slackness. It studies the phenomenon that linear programs come in primal-dual pairs that are solved simultaneously, and the consequences surrounding this. Unfortunately this theory is outside the scope of this thesis, but I encourage the interested reader to see Chvatal TODOCITE for a treatment of this topic. Its study is worthwhile for combinatorial optimizers as it has lead to, among other ideas, the famous class of algorithms known as primal-dual methods TODOCITE.
\subsection{Strategy}
\paragraph{}
After our brief detour into the theory of linear programming, we are now ready to describe the method through which iterative rounding can used to prove that a linear programming relaxation of a combinatorial optimization problem is integral. In this subsection we will describe the general approach at a high level, and in the following section we will apply this approach to bipartite matching to demonstrate the technical details. See the excellent monograph of Lau, Ravi, and Singh TODOCITE for other applications, and ways to extend this approach to giving approximation algorithms.
\paragraph{Step 1 - Linear Programming Formulation}
We are given some combinatorial optimization  problem $(CP)$ to start. Generally speaking such problems take the form of choosing an optimal subset $S \in \mathcal{S}$ where $\mathcal{S}$ is a family of feasible sets formed from elements of a discrete ground set $E$. From this optimization problem we attempt to write a linear programming relaxation $(LP)$. For $(LP)$ to be a relaxation of $(CP)$, there must be a bijection between solutions of $(CP)$ and integral solutions of $(LP)$. Classically this bijection is achieved via the mapping of sets to their corresponding incidence vectors (see $\chi$ in max weight matching section TODOLINK). Such $(LP)$ is called a relaxation of $(CP)$ because. in the case of maximization problems, the optimal value of $(CP)$ is bounded above by the optimal value of $(LP)$ and in the case that $(LP)$ is integral the optimal values are equal.
\paragraph{}
In some applications the linear programming relaxation given in step $1$ will have an exponential number of constraints. In this situation it is not immediate that the $(LP)$ is solvable in polynomial time. While this problem does not arise in the applications to matching we describe in this thesis, there is a way to circumvent it. The ellipsoid method operates with a separation oracle TODOCITE, which is a procedure that decides if a given point is feasible for the linear program, and in the case of infeasibility provides a hyperplane which separates the point from the feasible region. It is normal that the inequalities of $(LP)$ have some implicit description in terms of the combinatorics of $(CP)$, and so it may be possible to give a polynomial time separation oracle despite having exponential explicit constraints. If one can give such a separation oracle then their $(LP)$ is polynomial time solvable despite its exponential size.
\paragraph{Step 2 - Iterative Algorithm}
In this step we describe an algorithm which constructs an integral optimal solution from an extreme point optimal solution to $(LP)$. As we will show in the following lemma, the existence of such an algorithm implies that $(LP)$ is integral. The algorithm constructs integral optimal solution $x^*$ as follows:
\begin{enumerate}
\item Set $x$ to optimal extreme point. Let $E$ be the variable set.
\item For each variable $e$ for which $x_e  \in \{0,1\}$ set $x^*_e = x_e$. Remove $e$ from $E$.
\item If $E$ is now empty terminate.
\item Formulate the new $(LP)$ with reduced variable set $E$.
\item Recompute optimal solution to new $(LP)$. Repeat from step $1$.
\end{enumerate}
Intuitively this algorithm computes optimal solutions, fixes their integral variables, and recomputes a solution to the "residual" problem with the integral variables removed. This process terminates when all variables have been fixed to $0$ or $1$.
\paragraph{Lemma} If the iterative algorithm of Step $2$ is correct and terminates then the feasible region of $(LP)$ is integral.
\paragraph{Proof}
Let $x$ be an extreme point of the $P$, the feasible region of $(LP)$. Then $x$ is equivalently of a vertex of $P$. So there exists a supporting hyperplane described by $(h,\delta)$ for which $h^Tx = \delta$ and for all $x' \neq x \in P$, $h^Tx' < \delta$. If we consider the linear program with feasible region $P$ and objective function $h$ then $x$ is the unique optimal solution of this linear program. Hence for any extreme point of $P$ there exists an objective function for which that extreme point is the unique optimal solution of the corresponding linear program. Therefore if we run the algorithm of Step $2$ with extreme point $x$ and objective function $h$ then the integral solution returned is equal to $x$ by uniqueness. Thus $x$ is integral, and so any extreme of $P$ is integral. $\blacksquare$
\paragraph{Step 3 - Algorithm Analysis}
We need to establish two things in this phase, that the algorithm terminates in finite time giving an integral vector and that it indeed returns an optimal solution.
\subparagraph{Termination} To show that the algorithm terminates we demonstrate that there always exists an $e$ for which $x_e \in \{0,1\}$ for every iteration. Doing so proves inductively that the algorithm terminates with a $\{0,1\}$ valued vector. There is a standard approach to showing the existence of $x_e \in \{0,1\}$ which proceeds by contradiction. If we suppose that every $x_e \in (0,1)$ (classically $0\leq x \leq 1$ is part of our $(LP)$ formulation) then we may obtain a contradiction as follows. Since $x$ is a basic feasible solution we have that there are $|E|$ linearly independent constraints tight at $x$. Since $x_e > 0$ we know no such constraints are non-negativity constraints. We may then further use $x_e < 1$ to obtain some structure and show an upper bound on the number of linearly independent tight constraints that is smaller than $n$, contradicting that $x$ is an extreme point. This will become more concrete in the next section when we demonstrate an example.
\subparagraph{Correctness} It remains to show that the algorithm returns an optimal solution. This typically takes the form of an inductive argument on the size of the $(LP)$. In that context showing that the vector $x$ restricted to the variables not removed in line $2$ of the algorithm is a feasible solution to the residual linear program allows you to conclude that the objective value at $x$ is at most that of the integral solution returned. Since $x$ is optimal this implies that the integral solution returned has the same objective value as $x$ and is thus optimal.
\subsection{Application to Matching}
\section{Stable Matching}
\subsection{Problem}
\subsection{Gale Shapley Algorithm}
\subsection{Structure}

