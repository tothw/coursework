%======================================================================
\chapter{Preliminaries}
%======================================================================
This chapter gives background on the tools used in the main body of work. Most of this material, with possible exceptions being iterative rounding and the structure of stable matching instances, should be familiar to a student with comparable background to a Combinatorics and Optimization undergraduate at the University of Waterloo. We will emphasize the results in the covered areas that we will need later, but it is important to point out that these areas go much deeper than what is written here and so the end of each section includes suggested texts for further reading.

\section{Matching Theory}
\subsection{Graphs and Matchings}
\begin{definition} A graph $G$ is an ordered pair $(V,E)$ where $V$ is called the vertex set and $E \subseteq \{\{a,b\} : a,b \in V, a \neq b\}$ is called the edge set. The clause $a \neq b$ forbids self-loops and insisting that $E$ is a proper set forbids parallel edges. Some authors choose to work with more generality but for ease of exposition we will not. As defined above, $G$ is an undirected graph, but if one were to change $E$ to a set of ordered pairs in $V \times V$ then $G$ would be called a directed graph. We will work here with undirected graphs in most chapters. When $V$ and $E$ are not explicit we can used $V(G)$ and $E(G)$ to refer to respectively the vertex and edge sets of $G$.
\end{definition}
\begin{definition} In a graph $G$, two vertices $a,b \in V(G)$ are said to be adjacent, denoted $a \sim b$, if $ab \in E$ (notice $ab \in E$ is shorthand for $\{a,b\} \in E$). For any vertex $a$ we call $\delta(a) = \{b \in V(G): b \sim a\}$ the neighbourhood of $a$. The degree of $a$, $d(a)$, is defined as $d(a) = |\delta(a)|$. A path $P \subseteq E$ is a set of edges that describe a sequence of vertices $v_0, v_1, \dots, v_n$ such that for any $i \in \{0,\dots, n-1\}$, $v_iv_{i+1} \in P$. That is a path is a set of edges describing a sequence of adjacent vertices. A cycle is a path with $v_0 = v_n$.
\end{definition}
\begin{definition} A graph $G = (V,E)$ is said to be bipartite if there exists a partition of $V$ into $V_0, V_1$ such that for every edge $ab \in E$, $a \in V_0$ and $b \in V_1$. We will restrict our attention to bipartite graphs in this section as most of our work on stable matchings presupposes this case.
\end{definition}
\begin{definition} Given a graph $G = (V,E)$, a matching on $G$ is any $M \subseteq E$ satisfying that for all $e_1, e_2 \in M$, $e_1 \cap e_2 = \emptyset$. We use $V(M) = \{v \in V: \exists e \in M, v \in e\}$ to denote the set of vertices matched in $M$. Intuitively our definition of matching means that each vertex matched in $M$ (those in $V(M)$) is matched to exactly one "partner" (ie $|\delta(v) \cap M| = 1$). The partner of vertex $v$, denoted $M(v)$, is the vertex such that $vM(v) \in M$. When it is clear from contex we may also use $M$ to refer to the graph "induced" by $M$, by which we mean the graph $(V(M), M)$.
\end{definition}
\subsection{Maximum Cardinality Matching}
\paragraph{Problem} One classical problem in Matching Theory is the question of finding a maximum cardinality matching given a graph. This question was first was investigated by Berge in \cite{berge1957two}. He gave a characterization of when a matching is maximum. To understand the theorem statement we will need just a bit more definitions.
\begin{definition} A path $P$ in graph $G$ is said to be $M$-alternating for matching $M$ if its sequence of edges alternate being in $M$ and not being in $M$. More precisely if we order the edges of $P$ as $e_0, \dots, e_{n}$ then for any $i \in \{0, \dots, n-1\}$, edge $e_i \in M$ if and only if $e_{i+1} \in E \backslash M$. An $M$-alternating path $P$ is called $M$-augmenting if the first and last edges of $P$ are not in $M$. The name purposefully evokes the image of flow augmenting paths in network flow theory as there is a way to increase the cardinality of a matching $M$ by augmenting (taking symmetric difference, defined below) it with an $M$-augmenting path.
\end{definition}
\begin{definition} For any sets $S,T \subseteq U$ the symmetric difference of $S$ and $T$, denoted $S \triangle T$, is given by $S \triangle T = (S \cup T) \backslash (S \cap T)$. 
\end{definition}
\begin{theorem} Augmenting Path Theorem (Berge \cite{berge1957two}): A matching $M$ in graph $G$ is maximum if and only if there does not exist any $M$-augmenting path $P$.
\end{theorem}
\begin{proof}
\paragraph{} First suppose to the contrary that there exists $M$-augmenting path $P$. We claim that $M \triangle P$ is a matching of greater cardinality than $M$. To see that $M \triangle P$ is a matching let $v \in V(M \triangle P)$. If $v \in V(M)$ and $v \not\in V(P)$ then the edge incident upon $v$ has not changed, and no new edges incident to $v$ were added by symmetric difference with $P$. If $v \in V(M)$ and $v \in V(P)$ then as $P$ is an $M$-augmenting path, $vM(v) \in P$. Hence $vM(v) \notin M \triangle P$, and further there is $u \in V(G)$ such that $vu \in P\backslash M$. Thus $|\delta(v) \cap (M\triangle P)| =1$) as desired. Lastly if $v \not\in V(M)$ then $v$ is one of the endpoints of $P$ and is matched along its edge in $P$. So $M \triangle P$ is a matching. Further since every vertex in $V(M)$ is matched in $M \triangle P$, and the start and end vertices of $P$ are also matched, $M \triangle P$ matches more vertices that $M$ and thus is of greater cardinality.
\paragraph{}
Now suppose that $M$ is a matching which is not of maximum cardinality. That is there exists some matching $M'$ such that $|M'| > |M|$. Consider their symmetric difference $J = M' \triangle M$. Observe that each vertex in the graph $(V(G), J)$ has degree at most two (attaining this if it is matched along difference edges in $M$ and $M'$). Therefore $(V(G), J)$ consists only of vertex disjoint paths and cycles. The edges of said paths and cycles alternate belonging to $M'$ and to $M$ (observe that if this claim were to fail there would be a vertex with two edges incidenct upon it in the same matching, a contradiction). So the cycles are even in number of edges and contain the same number of edges from each of $M'$ and $M$. But since $|M'| > |M|$ there is a path with more edges in $M'$ than in $M$, $P$. This follows from counting edges in $M$ and $M'$ noticing that cycles contribute the same number to each. The path $P$ is $M$-augmenting.
\end{proof}
\paragraph{}
We cover this proof not only because it is intrinsically interesting, but also because the structure of $J$, the symmetric difference of two matchings, will arise in the future when we study the structure of stable matchings.
\paragraph{Further Reading}
This problem is very well understood. For instance Tutte's classic min-max theorem \cite{tutte1947factorization}, and Edmond's Blossom algorithm \cite{edmonds1965paths}. A textbook appropriate for advanced undergraduate or beginner graduate students is Combinatorial Optimization by Cook, Cunningham, Pulleybank, and Schrijver \cite{cook2009combinatorial} which contains a chapter covering the results mentioned here.
\subsection{Maximum Weight Matching}\label{GT:MWM}
\paragraph{Problem} Suppose we are given a graph $G$ and a weight function $w : E(G) \rightarrow \R$. The problem now is to find a matching $M$ which maximizes $\sum_{e \in M} w(e)$. This problem and its solution via the Hungarian Algorithm attributed to Kuhn and Munkres \cite{kuhn1955hungarian}\cite{munkres1957algorithms} is one of the earliest success stories of Combinatorial Optimization. Another approach which gives some flavour of the work to come is to model the problem via a linear program. It is this approach we will explain here.
\paragraph{Linear Program}
If you are uncomfortable with linear programming please see section \ref{IR:LP}, otherwise read on. Consider the following linear program:
\begin{align*}
	&\text{max} \sum_{e \in E(G)} w(e) x_e \\
	&\ni \sum_{e \in \delta(v)} x_e \leq 1 &\text{for all $v \in V(G)$} \\
	&\quad\quad\quad\ \ x_e \geq 0 &\text{for all $e \in E(G)$.}
\end{align*}
If we let $M$ be a maximum weight matching of $G$ with weight function $w$ then its incidence vector $\chi(M) \in \R^{|E(G)|}$ is the vector with $\chi(M)_e = 1$ if $e \in M$ and $\chi(M)_e = 0$ otherwise. Since $M$ is a matching $\chi(M)$ is feasible for the above linear program. Hence the optimal solution to the linear program is an upper bound on the maximum weight of a $M$ in $G$. In fact Birkhoff \cite{birkhoff1946tres} showed that all extreme point solutions to this program are integral and hence optimal extreme point solutions are incidence vectors of matchings. In the next section we will break down the previous statement and discuss a proof, not the original proof of Birkhoff, but a proof using the techniques of iterative rounding.
\section{Iterative Rounding}\label{IR}
\paragraph{}
The technique of iterative rounding was originally inspired by Jain's work on survivable network design \cite{jain2001factor}. Since that time it has proven to be a versatile technique seeing application across a wide variety of branches of Combinatorial Optimization including Matchings, Spanning Trees, Flows, and Network Design \cite{lau2011iterative} to name a few. Iterative Rounding was initially studied as a procedure for obtaining approximation algorithms, but has since been adapted to reprove many classical results. The earliest known use of iterative rounding for exact optimization problems was given by Steinitz in his study of rearrangements \cite{steinitz1913bedingt}. It is this latter application to proofs of integrality that we will focus on, but in the context of matching. We will begin with some necessary background from the theory of linear programming, then proceed to discuss the general form the iterative rounding technique takes in the context we are studying, and finish by demonstrating its application to maximum weight bipartite matching.
\subsection{Linear Programming Tools}\label{IR:LP}
\paragraph{}
Linear programming has proven to be a powerful and unifying tool in combinatorial optimization. In this section we will review the fundamentals of linear programming theory and discuss results necessary for application to iterative rounding. For a comprehensive treatment of linear programming consider the classic text of Chvatal \cite{chvatal1983linear}.
\begin{definition}
Let $A$ be an $m \times n$ matrix, let $b \in \R^m$ and let $c \in \R^n$. The goal of a standard linear programming problem is to find $x \in \{x \in \R^n : Ax \leq b, x \geq 0 \}$ which maximizes $c^Tx$. This is commonly written in the compact form $\max\{c^Tx : Ax \leq b, x \geq 0 \}$ or displayed as
\begin{align*}
\max\quad &c^Tx \\
Ax &\leq b \\
x &\geq 0.
\end{align*}
The linear function $c^Tx$ is referred to as the objective function in variables $x$. Given any row $a_i$ of $A$ and the corresponding $b_i$ entry of $b$, $a_i^Tx \leq b_i$ is a constraint. The constraints $x \geq 0$ (shorthand for $x_i \geq 0$ for all $i \in \{1, \dots, n\}$) are called non-negativity constraints. 
\end{definition}
\paragraph{}The choice of the above standard form eases exposition, but it is by no means rigid. If one wishes to apply a constraint of the form $a^T x = b$ or $a^T x \geq b$, or even finishes to be free of non-negativity constraints then such a linear program could be written down and converted to an equivalent program (possible with more variables) in standard form. To see how to do this reference Chvatal \cite{chvatal1983linear}, or try it as an easy exercise.
\begin{definition} Given a linear program in standard form $P = \{ x \in \R^n: Ax \leq b, x \geq 0 \}$ is referred to as the feasible region since it describes the space of vectors which satisfy the constraints of the linear program. Such $P$ is a geometric object called a polyhedron, which can be described as the space formed by the intersection of a finite number of half-spaces. A polyhedron is unbounded if there exists $d \in \R^n\backslash \{0\}$ such that for all $\alpha \in \R$ with $\alpha \geq 0$ and $x \in P$, $x + \alpha d \in P$. Inuitively this says unbounded polyhedra have directions in which any point in the polyhedron can be translated arbitrarily far along and remain in the polyhedron. A polyhedron which is not unbounded is called bounded, and such polyhedra are called polytopes. 
\end{definition}
\paragraph{}Observe that if $P$ is unbounded the corresponding linear program may not have a finite optimal value. In particular this happens when translating along a direction $d$ can increase the objective value, that is for $x \in P$ we have the situation $c^T(x + \alpha d) > c^Tx$. It should be intuitive from the geometry that linear programs whose feasible regions are polytopes always have finite optimal values, but for the critical reader who needs a proof I refer you to Chvatal \cite{chvatal1983linear}.
\begin{definition}
Let $x$ be a vector in $P$. Then $x$ is a vertex of $P$ if there exists $h$ and $\delta$ such that $h^Tx = \delta$ and for all $y \neq x \in P$ $h^Ty < \delta$. The hyperplane specified by $(h,\delta)$ is called a supporting hyperplane for $x$. We say $x$ is an extreme point if there does not exist distinct $y^1, y^2 \in P$ such that $x$ is a convex combination of $y^1, y^2$. Precisely there does not exists $y^1 \neq y^2 \in P$ such that $x = \frac{1}{2} y^1 + \frac{1}{2}y^2$. The vector $x$ is called a basic feasible solution if there exists $n$ linearly independent rows of $A$, say $a_1, \dots, a_n$ (not necessarily the first $n$ rows of $A$), with corresponding entries of $b$: $b_1, \dots, b_n$ satisfying $a_i x = b_i$ for all $i\in \{1,\dots, n\}$.
\end{definition}
\paragraph{} It is not immediately obvious that the three concepts $x$ being a vertex, an extreme point, or a basic feasible solution are equivalent but in the next lemma we will show the surprising fact that they are indeed.
\begin{lemma} Let $x \in P$ then the following are equivalent: $x$ is a vertex of $P$, $x$ is an extreme point of $P$, and $x$ is a basic feasible solution of $P$.
\end{lemma}
\begin{proof}
First suppose that $x$ is a vertex of $P$ with supporting hyperplane $h^T x = \delta$. Then for all $y \neq x \in P$, $h^Ty < \delta$. Suppose for a contradiction that $x$ is not an extreme point of $P$. Then there exists $y^1 \neq y^2 \in P$ such that $x = \frac{1}{2}y^1 + \frac{1}{2} y^2$. So then $$\delta = h^T x = h^T(\frac{1}{2} y^1 + \frac{1}{2} y^2) = \frac{1}{2}h^Ty^1 + \frac{1}{2} h^Ty^2 < \frac{1}{2}\delta + \frac{1}{2} \delta = \delta,$$ yielding $\delta < \delta$, a contradiction. Thus $x$ is a vertex implies $x$ is an extreme point.
\paragraph{}
For ease of exposition, at this point let $$A' = \begin{bmatrix} A \\ -I \end{bmatrix} \quad b' = \begin{bmatrix} b \\ 0 \end{bmatrix}$$ and thus $P = \{x\in P : A'x \leq b'\}$. We proceed by contraposition. Suppose that $x$ is not a basic feasible solution. Then there are not $n$ linearly independent constraints tight at $x$. Let $k$ be the number of constraints satisfied by $x$ at equality. We may assume that the first $k$ rows of $A'$, $a_1, \dots, a_k$ along with the first $k$ entries of $b'$ describe the constraints $x$ satisfies at equality. Notice that either $k<n$, or $k=n$ and $a_1, \dots, a_k$ are not linearly independent. In either case the system (in variables $y$):
\begin{align*}
a_1^T y &= 0 \\
&\dots \\
a_k^T y &= 0
\end{align*}
has a non-trivial solution. Let $d \neq 0 \in \R^n$ be such a solution. Since adding or subtracting multiples of $d$ from $x$ does not affect satisfaction of tight constraints, and all other constraints satisfied by $x$ are not tight (that is, $a_i x < b_i$ for such constraints), there exists $\epsilon > 0$ such that $x+ \epsilon d, x-\epsilon d \in P$. Thus letting $y^1 = x+ \epsilon d$ and $y^2 = x-\epsilon d$ we have
$$\frac{1}{2}(y^1 + y^2) = \frac{1}{2}(2x) = x.$$
Since $y^1 \neq y^2 \in P$ and $x = \frac{1}{2}y^1 + \frac{1}{2}y^2$, $x$ is not an extreme point. Thus we have shown that $x$ is extreme point implies that $x$ is a basic feasible solution.
\paragraph{}
Now suppose that $x$ is a basic feasible solution. Suppose that rows of $A'$, $a_1, \dots, a_n$, along with $b_1, \dots, b_n$ describe the linearly independent constraints satisfied by $x$ at equality. By linear independence, $x$ is the unique solution to 
$$\begin{bmatrix} a_1 \\ \dots \\ a_n \end{bmatrix} x = \begin{bmatrix} b_1 \\ \dots \\ b_n \end{bmatrix}.$$
Let $h^T = \sum_{i=1}^n a_i$ and let $\delta = \sum_{i=1}^n b_i$. We claim that $(h, \delta)$ describe a supporting hyperplane for $x$. Indeed let $y \in P$. Then 
$$ h^T y = \sum_{i=1}^n a_i y \leq \sum_{i=1}^n b_i$$
with the last inequality following from $y \in P$ as $a_i y \leq b_i$ for all $i$. Observe that equality holds above if and only if $a_i y = b_i$ for all $i$, but such an event would imply that $y = x$ by the uniqueness of $xs$ previously demonstrated. Hence if $y \in P$ with $y \neq x$ then $h^T y < \delta$. Therefore $x$ is a basic feasible solution implies that $x$ is a vertex, and hence the lemma holds. 
\end{proof}
\begin{lemma}The following lemma claims that we can always find extreme point optimal solutions. Let $x$ be an optimal solution to the linear program $\max\{c^Tx : x \in P \}$. We claim there exists $x^* \in P$ such that $x^*$ is an extreme point and $c^Tx = c^Tx^*$.
\end{lemma}
\begin{proof}
We proceed by induction on the number of tight constraints (including non-negativity constraints). If all constraints are tight, clearly $x$ is extreme. Hence we will demonstrate that if $x$ is not extreme we can always find a point with more tight constraints (possibly non-negativity constraints, which manifest as $0$-entries of $xs$), and so the result holds inductively. Since $x$ is not extreme there exists $y^1 \neq y^2 \in P$ such that $$x = \frac{1}{2} y^1 + \frac{1}{2}y^2.$$
We define a direction $d\neq 0$ along which we will translate $x$ without ruining optimality. Let
$$ d = \frac{1}{2}y^2 - \frac{1}{2} y^1.$$
Then $x + d = y^2 \in P$ and $x - d = y^1 \in P$. We have the following
\begin{align*} A(x+d) &\leq b \\ x+d &\geq 0 \\
A(x-d) &\leq b \\ x-d &\geq 0 .\end{align*}
Let $A^=$ be the collection of rows of $A$ along with corresponding entries of $b$, $b^=$, that satisfy $A^= x = b^=$. So by the above inequalities $A^=(-d) \leq 0$ and $A^=d \leq 0$. Therefore $A^=d = 0$. Further, by the optimality of $x$,
\begin{align*}
c^Tx &\geq c^T(x+d) \\
\text{and } c^Tx &\geq c^T(x-d) \\
\text{which implies } c^Td = 0.
\end{align*}
Thus for any $\epsilon$, $x+\epsilon d$ satisfies $A^=(x+\epsilon d) = b^=$ and $c^T(x+\epsilon d) = c^Tx$. Since $d \neq 0$, we may assume that there exists $i$ such that $d_i < 0$ (otherwise we may used $-d$ instead of $d$). We define $\epsilon > 0$, which is finite as $d_i < 0$, as follows:
$$ \epsilon = \min\{ \min\{\frac{x_j}{-y_j}: y_j < 0\}, \min \{\frac{a_i x - b_i}{-a_iy}: a_i x < b_i, a_i y < 0 \}\}.$$
where $a_i$ are the rows $A$ and $b_i$ is the corresponding entry of $b$. By our choice of $\epsilon$ either $x+\epsilon d$ has one more $0$ entry (in the $\frac{x_j}{-y_j}$ case) or one more tight constraint (in the $\frac{a_i x - b_i}{-a_i}$ case) than $x$. Further my the minimality of $\epsilon$ the non-tight constraints remain feasible, and as previously argued the tight constraints are not broken and $x + \epsilon d$ remains optimal.
\end{proof}
\paragraph{Integrality}
The previous lemma tells us that an algorithm which solves linear programs need only concern itself with extreme point solutions. So extreme point solutions of are of particular interest in applications. We call a polyhedron $P$ integral if every extreme point $x$ of $P$ is contained in $\Z^n$. Integrality is important when using linear programs to model discrete optimization problems since fractional solutions rarely have an interpretation in this context. Later in this section we will demonstrate how iterative rounding can be used to prove that the feasible region of a linear program is integral.
\paragraph{Algorithms for Linear Programming}
The most widely taught algorithm for solving linear programs is the simplex method\cite{dantzig1955generalized}. The algorithm works by starting at an extreme point and traveling along edges to adjacent extreme points which improve the objective value until the optimal solution is found. Theoretically the simplex method needs to examine an exponential number of extreme points to find an optimal solution, yet it has been observed to work efficiently in practice possibly due to its average case polynomial time performance \cite{smale1983average}. The ellipsoid method came later, initially as an approximation algorithm for real convex minimization, and later shown to be able to solve linear programs in polynomial time \cite{grotschel1981ellipsoid}. While not thought to be faster than simplex in practice, the polynomial time worst case bound of the ellipsoid method is important for discrete optimization as it allows the use of linear programming for giving polynomial time algorithms for combinatorial problems,
\paragraph{Further Reading}
The theory of linear programming is rich and very well understood. One particularly important topic not mentioned here that one needs to know to truly understand linear programming is duality theory and complementary slackness. It studies the phenomenon that linear programs come in primal-dual pairs that are solved simultaneously, and the consequences surrounding this. Unfortunately this theory is outside the scope of this thesis, but I encourage the interested reader to see Chvatal \cite{chvatal1983linear} for a treatment of this topic. Its study is worthwhile for combinatorial optimizers as it has lead to, among other ideas, the famous class of algorithms known as primal-dual methods.
\subsection{Strategy}
\paragraph{}
After our brief detour into the theory of linear programming, we are now ready to describe the method through which iterative rounding can used to prove that a linear programming relaxation of a combinatorial optimization problem is integral. In this subsection we will describe the general approach at a high level, and in the following section we will apply this approach to bipartite matching to demonstrate the technical details. See the excellent monograph of Lau, Ravi, and Singh\cite{lau2011iterative} for other applications, and ways to extend this approach to giving approximation algorithms.
\paragraph{Step 1 - Linear Programming Formulation}
We are given some combinatorial optimization  problem $(CP)$ to start. Generally speaking such problems take the form of choosing an optimal subset $S \in \mathcal{S}$ where $\mathcal{S}$ is a family of feasible sets formed from elements of a discrete ground set $E$. From this optimization problem we attempt to write a linear programming relaxation $(LP)$. For $(LP)$ to be a relaxation of $(CP)$, there must be a bijection between solutions of $(CP)$ and integral solutions of $(LP)$. Classically this bijection is achieved via the mapping of sets to their corresponding incidence vectors (see $\chi$ in max weight matching section \ref{GT:MWM}). Such $(LP)$ is called a relaxation of $(CP)$ because. in the case of maximization problems, the optimal value of $(CP)$ is bounded above by the optimal value of $(LP)$ and in the case that $(LP)$ is integral the optimal values are equal.
\paragraph{}
In some applications the linear programming relaxation given in step $1$ will have an exponential number of constraints. In this situation it is not immediate that the $(LP)$ is solvable in polynomial time. While this problem does not arise in the applications to matching we describe in this thesis, there is a way to circumvent it. The ellipsoid method operates with a separation oracle \cite{grotschel1981ellipsoid}, which is a procedure that decides if a given point is feasible for the linear program, and in the case of infeasibility provides a hyperplane which separates the point from the feasible region. It is normal that the inequalities of $(LP)$ have some implicit description in terms of the combinatorics of $(CP)$, and so it may be possible to give a polynomial time separation oracle despite having exponential explicit constraints. If one can give such a separation oracle then their $(LP)$ is polynomial time solvable despite its exponential size.
\paragraph{Step 2 - Iterative Algorithm}
In this step we describe an algorithm which constructs an integral optimal solution from an extreme point optimal solution to $(LP)$. As we will show in the following lemma, the existence of such an algorithm implies that $(LP)$ is integral. The algorithm constructs integral optimal solution $x^*$ as follows:
\begin{enumerate}
\item Set $x$ to optimal extreme point. Let $E$ be the variable set.
\item For each variable $e$ for which $x_e  \in \{0,1\}$ set $x^*_e = x_e$. Remove $e$ from $E$.
\item If $E$ is now empty terminate.
\item Formulate the new $(LP)$ with reduced variable set $E$.
\item Recompute optimal solution to new $(LP)$. Repeat from step $1$.
\end{enumerate}
Intuitively this algorithm computes optimal solutions, fixes their integral variables, and recomputes a solution to the "residual" problem with the integral variables removed. This process terminates when all variables have been fixed to $0$ or $1$.
\begin{lemma} If the iterative algorithm of Step $2$ is correct and terminates then the feasible region of $(LP)$ is integral.
\end{lemma}
\begin{proof}
\paragraph{}
Let $x$ be an extreme point of the $P$, the feasible region of $(LP)$. Then $x$ is equivalently of a vertex of $P$. So there exists a supporting hyperplane described by $(h,\delta)$ for which $h^Tx = \delta$ and for all $x' \neq x \in P$, $h^Tx' < \delta$. If we consider the linear program with feasible region $P$ and objective function $h$ then $x$ is the unique optimal solution of this linear program. Hence for any extreme point of $P$ there exists an objective function for which that extreme point is the unique optimal solution of the corresponding linear program. Therefore if we run the algorithm of Step $2$ with extreme point $x$ and objective function $h$ then the integral solution returned is equal to $x$ by uniqueness. Thus $x$ is integral, and so any extreme of $P$ is integral. 
\end{proof}
\paragraph{Step 3 - Algorithm Analysis}
We need to establish two things in this phase, that the algorithm terminates in finite time giving an integral vector and that it indeed returns an optimal solution.
\subparagraph{Termination} To show that the algorithm terminates we demonstrate that there always exists an $e$ for which $x_e \in \{0,1\}$ for every iteration. Doing so proves inductively that the algorithm terminates with a $\{0,1\}$ valued vector. There is a standard approach to showing the existence of $x_e \in \{0,1\}$ which proceeds by contradiction. If we suppose that every $x_e \in (0,1)$ (classically $0\leq x \leq 1$ is part of our $(LP)$ formulation) then we may obtain a contradiction as follows. Since $x$ is a basic feasible solution we have that there are $|E|$ linearly independent constraints tight at $x$. Since $x_e > 0$ we know no such constraints are non-negativity constraints. We may then further use $x_e < 1$ to obtain some structure and show an upper bound on the number of linearly independent tight constraints that is smaller than $n$, contradicting that $x$ is an extreme point. This will become more concrete in the next section when we demonstrate an example.
\subparagraph{Correctness} It remains to show that the algorithm returns an optimal solution. This typically takes the form of an inductive argument on the size of the $(LP)$. In that context showing that the vector $x$ restricted to the variables not removed in line $2$ of the algorithm is a feasible solution to the residual linear program allows you to conclude that the objective value at $x$ is at most that of the integral solution returned. Since $x$ is optimal this implies that the integral solution returned has the same objective value as $x$ and is thus optimal.
\subsection{Application to Matching}
\paragraph{}
Recall the linear programming relaxation of maximum weight matching in bipartite graphs:
\begin{align*}
	&\text{max} \sum_{e \in E(G)} w(e) x_e \\
	&\ni \sum_{e \in \delta(v)} x_e \leq 1 &\text{for all $v \in V(G)$} \\
	&\quad\quad\quad\ \ x_e \geq 0 &\text{for all $e \in E(G)$.}
\end{align*}
We will use iterative rounding to prove that this linear program, $LP(G)$, is integral. Consider an iterative rounding algorithm for bipartite matching with weighted input graph $G$.
\begin{enumerate}
\item Set $M \leftarrow \emptyset$
\item While $|E(G)| > 0$\begin{enumerate}
\item Let $x$ be an extreme point optimal solution to $LP(G)$.
\item For each edge $e$ with $x_e = 1$: $M \leftarrow M \cup \{e\}$.
\item For each edge $e$ with $x_e \in \{0,1\}$ : $G \leftarrow G - e$.
\end{enumerate}
\item return $M$
\end{enumerate}
To prove that $LP(G)$ is integral for any weighted bipartite graph $G$ we need to show that our iterative algorithm always terminates in finite time and returns an integral solution of optimal value.
\paragraph{} Since our graph $G$ is finite, demonstrating termination is simply a matter of verifying that for any extreme point solution $x$ of $LP(G)$ there exists an edge $e$ with $x_e \in \{0,1\}$. In doing so we will need the following lemma about extreme points of $LP(G)$.
\begin{lemma} Let $x$ be an extreme point optimal solution to $LP(G)$. Suppose that for all $e \in E(G)$, $x_e > 0$. Then there exists $W \subseteq V(G)$ satisfying
\begin{itemize}
\item $\sum_{e\in\delta(v)} x_e = 1$ for all $v \in W$ \\
\item $\{\chi(\delta(v)) : v \in W\}$ is linearly independent (recall that $\chi : E \rightarrow \{0,1\}^{|E|}$ maps edge sets to incidence vectors)
\item $|W| = |E|$.
\end{itemize} 
\end{lemma}
\begin{proof}
\paragraph{}
The proof is immediate from observing $x$ is a basic feasible solution and no non-negativity constraints are tight.
\end{proof}
\paragraph{} We will now prove that our iterative rounding algorithm always terminates returning a matching (equivalently an integral solution). We will do this via a contradiction argument proving that a $01$ variable can always be found.
\begin{lemma} Let $x$ be an optimal extreme point solution to $LP(G)$. Then there exists $e \in E(G)$ such that $x_e \in \{0,1\}$.
\end{lemma}
\begin{proof}
\paragraph{}
Suppose for a contradiction that for all $e\in E(G)$, $0 < x_e < 1$. By our previous lemma there exists $W \subseteq V(G)$ containing vertices corresponding to $|E(G)|$ linearly independent tight vertex constraints. Let $v \in W$. Since $\sum_{e\in \delta(v)} x_e = 1$ and $x_e < 1$ for all $e$, we have $d(v) \geq 2$. So then
$$2|W| = 2|E| = \sum_{v \in V(G)} d(v) \geq \sum_{v\in W} d(v) \geq 2|W|.$$
The inequalities about thus hold as equalities. Since $\sum_{v\in V(G)} d(v) = \sum_{v \in W} d(v)$, $d(v) = 0$ for all $v \not\in W$. Since $\sum_{v \in W} d(v) = 2|W|$, $d(v) = 2$ for all $v \in W$. Therefore $E(G)$ is a set of cycles covering the vertices of $W$. Let $C \subseteq E(G)$ be a cycle with $V(C) \subseteq W$. Let $V_1, V_2$ be a bipartition of the vertex set of $G$ (which exists since $G$ is bipartite). Further since $G$ is bipartite for each edge $vw \in E(C)$ one of $v,w \in V_1$ and the other of $v,w \in V_2$. Hence we have
$$\sum_{v \in V(C) \cap V_1} \chi(\delta(v)) = \sum_{v\in V(C) \cap V_2} \chi(\delta(v))$$
which contradicts the linear independence of $\{\chi(\delta(v)) : v \in W\}$.
\end{proof}
\paragraph{}
It remains to verify that our algorithm returns an optimal solution. We show that via an inductive argument in the following lemma.
\begin{lemma}
The iterative rounding algorithm returns a matching of weight at least that of an optimal solution $LP(G)$.
\end{lemma}
\begin{proof}
\paragraph{}
We proceed by induction on the number of iterations the algorithm runs to solve the problem. In the base case if the algorithm runs for a single iteration then the returned solution is exactly the incidence vector of an extreme point optimal solution to $LP(G)$. That is, the lemma holds trivially. So for induction suppose the algorithm needs $k$ iterations to return an answer for input graph $G$ and for any graph $G'$ such that the algorithm needs less than $k$ iterations to solve the problem, the algorithm returns an optimal matching. Let $x$ be the extreme point of $LP(G)$ found in step $2(a)$. Let $R$ be the set of edges removed in step $2(c)$ of the algorithm when run on $G$. Let $G'$ be the residual graph ($G$ restricted to edges $E(G) \backslash R$). The following iteration is equivalent to running the algorithm on $G'$. Let $M'$ be the matching returned. By the inductive hypothesis $w(\chi(M')) \geq w(x')$ for any $x'$ feasible for $LP(G')$. Let $x'$ be the point $x$ restricted to edges variables in $E(G')$. Then $w(x) = w(x') + w(\chi(R))$. Further, as $x'$ is feasible for $LP(G')$, we have
$w(\chi(M')) \geq w(x')$. Let $M$ be the matching returned. Then $M = M' \cup R$ and thus
$$w(\chi(M)) = w(\chi(M')) + w(\chi(R)) \geq w(x') + w(\chi(R)) = w(x).$$
Therefore by induction the algorithm always returns a matching with weight at least that of an optimal solution to $LP(G)$. 
\end{proof}
\paragraph{}
As discussed in the previous section, the above lemmas imply that that $LP(G)$ is integral.
\section{Stable Matching}
\subsection{Problem}
\paragraph{Input}The stable matching problem gives you a bipartite graph $G=(V, E)$ with bipartition $V = A\cup B$, where each $v \in V$ has a strict order over $\delta(v)$ referred to as $v$'s preferences. We will use $u <_v w$ to denote that $(u,w)$ is in $v$'s order of $\delta(v)$, and we will use $>_v$, $\leq_v$, $\geq_v$, and $=_v$ analogously.
\paragraph{Output} Given a stable matching instance you are to find a matching $M$ that is stable. A matching is called stable if there is no blocking pair $(a,b)$. A pair $(a,b)$ is said to block $M$ if $ab \not\in M$ and $b >_a M(a)$ and $a >_b M(b)$.
\paragraph{Application} The sets $A,B$ are colloqially referred to as men and women respectively. This alludes to the toy application of stable matching in forming couples from single people together who have preferences over the opposite gender. In this context a blocking pair references to a pair who mutually prefer each other to their partner. One of the original applications of stable matching \cite{roth1984evolution} was to pair medical students with hospitals at which they were to do their residency, and another was college admissions.
\subsection{Gale Shapley Algorithm}
\paragraph{}
In their original work, Gale and Shapley gave an algorithm, called deferred acceptance \cite{gale1962college}, for solving the stable matching problem and in doing so proved that every bipartite graph with strict preference orders has a stable matching.
\paragraph{Deferred Acceptance Algorithm}
In the following description of the algorithm we adopt the convention that vertices prefer to be matched than to be unmatched. For each vertex in $a\in A$ we maintain a set $P(a)$ of vertices in $B$ that $a$ has previously "proposed" to. Given an input graph $G=(A \cup B, E)$ with preferences do:
\begin{enumerate}
\item Assign $M \leftarrow \emptyset$
\item Assign $P(a) \leftarrow \emptyset$ for all $a \in A$
\item While there exists $a \in A$ such that $a \not\in V(M)$ and $P(a) \neq \delta(a)$:
	\begin{enumerate}
	\item Let $b$ be such that $b \geq_a b'$ for all $b' \in \delta(v) \backslash P(a)$
	\item Assign $P(a) \leftarrow P(a) \cup \{b\}$
	\item If $a >_b M(b)$ then assign $M \leftarrow M \cup \{ab\} \backslash \{M(b)b\}$
	\end{enumerate}
\item Return $M$.
\end{enumerate}
\begin{lemma} Deferred Acceptance returns a matching.
\end{lemma}
\begin{proof}
\paragraph{}
From step $3(c)$ it is clear that each $b \in B$ is matched to at most one $a \in A$. Further any $a \in A$ matched in step $3(c)$ is unmatched by the While condition of step $3$, and hence each $a \in A$ is matched to at most one $b \in B$. Therefore $M$ returned is a matching. \end{proof}
\begin{theorem}(Gale-Shapley): The matching returned by Deferred Acceptance is stable.
\end{theorem}
\begin{proof}
\paragraph{}
We claim that Deferred Acceptance maintains the following invariant: at any iteration for any $a \in A$ for all $b \in P(a)$, $M(b) \geq_b a$. We proved this claim by induction. In the base case, at the start of the algorithm $P(a) = \emptyset$ for all $a \in A$ and hence the claim is vacuously true. In the inductive case consider some iteration $i$. Let $a\in A$ be the vertex chosen in step $3$. Let $b \in B$ be as chosen in step $3(a)$. In step $3(b)$, $b$ is added to $P(a)$. By induction the invariant holds for all $b' \in P(a)$ except $b$. In step $3(c)$ if $a \leq_b M(b)$ then the invariant holds for $b \in P(a)$ and no changes are made. If $a >_b M(b)$ then $b$ is matched to $a$ and hence after step $3(c)$ the invariant holds for $b$ and thus the entirety of $P(a)$. For all other $a' \neq a$, $P(a')$ is unchanged and hence by induction the invariant holds for all $b' \in P(a')$ except $b$. So consider if $b \in P(a')$. Observe that $a$ is preferred by $b$ to her match previous to this iteration and hence by transitivity the invariant also holds for $b \in P(a')$. Therefore by induction the invariant holds. It is easy to see from step $3$ that the algorithm also maintains the invariant that for all $b \in \delta(a)\backslash P(a)$, $M(a) \geq_a b$. Hence if $ab \in E$ with $ab \not\in M$ at termination of the algorithm then either $b \in P(a)$ or $b \not\in P(a)$. If $b \in P(a)$ then $M(b) >_b a$ (as $a \neq M(b)$) and so $ab$ is not a blocking pair. If $b \not\in P(a)$ then as $ab \in E$, $b \in \delta(a) \backslash P(a)$ and so $M(a) >_a b$ (as $b \neq M(a)$). Therefore in either case $ab$ does not block $M$. Therefore $M$ is stable.
\end{proof}
\subsection{Structure}
\paragraph{}
Over time a rich body of literature around the structure of the set of stable matchings for some bipartite graph with preferences has been developed \cite{roth1992two}. In this subsection we will explore some of these results that will come up in our chapter on stable matching polytopes.
\begin{definition} We say a stable matching $M$ is $A$-optimal with respect to our problem instance if for every stable matching $M'$ in this problem instance and for every $a \in A$, $M(a) \geq_a M'(a)$. That is, every man in $A$ likes $M$ at least as well as they like any other stable matching. We can define $B$-optimal analogously. Gale and Shapley showed that not only to $A$-optimal and $B$-optimal stable matchings exist in any problem instance, but also that their Deferred Acceptance algorithm computes an $A$-optimal stable matching \cite{gale1962college} (and could compute $B$-optimal matching by switching roles of $A$ and $B$).
\end{definition}
\begin{theorem} (Gale and Shapley): For any biparite graph $G = (A\cup B, E)$ with strict preferences there always exists a unique $A$-optimal ($B$-optimal analogously) stable matching and it can be computed via the Deferred Acceptance algorithm.
\end{theorem}
\begin{proof}
\paragraph{}
For any $v \in A\cup B$ let $C(v) = \{w \in \delta(v) : \text{there exists stable matching } M, w = M(v) \}$ be the set of partners $v$ has a chance of being matched to in a stable matching. We will show that no $a \in A$ is ever rejected by any $b \in C(a)$ during the operation of Deferred Acceptance and hence since each $a \in A$ proposes to their most preferred choice in $C(a)$ before any other vertex in $C(a)$ the resulting matching is $A$-optimal. The uniqueness is achieved since the preferences are strict. We proceed by induction on the number of iterations Deferred Acceptance has ran. In the base case no proposals and hence no rejections have yet happened. So consider the inductive case. Suppose that at the given iterations $b$ rejects $a$ (that is, either $a$ is unsuccessful in proposing to $b$ or $b$ leaves $a$ for a better proposer at this step). Let $a' \neq$ be the man $b$ is matched to at the end of step $3(c)$ of the iteration under consideration. Then $a' >_b a$. By our induction hypothesis $P(a') \cap C(a') = \{b\}$ as $a'$ has not been rejected by any $b' \in C(a')$. Let $M$ be a matching formed by matching $a$ to $b$ and matching every other $v$ to a vertex in $C(v)$. Then $b >_{a'} M(a')$ as $b$ is $a'$ most preferred vertex in $C(a')$. Further $a' >_b a=M(b)$, and thus the pair $(a',b)$ blocks $M$. Since $(a'b)$ blocks any such matching $M$, $b \not\in C(a)$ (as otherwise we could form a stable $M$). Hence the claim holds by induction, and we have proven the theorem. \end{proof}
\begin{theorem} (Knuth) \cite{knuthmariages}: Knuth took the above idea a step further to show that the interests of $A$ and $B$ are opposed to each other. Let $M$ and $M'$ be two stable matchings for some problem instance. Let $M >_A M'$ denote that for all $a \in A$, $M(a) \geq_a M'(a)$. Define $M >_B M'$ analogously. We have that $M >_A M'$ if and only if $M' >_B M$.
\end{theorem}
\begin{proof}
\paragraph{}
Suppose that $M >_A M'$. We will only show that $M' >_B M$ since the other direction is symmetric about the roles of $A$ and $B$. Suppose for a contradiction that there exists $b \in B$ for which $M(b) >_b M'(b)$. Let $a = M(b)$. Since preferences are strict, $M'(b) \neq M(b)$ and thus $M'(a) \neq M(a)$ as otherwise $b$ has two matches in $M'$. Since $M >_A M'$ and $M(a) \neq M'(a)$, $b=M(a) >_a M'(a)$. That is we have $a >_b M'(b)$ and $b >_a M'(a)$ and thus $(a,b)$ blocks $M'$. So $M'$ is not a stable matching, a contradiction. \end{proof}
\begin{corollary}An immediate corollary is that the $A$-optimal stable matching matches each $b \in B$ to their least preferred partner they have a chance of being stably matched with (and vice-versa regarding the $B$-optimal stable matching). 
\end{corollary}
\paragraph{}
These theorems begin to point towards an interesting structure for the set of stable matchings of a given problem instance. It was first observed by Conway \cite{knuthmariages} that this set forms an algebraic structure called a lattice. If you are not familiar with lattices in this context, don't worry. We will explain all the terms necessary to continue.
\begin{definition}
Let $L$ be a set endowed with a partial order $\geq$. The set $L$ is said for be a lattice if for any $x, y \in L$ there exists $sup(x,y) \in L$ such that $sup(x,y) \geq x$ and $sup(x,y) \geq y)$, and there exists $inf(x,y) \in L$ such that $x \geq inf(x,y)$ and $y \geq inf(x,y)$. A lattice is said to be complete if each $X \subseteq L$ also has functions $sup$ and $inf$. 
\end{definition}
\paragraph{Lattice functions for stable matchings} We now describe functions that will behave as $sup$ and $inf$ for stable matchings. Let $M, M'$ be any two stable matchings for a given problem instance. Let $M^* = sup(M,M')$ be given as follows. For all $a \in A$, $M^*(a) = M(a)$ if $M(a) >_a M'(a)$ and $M^*(a) = M'(a)$ otherwise. For all $b \in B$, $M^*(b) = M(b)$ if $M(b) <_b M'(b)$ and $M^*(b) = M'(b)$ otherwise. We can define $inf(M,M')$ analogously by interchanging the roles of $A$ and $B$. Intuitively $sup$  presents each $a \in A$ with a choice between their partner in $M$ and partner in $M'$ and pairs them with their preferred choice. In the next theorem we need to show $sup$ and $inf$ map to stable matchings and respect the partial order $>_A$.
\begin{theorem}(Conway) \cite{knuthmariages}: The set of stable matchings for a given problem instance endowed with the partial order $>_A$ is a lattice via the functions $sup$ and $inf$ defined above.
\end{theorem}
\begin{proof}
\paragraph{}
We will demonstrate that $sup$ behaves as desired and note that the argument is symmetric about the roles of $A$ and $B$ for $inf$. Let $M, M'$ be stable matchings. It is clear from the construction of $sup(M,M')$ that $sup(M,M') >_A M$ and $sup(M,M') >_A M'$ provided that $sup(M,M')$ is a stable matching. Let $M^* = sup(M,M')$. We first show that $M^*$ is a matching and then verify stability. To show that $M^*$ is a matching we will prove that $M^*(a) = b$ if and only if $M^*(b) = a$. First, for sufficiency, suppose that $M^*(a) = b$.  We may assume without loss of generality that $b = M(a)$. Then $b \geq_a M'(a)$. Since $M'$ is stable, $a \leq_b M'(b)$ and hence $M^*(b) = a$ as desired. Now to show necessity suppose that $M^*(b) = a$. We may assume that $a = M(b)$. So $a \leq_b M'(b)$. Again by the stability of $M'$, $b \geq_a M'(a)$. So $M^*(a) = b$ as desired. Therefore $M^*$ is a matching. 
\paragraph{}
Now to see stability. Suppose for a contradiction that $ab$ blocks $M^*$. Then $b >_a M*(a)$ and thus $b>_a M(a)$ and $b >_a M'(a)$. Also $a >_b M^*(b)$. So if $M^*(b) = M(b)$ then $ab$ blocks $M$ and if $M^*(b) = M'(b)$ then $ab$ block $M'$. Therefore in either case we have  a contradiction. \end{proof}
\paragraph{Further Reading}
The set of stable matchings of a given graph with preferences was been widely studied in academic literature. We have covered here the results key to our work in the following chapter, but many more interesting results are known. For instance the surprising equivalence between distributive lattices and sets of stable matchings that we only began to set the stage for here. To read more on these things I recommend the survey text of Roth and Sotomayor\cite{roth1992two}.